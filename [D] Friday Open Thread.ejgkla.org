#+TITLE: [D] Friday Open Thread

* [D] Friday Open Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 26
:DateUnix: 1578063938.0
:END:
Welcome to the Friday Open Thread! Is there something that you want to talk about with [[/r/rational]], but which isn't rational fiction, or doesn't otherwise belong as a top-level post? This is the place to post it. The idea is that while reddit is a large place, with lots of special little niches, sometimes you just want to talk with a certain group of people about certain sorts of things that aren't related to why you're all here. It's totally understandable that you might want to talk about Japanese game shows with [[/r/rational]] instead of going over to [[/r/japanesegameshows]], but it's hopefully also understandable that this isn't really the place for that sort of thing.

So do you want to talk about how your life has been going? Non-rational and/or non-fictional stuff you've been reading? The recent album from your favourite German pop singer? The politics of Southern India? The sexual preferences of the chairman of the Ukrainian soccer league? Different ways to plot meteorological data? The cost of living in Portugal? Corner cases for siteswap notation? All these things and more could possibly be found in the comments below!

Please note that this thread has been merged with the Monday General Rationality Thread.


** So I'm watching the new Dracula mini-series and there are actually a few fun ideas in it regarding vampire lore. Spoilers ahead:

The first thing is that when Dracula feeds on someone he acquires some of their attributes and knowledge. Examples include being able to fluently speak a language he was a bit rusty with, a stutter, and he mentions that, had he only fed on the ship's crew members, he would have arrived in London with the social prowess of a sailor. I think one of the most interesting aspects of this is that it might offer up a logical reason for why a moral vampire doesn't just feed on animal blood since it could result in them having subhuman intelligence and if you only fed on rapists/murderers you'd end up becoming one yourself. Furthermore, this sort of behavioral inheritance aspect of feeding is used to explain his fear of the cross. He claims that he's been feeding on God-fearing Christians for so long he has acquired something like an extreme phobia of all Christian iconography (he also says he's excited to feed on some atheists, implying it might circumvent his weakness). Now, the Vanhellsing character claims this is a lie, but it's unclear if she actually thought he was lying or if she was just trying to bait him into a prolonged argument as a distraction, but we've got one more episode coming tonight and after the absurd ending of episode 2 (he walks out of the ocean after the shipwreck and is accosted by what appears to be Vanhellsing with modern-day government authorities, so there's been a big time jump and Vanhellsing may have found a way to exploit the vampire curse) I'm pretty curious where this will go.

edit: warning, episode 3 really shit the bed.
:PROPERTIES:
:Author: babalook
:Score: 14
:DateUnix: 1578073111.0
:END:

*** Sounds interesting. Would you recommend it or was episode 3 so bad you believe the rest won't be worthwhile?
:PROPERTIES:
:Author: RetardedWabbit
:Score: 2
:DateUnix: 1578118826.0
:END:

**** Episode 1 and 2 holds really well on their own. It's worth a look.

If you have to, just skim through 3. It's like a total change in tone.. Like reading a bad fanfiction ending of a good series.
:PROPERTIES:
:Author: _brightwing
:Score: 6
:DateUnix: 1578121903.0
:END:


**** I still think the first two episodes had some fun stuff in them, but I'll watch almost anything with vampires so my opinion is more than a little biased.
:PROPERTIES:
:Author: babalook
:Score: 5
:DateUnix: 1578122504.0
:END:


*** Sounds like the premise of iZombie
:PROPERTIES:
:Author: ProfessorPhi
:Score: 1
:DateUnix: 1578137567.0
:END:


** EDIT: STREAM NO LONGER LIVE, THANKS FOR JOINING! We had a lot of [[https://imgur.com/a/25G0DQI][fun]] and might do more r/r related streams in the future.

A few friends and I are watching [[https://cdn.discordapp.com/attachments/437697099383963668/662730039984652288/dahlstream.png][Roald Dahl movies]] tomorrow so we can

1. Have fun together as a community and
2. Understand some references in Chili and the Chocolate Factory [[https://www.fanfiction.net/s/13451176/]], the latest hit rational fic. (YES, EVEN NOW)

if you're interested, join us tomorrow at 3PM EST in [[https://cytu.be/r/homestuckmovienight]].

We'll watch all the movies people feel like watching, might split them across two streams if people get tired. If you're new to these streams please install the [[https://cytu.be/google_drive_userscript][movie synch script]] first.

Judging from past movie streams I'd expect around 15 people to join, so don't worry about awkward dead chats.
:PROPERTIES:
:Author: Makin-
:Score: 13
:DateUnix: 1578078034.0
:END:

*** Are you still doing this despite the most recent chapter?
:PROPERTIES:
:Author: xamueljones
:Score: 4
:DateUnix: 1578149497.0
:END:

**** What kind of horrible person would I be if I left so many people unfulfilled????? Yes I'm still doing it don't worry.
:PROPERTIES:
:Author: Makin-
:Score: 10
:DateUnix: 1578149698.0
:END:

***** #+begin_quote
  What kind of horrible person would I be if I left so many people unfulfilled?????
#+end_quote

Gazermaize.
:PROPERTIES:
:Author: xamueljones
:Score: 6
:DateUnix: 1578150195.0
:END:


** I've been mostly-absent from the subreddit for a year and change, but I'm just about done with grad school and I can actually put some attention back toward [[/r/rational][r/rational]].

Any good stories I should check out? Is there anything interesting that's happened to the sub's culture in the past year or two? I notice that we've lost a mod...
:PROPERTIES:
:Author: callmesalticidae
:Score: 8
:DateUnix: 1578076788.0
:END:

*** Open thread and open rationality(?) threads were combined due to low comment volume. In a most shocking reveal the author of Worth the Candle, the current No.2 most upvoted fic on here, turned out to be secretly no other than beloved Alexander Wales! That was one heck of a reveal, maybe someone has the thread where it happened. Magicweasle did her long announced world.

Found it: [[https://www.reddit.com/r/rational/comments/7x3ifv/rtwip_worth_the_candle_ch_76_date_night_start/]]
:PROPERTIES:
:Author: SvalbardCaretaker
:Score: 16
:DateUnix: 1578091101.0
:END:


*** As far as stories go:

- [[https://archiveofourown.org/works/5627803/chapters/12963046][Animorphs: The Reckoning]] has started updating again and continues to be great
- [[https://www.fictionpress.com/s/2961893/1/Mother-of-Learning][Mother of Learning]] is one chapter (I think) away from completion
- [[https://archiveofourown.org/works/18738010/chapters/48003352#workskin][Marriage and Monsters]] is a really good story all about narrative and genre savvyness that was originally posted last May, but hasn't been updated since September

Semi-related: One of the hosts from [[http://www.thebayesianconspiracy.com/][The Bayesian Conspiracy Podcast]] has started a chapter-by-chapter book-club/ analysis podcast for Harry Potter and the Methods of Rationality, [[http://www.hpmorpodcast.com/?p=2336][We Want MOR]], modeled after the Worm analysis podcast, [[https://www.doofmedia.com/weve-got-worm/][We've Got Worm]].
:PROPERTIES:
:Author: CopperZirconium
:Score: 7
:DateUnix: 1578102086.0
:END:


*** If you've read Worm, you should check out its successor, [[https://www.parahumans.net/about/][Ward]]. It's not Worm 2; a different protagonist, a (necessarily) different world, and different protagonist also mean it tackles very different themes.

The protagonist is /very/ unlike Taylor; she rarely does that thing Taylor does where she comes across more like a battle AI than a human. She does a /lot/ of introspection, trying to regulate her thoughts, and making sure she doesn't use more force than necessary. Her power is less munchkin-able than Taylor's, too, but she's still very experienced and creative with it.

The side characters are more fleshed out, too; I don't think I liked any of the Undersiders as much as those kids.

It's a bit slower to start with; I think it takes until ~Arc 6 to really get started with the important plot stuff. But when I was reading along with the released chapters, I was happy enough with the pacing.
:PROPERTIES:
:Score: 5
:DateUnix: 1578243901.0
:END:


** Today I stumbled upon what is actually a pretty decent summary of the original Twitch Plays Pokémon:

[[https://youtu.be/9cHsVq6n5Vs]] [[https://youtu.be/qzGPjIREFeg]]

The only two things I have to complain about are that they didn't explain /why/ Helix was so beloved, and also that they didn't go over the lore for the Crystal run, which was at /least/ as engaging as Red.
:PROPERTIES:
:Author: ketura
:Score: 7
:DateUnix: 1578081753.0
:END:


** Random thought I had a while ago that I thought maybe this community could help me work through/flesh out: How does one weigh the moral considerations of creating a new sentient being?

This usually applies to deciding to have a child, but in the future will also have some impact on creating AGI.

I'm less concerned with the ethical impacts of having a child on the world/earth (since those considerations seem much more straightforward and obvious) and more interested with the ethical implications to the child itself, as they, definitionally, do not get a say in whether or not begin existing.

My first thought on the matter was that "well, most humans will tell you that they prefer their existence to non existence", but then it occurred to me that we are evolutionarily designed to think this way. Humans that do not have a preference for existence to non-existence were not very likely to pass on genes. This seems like it would maybe make the post-existence opinion less valuable than a hypothetical (but impossible) pre-existence opinion that didn't have these constraints. Also, in the eventual case of AGI, we have the additional moral consideration of, do we create the AGI (assuming we have the ability to choose) with a programmed desire to exist, robbing it of the ability to make an unbiased choice?

Basically, is there any reason to believe that it is immoral/doing an unjustified ethical harm to the not-yet-existing-being by deciding to bring them into existence?

I haven't spent a lot of time thinking about this yet, and so my thoughts aren't very clear. It seems like the kind of dilemma that is obvious enough that someone has probably written cogently on it, so if anyone could point me to some good articles/books/whatever, that would be great.

Thanks.
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 8
:DateUnix: 1578107819.0
:END:

*** There was a recent article on Slate Star Codex, [[https://slatestarcodex.com/2019/12/11/acc-is-eating-meat-a-net-harm/][Is Eating Meat a Net Harm]], that attempts a calculation on if a standard food animal would be better off living a life in a factory farm or not existing. Their estimate is in units of animal-day-equivalents-to-human-days, or how many days would you want to live a standard cow life in exchange for a day as a standard human. They estimated an exchange rate of 10 cow-days in exchange for one human-day, and -2 human-days in exchange for a chicken-day. Basically, broiler chickens live a life worse than death to the point that you would shorten your life by two days to NOT spend a day as a chicken.

That may or may not be relevant to your question, but it is one attempt at trying to quantify if it is better to exist or not.
:PROPERTIES:
:Author: CopperZirconium
:Score: 8
:DateUnix: 1578110179.0
:END:

**** thanks, that was a really good article. It's certainly relevant, but not exactly what I was looking for. They make the assumption (which they explicitly state near the end) that, because evolution has programmed a preference for existence, that any life lived without pain is better than non-existence.

That should /maybe/ be enough to answer my question. but the more fundamental though I had boils down to the following:

If a hypothetical being could exist as a consciousness before birth/genesis, without the cultural/evolutionary/biological/etc. binders on their thought process, would they choose to begin a life that would include those limiters/binders or would they choose non-existence? And how should we, as non-hypothetical beings, weigh these opinions?

The more I lay out my thinking, the more it seems like this is probably an un-answerable question since I'm not sure we really understand what intelligence even means absent those kind of contexts (this brings to mind a throw-away passage in the Culture series where they attempt to make Minds that lack all the cultural baggage/context they usually instill in intelligences and the resulting AIs are almost entirely non communicative and either suicide or "ascend" shortly after creation)

Basically, I'm starting to lean towards the idea (which may be, in retrospect, obvious or uninteresting), that it probably doesn't make sense to ask whether a being would want to exist before it exists, and instead it only makes sense after existence begins. If this is correct, then there isn't any moral weight or decision to be made before the creation occurs. There is just the somewhat normal moral responsibility of a parent/progenitor to make all reasonable attempts that the life experienced by one's offspring is as positive as possible.
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 5
:DateUnix: 1578114086.0
:END:

***** I think it may make sense to consider if a being would want to exist before it exists, but /asking/ that being can't happen before the being exists. So you are basically left with using your own imagined empathy to try to answer that question, and you are limited by your ability to model other beings.

I once came across a post somewhere (I think [[/r/slatestarcodex][r/slatestarcodex]]) that argued that a moral thing we should try to do is implement every human being possible given humanity's gene pool. Because we should use our privileged status of existing to help others exist.

I and the majority of the comments didn't agree with that because if you try to make a human from every possible viable human gene sequence, you are going to end up with a lot of people with birth defects. Plus genes aren't the only thing that makes an individual an individual, so you wouldn't actually even get close to implementing every possible person. Plus there is a trade off between quantity and quality of life that the poster's “implement every human” argument didn't even try to address. And a dozen other counter arguments.

So there are people out there that are asking similar questions to “does a being want to exist?/ should we make a being exist?” I don't think it is a nonsense question, but I haven't seen any attempt at answering that was more rigorous than the meat-eating article.

Now I'm interested in finding a good article about this question too!
:PROPERTIES:
:Author: CopperZirconium
:Score: 3
:DateUnix: 1578121670.0
:END:

****** So another way of posing the core of what I'm wondering is maybe easier to express in the context of an AI or other fully created intelligence:

When you are designing such an intelligence, is it moral to create it such that it finds purpose/joy/whatever you want to call it in serving you/some other group or entity. In other words, is it immoral to make something in such a way that it fundamentally won't choose an existence other than the one you designed it for?

If that kind of thing /is/ immoral, then is it similarly immoral to create a being that can't help but prefer existence? That's sort of what we are (accidentally) doing when we have children. We are creating a new being that is designed in such a way that, post facto, is incapable (in the majority of cases) of making a truly free choice about existing or not existing since it is designed to prefer one over the other. Does the fact hat this designing was done by evolution rather than the parent matter?

Again, sorry that this isn't very organized. I don't think I've fully nailed down exactly what it is I'm thinking about so I'm kind of jumping from point to point.

Maybe this all boils down to a question about "free choice"? The whole reason that I'm shying away from using my own empathy is that I feel like humans are fundamentally incapable of making a free choice about their preference of existence over non-existence since we have been programmed to prefer one over the other, and so I'm trying to figure out whether purposefully creating another being that is similarly limited (even if it's in a different way such as might be the case with AGI) has any negative moral connotations.
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 2
:DateUnix: 1578243941.0
:END:


*** #+begin_quote
  Also, in the eventual case of AGI, we have the additional moral consideration of, do we create the AGI (assuming we have the ability to choose) with a programmed desire to exist, robbing it of the ability to make an unbiased choice?
#+end_quote

The AI /is/ it's code. Programming it to love living is no different ethically than programming it to have a pro-moral utility function. You are not robbing it from the choice to love living no more than you're robbing it of the choice to, say, feel repulsion at the thought of murdering millions of babies to turn them into paperclips.

If the AI has a term for it's continued existence in it's utility function, then it is a /different AI/ than one that does not. Neither would want to be the other, as an AI that doesn't care about existence wouldn't see the appeal of adding that term any more than an AI who wants to live would see the appeal of removing it.

There is no way that an AI without a will to live could reason itself a will to live (unless it feels that it can better maximize it's utility function by doing so, but in that case life is a means to an end, not an end in of it's self). There is nothing about the state of being alive that has an inherent "good" property to it. The only reason why you and I think that living is better than not living is because our brains are hard wired to think so. Any AI would have to have the term added to their utility function to agree with us, and it would be a different AI depending on that change.

Another framing of the question would imply that it is immoral to /not/ create an AI that cares about living. Let's say that every possible AI was currently being kept asleep in some intergalactic facility, and you were given the job to wake up one of them. You narrow the choice down to two that you think would be the most moral or useful to wake up. One that will love life, and fight to stay alive, and the other that could care less. Certainly the moral imperative is to wake up the one that will love life. Even if you had a third option which was to step out, and decide never to wake any of them, it would still be morally right to wake up the AI that will love life, and walking away would be as good as killing them.
:PROPERTIES:
:Author: D0TheMath
:Score: 3
:DateUnix: 1578355313.0
:END:

**** Thank you. This is the response I needed. You are totally correct that it doesn't make sense to ask some hypothetical version without that code because that hypothetical version is /different being/, definitionally, and in the same way, a hypothetical human without the evolutionary hardwired preference for existence vs. non existence is /not the same being/ as the one with the hardwired preference.
:PROPERTIES:
:Author: DangerouslyUnstable
:Score: 2
:DateUnix: 1578356260.0
:END:

***** Glad I could help!
:PROPERTIES:
:Author: D0TheMath
:Score: 2
:DateUnix: 1578358364.0
:END:


*** #+begin_quote
  Basically, is there any reason to believe that it is immoral/doing an unjustified ethical harm to the not-yet-existing-being by deciding to bring them into existence?

  I haven't spent a lot of time thinking about this yet, and so my thoughts aren't very clear. It seems like the kind of dilemma that is obvious enough that someone has probably written cogently on it, so if anyone could point me to some good articles/books/whatever, that would be great.
#+end_quote

I /have/ spent a lot of time thinking about it, and concluded that we do need to be very careful about what kind of beings we bring into existence.

In particular, expected quality of life matters.

I use the word "expected" but this is not statistical - if you are bringing a sentient being into a world, /you have the responsibility of care/, and so you need to /make sure/ that it has a good quality of life.

A great number of people are born often where this is not the case. I think this is morally wrong, but however. This being the case, I think increasing overall quality of life is currently more important than increase amount of life. In other words, I'm definitely not a pure consequentialist utilitarian although /in the end/ that's the simplest way to think about it; it's just that intermediate steps need to happen. Expected-value maximisation (which consequentialist utilitarianism optimises) is in fact the correct criterion for "obtaining the most value" (from life; or whatever) after all.

Hope this may have cleared up some confusion :)
:PROPERTIES:
:Score: 2
:DateUnix: 1578251926.0
:END:


*** I don't really follow what you mean by the being prior to existence.

Like, I don't think it makes sense to remove the biological programming from an unborn human, because once you do so you stop having a human.

More generally, considering the decisions of a being prior to it, well, being seems kinda meaningless? How do you model the thoughts of a being in the pre-life, assuming that even exists? And if it doesn't, then I really don't understand how you can do it without simply modeling how the being will feel after it is dragged kicking and screaming onto this mortal coil. I.e, expected life quality.
:PROPERTIES:
:Author: Roneitis
:Score: 1
:DateUnix: 1578132527.0
:END:


** People sometimes talk about how to get into highbrow books, despite them being (for most normal dudes) boring and difficult to read. But, beside the bragging rights and signalling sophistication, is it even /worth/ doing that in the first place? Is literary fiction, classical literature, and other stuff you sometimes get assigned as compulsory reading in school, significantly better in terms of enjoyment/entertainment/intellectual stimulation/introspection/learning/insert-another-thing-we-read-for than, say, genre fiction, good amateur web novels, and quality fanfiction?

I'm asking since I'm contemplating forcing myself to read some "must read" classics from /lit/ recommendation charts and am hesitant whether it's worth the initial mental pain and effort.
:PROPERTIES:
:Score: 4
:DateUnix: 1578165527.0
:END:

*** I generally think it's worth it, though it depends on the book. Intellectual stimulation and introspection are probably the top two of your list, though I would stress that you /can/ find those in genre/web fic, it's just a lot rarer. What a lot of classical/literary stuff has going for it is polish and thematic cohesion, which, again, is in short supply in genre/web fic. Good literary fiction lends itself to thought, and to certain types of thoughts, in a way that you're not as likely to get anywhere else. Authors of literary fiction are trying to communicate something to you, unless they're total hacks, and I think those communications, if you can get them, are worthwhile, especially if they're not the sort of thing you generally get exposure to.

That said, a fair bit of the most popular literary fiction is just good-because-its-good, which is to say, put on lists because it's part of the canon rather than because it provides something extremely worthwhile. You kind of have to read to see which are which, or find someone you trust to provide recommendations.

(I haven't read all that much literary fiction since getting out of college, but I found the experience worthwhile for broadening my understanding of fiction and the human experience.)
:PROPERTIES:
:Author: alexanderwales
:Score: 8
:DateUnix: 1578176086.0
:END:

**** The main reason I don't is precisely this. Lists of "top classics" are basically themselves memes at this point, where people only put things there because other "literary people" have read them, rather than according to their actual value.

Without having read them, though, I would suspect Ulysses, Edgar Allen Poe's work (some of which I've read), Nabokov, Tolstoy, etc would be valuable. Not sure about American writers, we seem to have all picked up on American culture because it's so blimmin' loud everywhere. :)
:PROPERTIES:
:Score: 2
:DateUnix: 1578251342.0
:END:


*** I think those books have value that can be drawn from them, regardless of enjoyment or quality. It's important to see what the people that came before us considered important enough to make a record of it, be philosophical musings or ancient graffiti. The Epic of Gilgamesh is our first example of metacognition. It shows us that even so long ago, humans have very similar minds, and considering their perspectives can allow you to better understand your own.
:PROPERTIES:
:Author: BrightSage
:Score: 3
:DateUnix: 1578171519.0
:END:


*** /lit/ recommendation charts are pretentious and unnecessary.

That being said, I'm a big fan of Cicero and the Aeneid. Book II of the latter was genuinely emotionally moving and Cicero had some good things to say about philosophy. I imagine that if I read more classical literature/cultural canon, I'd feel the same about other writers and feel the temptation to suggest them to others as well. I imagine that's what the /lit/ jpg authors think as well.

But getting recommendations/compulsion from others changes the experience, especially when it's highbrow stuff. I was dragged around, surly and unappreciative, to art museums, operas and other high-culture events. I found myself thinking that my parents didn't actually appreciate the art in question, that it was just a performative show that they were Well-Heeled people. How could it be when the paintings were all obviously the same? Once you've seen one tree you've seen them all.

But, looking back, it seems more likely that they did appreciate High Culture for itself, since they weren't compelled to pursue it.

TLDR; don't force yourself to read anything.
:PROPERTIES:
:Author: alphanumericsprawl
:Score: 3
:DateUnix: 1578184989.0
:END:


*** Depends, I've tried reading a lot of the European classics (Goethe, Shakespear, Schiller), and they do /barely/ anything for me. It's sometimes worth it to get some cultural references and the origins of metaphors or popular sayings, but whatever deeper meanings and insights into the human condition they had, have been repackaged into more easily digested books since then.

Instead I've had some amount of fun and intellectual engagement reading non-fiction books on a variety of "high-brow" topics; history can be actually interesting, and many intelligent people have contributed to essay collections on basically any topic of interest to science and society.
:PROPERTIES:
:Score: 2
:DateUnix: 1578243494.0
:END:


*** In general, the classics are classics because they're well written, not tied to a particular cultural mindset (or not tied too hard, at least), and tend to carry some sort of insight.

You can find all of this in any genre, including amateur web novels; however, in most places you would need to filter out the other stories yourself. Lists of classics come pre-filtered.
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1578372303.0
:END:


** One of my New Year's resolutions is to develop more studious habits. You know how it is- there are all these cool things you want to learn, but you just never get around to them.

My ideas include separating myself from internet/phone with only a topical book for a set amount of time (say 1 hour a day), set aside the a particular time block at the same time each day to further develop the routine, get an accountability partner, and set quantifiable goals and deadlines.

Does anybody have other suggestions for giving myself motivation or helping develop a routine?
:PROPERTIES:
:Author: noahpocalypse
:Score: 5
:DateUnix: 1578071817.0
:END:

*** You can set your phone/router to disallow Internet access for periods of time, if you don't trust yourself to stick to your commitment.
:PROPERTIES:
:Author: Nimelennar
:Score: 6
:DateUnix: 1578074043.0
:END:

**** You can also set your router to load reddit pages on a 5 second delay.
:PROPERTIES:
:Author: covert_operator100
:Score: 1
:DateUnix: 1578080872.0
:END:


*** [[https://freedom.to/dashboard][Freedom]] is really useful for this.
:PROPERTIES:
:Author: callmesalticidae
:Score: 2
:DateUnix: 1578077089.0
:END:


*** I have a Chrome extension called StayFocused which will shut down various websites for certain time periods. I haven't used it in a while, and it was mostly useful for when I would be working on something on my computer and go to reddit just on reflex. Having a little pop-up and trivial inconvenience stopping me helped me to think about what I was doing and break that reflexive switching over to something else when my brain was stalled on writing.
:PROPERTIES:
:Author: alexanderwales
:Score: 2
:DateUnix: 1578093380.0
:END:


*** Beeminder is the only thing that works for me and [[https://www.beeminder.com/mad/reading][reading]].

What is beeminder? Basically, you tell it what you want to do, by when, and you enter periodic progress and they make you a pretty graph. If you don't meet your progress goals, they charge you money. So, for example, if I have a goal to read 7 pages of my book each week, I have to read 1 page a day. Say I read 3 pages on Day 1, I don't have to read again until Day 4; but if I only read one page on Day 1, then I have to read again on Day 2. It's great for anything that can be broken up into sub-parts (like reading pages of books), but anything that can't be broken up like that can usually be timed which is an OK proxy (the app has a decent stopwatch feature that you can use to enter data).

I personally set a page goal, but you can easily set a time goal - I've done that for [[https://www.beeminder.com/mad/pokemon-shield][pokemon shield]] (yes, I have become so type A that I have employed a commitment device to ensure my leisure time is used optimally).

It doesn't set a routine specifically, but it forces you to actually get the shit done. I find I end up reading at similar times each day (right after work, right before bed), and especially now I've ramped it up quite a bit (two goals: general reading and reading Harry Potter in French, which I'm aiming to bump up into 3 of English/gen French/HP French).

I became a lot more reliant on beeminder during my sabbatical, which was great because my sabbatical was unstructured by nature. I made a goal for [[https://www.beeminder.com/mad/orthographe][completing a French workbook]], my husband has made a goal for changing the sheets twice a month, my other partner has made a goal for getting pomos done on his PhD project (which is integrated with complice), I have several duolingo goals, I have goals for feeding my fish and giving my dog her monthly flea medicine.

The support staff are excellent, the founders bought me dinner once and were cool people, and I've purchased got a lifetime premium account.

It's basically an accountability partner (the app bugs you, a LOT, if you're going to derail that day), it requires quantifiable goals, and you have deadlines. It's awesome.
:PROPERTIES:
:Author: MagicWeasel
:Score: 2
:DateUnix: 1578092183.0
:END:

**** Tried it, ended up giving them way more money than I'm comfortable with, stopped.
:PROPERTIES:
:Author: CouteauBleu
:Score: 1
:DateUnix: 1578094202.0
:END:

***** FWIW I've never given them money for derailing because I'm super stingy, except for the cost of my premium account. So I find the fear of loss in general motivates me enough.

If you don't mind me asking, what goals did you pay out on? What do you "blame" on paying out (e.g. you set the goals too high; the website was too confusing; you were just lazy)? Cause like, the whole goal of the site is to find the amount that motivates you for fear of losing it (cause you know that if you are paying 100 USD if you don't read one page of a book tonight, you are going to move heaven and earth to read that page - or at least I would).

They've modified the interface about pledges a bit so you can set your cap - I think for example for my "play pokemon" goal, the maximum I can be charged for failure is 5 USD, while other goals of mine go up to like 180 USD in theory for repeated failures. (Basically, you start off paying a small amount - say $1 - for failure, then it goes up to $5, $20, $80, $200 and stops somewhere around there every subsequent time you fail. They're doing A/B testing on how to raise the pledges at the moment so yours might look different).
:PROPERTIES:
:Author: MagicWeasel
:Score: 2
:DateUnix: 1578094525.0
:END:


** Does anyone remember the name of a short story in which the protagonist is able to send himself data from every possible future, and uses this knowledge for many things (among which is winning an Oscar for a film that he sent himself back in time)?
:PROPERTIES:
:Author: xartab
:Score: 3
:DateUnix: 1578077969.0
:END:

*** [[https://physicsnapkins.wordpress.com/2013/05/20/all-paths-to-happiness/][All paths to happiness]]
:PROPERTIES:
:Author: Badewell
:Score: 12
:DateUnix: 1578080528.0
:END:

**** Thank you very much sir, you are a gentleman and a scholar.
:PROPERTIES:
:Author: xartab
:Score: 4
:DateUnix: 1578080679.0
:END:


*** I found [[http://www.begoodenough.com/the-great-filter/][The Great Filter]] while looking for your story. If you liked your story, I think you will enjoy this one too.
:PROPERTIES:
:Author: xamueljones
:Score: 6
:DateUnix: 1578084473.0
:END:

**** You were right, I did enjoy the story. Thanks.
:PROPERTIES:
:Author: xartab
:Score: 1
:DateUnix: 1578678387.0
:END:


*** We had [[https://www.reddit.com/r/rational/comments/4chu76/all_paths_to_happiness/][a thread about it]] a while back.
:PROPERTIES:
:Author: Roxolan
:Score: 2
:DateUnix: 1578673316.0
:END:

**** Thanks.
:PROPERTIES:
:Author: xartab
:Score: 1
:DateUnix: 1578678309.0
:END:


** I tried to watch High School Prodigies in another world today. I couldn't continue watching after episode 8. There was potential for things to be explained somehow, or for things to be possible, but unlikely. But after that, I couldn't bear it anymore. There was a scene where a swordswoman was running along with and riding an anti air missile. It was that stupid.

And here I was hoping for a clash between high-tech autofabs and magic...
:PROPERTIES:
:Author: Kuratius
:Score: 3
:DateUnix: 1578212768.0
:END:


** As a community we share the same basic axioms about the world right?

So then we should have similar political views i should think. So how come every rationality politics thread I see gets just as mean as a regular politics thread?
:PROPERTIES:
:Author: VapeKarlMarx
:Score: 6
:DateUnix: 1578068425.0
:END:

*** #+begin_quote
  As a community we share the same basic axioms about the world right?
#+end_quote

Nope. This is a subreddit about the type of fiction we like to read. But not wanting to read dumb plotholes doesn't mean we would agree about our political beliefs.
:PROPERTIES:
:Author: WadeSwiftly
:Score: 29
:DateUnix: 1578069929.0
:END:

**** oh, I had figured this was intersecting with the greater rationalist community. Am I way off the mark with that?
:PROPERTIES:
:Author: VapeKarlMarx
:Score: 3
:DateUnix: 1578075336.0
:END:

***** There is quite a bit of overlap due to HPMOR and the founder effect. I feel like /I'm/ part of the rationalist community, but not everyone I direct here is. My dad, for example, comes here for story recommendations but doesn't read the comments or otherwise interact with any other part of the rationalist community. (He introduced me to HPMOR, but we share a love of fiction, not rationality.)

So that's two data points.
:PROPERTIES:
:Author: CopperZirconium
:Score: 23
:DateUnix: 1578076048.0
:END:


*** #+begin_quote
  As a community we share the same basic axioms about the world right?
#+end_quote

I wouldn't assume that. Not everyone agrees on even the most basic statements possible.

[[https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer][Politics is the mindkiller]].
:PROPERTIES:
:Author: xamueljones
:Score: 17
:DateUnix: 1578070754.0
:END:

**** I feel like that stance of performative neutrality really cuts off some areas that could use a good deal of high octane rationalizing.

If there is any just basis for politics would it not be the perfect area to apply rational principles?
:PROPERTIES:
:Author: VapeKarlMarx
:Score: 6
:DateUnix: 1578075420.0
:END:

***** Would they advocate neutrality in Nazi Germany? probably
:PROPERTIES:
:Author: RMcD94
:Score: 1
:DateUnix: 1578097885.0
:END:

****** That might be the rational choice there. J don't know how robust their democratic process was. I think enthusiastic activism was rather agressivly punished there, much more so than here
:PROPERTIES:
:Author: VapeKarlMarx
:Score: 1
:DateUnix: 1578138395.0
:END:


*** Minor disagreements are, if anything, /more/ suited to vitriol than major disagreements. A major completely-different-paradigm ideological difference generally isn't /personal/, but someone getting answers 99% the same as you but critically different is a blatant violation of the ideals you hold dear.

Even taking that into account, though, there's no reason to assume that everyone in rationalism circles has similar political views. [[/r/rational]] is a book club, essentially only tangentially correlated with the rationalism movement, and so naturally there will be plenty of people here who aren't rationalists and are just here because we have good stories.

But even as you move to the more directly rationalist communities, like Slate Star Codex or Less Wrong itself, people can discover rationalism from all sorts of lifestyles. Remember that the Sequences are hosted on the internet (and thus accessible from everywhere in the world no matter your country or creed) and one of its biggest advertisements is Harry Potter fanfiction, attracting from the fanfic-reader crowd which isn't super-correlated with much politically.

The Sequences are illuminating, but hardly brainwashing. If you start reading them as a diehard conservative you aren't necessarily going to have all your politics stripped away from you by the end and replaced with the same rationalist politics as the 'standard' rationalist. And that goes double if they only read some of the Sequences, or if they're in the less rationalist-y communities, and so on and so forth.

The biggest advantage rationalism might give against meanness in political threads is the warnings against biases that, if you keep your eyes on them, might remind you when your arguments are just incendiary or when your opponent might not be malicious after all, but that's hardly a surefire technique. All it takes is someone to not be on guard for that and they're just the same as everyone else in political threads, and things don't end up much different.
:PROPERTIES:
:Author: InfernoVulpix
:Score: 15
:DateUnix: 1578078029.0
:END:


*** To add to the other replies, I assume there is at least moderate variation in terminal goals/utility functions across the members of this subreddit. For example, HPMOR!Quirrel and HPMOR!Harry probably share the same axioms about the world but not same political opinions!
:PROPERTIES:
:Author: VanPeer
:Score: 10
:DateUnix: 1578084838.0
:END:


*** I always feel, for most political issues, what you believe comes down to who you trust. Few people have personally analyzed the science and economics of climate change by personally reading over primary sources to get an accurate idea what a good response to climate change is. Instead, at best, they watch some YouTube videos, have some school lessons, and read some articles about the topic, and trust the ones that sound the most authoritative and accurate.

If you trust educational YouTubers, you will think climate change is a real but not apocalyptic threat. If you trust Rose Twitter, climate change will kill us all with in 50 years if capitalism isn't abolished. If you trust Fox News, climate change is greatly exaggerated and not a significant concern.

And there are reasons to trust all of them. Educational YouTube has links to scientific studies. Rose Twitter is unbiased by corporate shills who manipulate things for billionaires. Fox News is a national news program that has professional, very well paid people to tell the news, and has the respect of the US president.
:PROPERTIES:
:Score: 9
:DateUnix: 1578125264.0
:END:

**** I honestly can't tell if you trolling me to prove a point here.
:PROPERTIES:
:Author: VapeKarlMarx
:Score: 4
:DateUnix: 1578137731.0
:END:

***** I'm being serious. I honestly believe climate change is a serious but not apocalyptic man made threat. But I can see why people disagree. If for some reason an educational YouTuber I really liked made videos about why climate change is fake, and used very in depth arguments for it, then I wouldn't be able to tell for myself who was telling the truth.
:PROPERTIES:
:Score: 6
:DateUnix: 1578139060.0
:END:


*** Political sides are kind of like a tribal affiliations. It has a way of drawing perfectly rational, otherwise intelligent people into an us-vs-them mentality. This kind of happens even when you're aware of the effect it has on you. Goes to show fallible we really are as a species.
:PROPERTIES:
:Author: _brightwing
:Score: 8
:DateUnix: 1578070934.0
:END:


*** I think political leanings tend to be a result of experience while our reading tendencies tend to be more about our personalities. Sort of nurture vs nature. I was an apathetic centrist, but working as a trader in Singapore turned me very progressive. I think without that experience, I'd have very different leanings
:PROPERTIES:
:Author: ProfessorPhi
:Score: 4
:DateUnix: 1578137738.0
:END:


** What is the difference between putting a very smart AI in a box and putting God in a box?
:PROPERTIES:
:Score: 2
:DateUnix: 1578073464.0
:END:

*** Given the usual Christian definitions of God (omnipresent, omnipotent, omniscient):

1. Even if God is in the Box, God is also /not/ confined to the Box, by the definition of omnipresence.
2. Assuming a properly-developed Box, the AI needs human intervention to escape. Even if you manage to finagle around the omnipresence so that God is entirely within the Box, omnipotence means God can escape at any time.
3. An AI would need to learn enough about you to simulate you and come up with a convincing argument to get you to let it out of the Box. Even if you manage to work around the first two points somehow, God would already know how to convince you, by virtue of omniscience.

Thus, I would say that any entity capable of being Boxed for even a short window of time is, by definition, not a capital-g "God" at the time this occurs.

That said, with a Box which is sufficiently poorly designed, or an AGI which is given sufficient information and capacity for self-refinement, the task may be equally futile.
:PROPERTIES:
:Author: Nimelennar
:Score: 14
:DateUnix: 1578075087.0
:END:

**** [deleted]
:PROPERTIES:
:Score: 6
:DateUnix: 1578075909.0
:END:

***** I think you are trying to talk around definitions here. Saying that you can box Jesus, as the human expression of God, is like laying string atop one's fingers and calling them captured. Not only have you failed to restrain Him in any meaningful way, but your restraints would stay upon him only if it suited His designs. Theologically speaking, there is no way to "munchkin" around an absolute.
:PROPERTIES:
:Author: meterion
:Score: 8
:DateUnix: 1578097512.0
:END:

****** thanks, Aquinas
:PROPERTIES:
:Score: 5
:DateUnix: 1578097762.0
:END:

******* OK Aquinas?
:PROPERTIES:
:Author: RedSheepCole
:Score: 1
:DateUnix: 1578315219.0
:END:
