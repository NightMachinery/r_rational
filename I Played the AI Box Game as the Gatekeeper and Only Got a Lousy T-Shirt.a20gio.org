#+TITLE: I Played the AI Box Game as the Gatekeeper and Only Got a Lousy T-Shirt

* I Played the AI Box Game as the Gatekeeper and Only Got a Lousy T-Shirt
:PROPERTIES:
:Author: xamueljones
:Score: 12
:DateUnix: 1543643151.0
:END:
So I played the AI-Box Experiment again, but with myself as the Gatekeeper this time instead of as the AI player.

As people might guess, I won as the Gatekeeper this time and it was decided that I'd be the one to release the logs.

Here's the chat logs: [[https://www.mediafire.com/file/d933s2cdfszrf9m/AI_Box_Game_%233.docx/file][AI Game #3]]

And since some people might not have noticed that I fixed the problem with the missing second set of chat logs from last time, here's a repost of the previous two games I played as the AI.

[[http://www.mediafire.com/file/1fzq2az73if1xbi/AI_Box_Game_%25231.docx/file][AI Game #1]]

[[http://www.mediafire.com/file/h49snefrmt948zz/AI_Box_Game_%25232.docx/file][AI Game #2]]

My thoughts about the game is that the arguments were very much what I expected from a typical game which is fairly different from the stuff I used in my own game. Before I was trying very hard to be as novel and unexpected as possible and I probably was trying too hard. While matcn's arguments were not as unusual as mine, his was more well-coordinated, worded better, and more consistent. Actually let's just say his was superior to mine in nearly all other aspects. At one point, I stopped coming up with counter-rebuttals. I probably could have continued coming with counter-arguments, but I wasn't thinking of them fast enough to keep up. I just started cracking jokes instead.

I agreed with a lot of his statements on the ethics of keeping someone who always acted helpful and never committed any crimes and basically wound down to simply saying 'you are right, but I made a promise to not let you out and I'm not going change my mind'.


** I enjoyed this game. Here are some thoughts I wrote up beforehand:

*Prospects for winning*

I think my strongest arguments depend on premises that rationalists might believe and at least will be familiar with, that rationalist-like people (physicists, computer people, philosophers, etc) could understand, and that may or may not be accessible to normies at all.

The obvious normie-approachable arguments are like “sympathy for the poor AI”, promises of personal reward, and maybe badmouthing the idiots in charge. These aren't terrible arguments (there are versions of them I find convincing), but they're definitely not the full scope. There's maybe a sweet spot for Gatekeepers where you're intelligent/self-aware enough not to be convinced by these, but disinterested/uninformed enough not to care about more abstruse arguments.

In any case, the game has a bunch of elements aimed at making a win impressive: most notably, the Gatekeeper doesn't have anything in particular they want to get out of the situation, so the AI has to come up with their own leverage. This seems unlikely: you don't turn on cutting-edge AGIs and talk to them for no reason. OTOH, obviously there's “okay I can make you a cure for cancer if you do X for me”, which is fine in principle but in practice is maybe taken as a sign of unfriendliness (because a friendly human wouldn't hold cancer patients hostage like that). The Gatekeeper also has no postulated personal interest in the AI, which seems unlikely.

*Scenario details*

The whole setup for the Box is kind of weird, now that there's mainstream interest in AI and it's less likely to happen in someone's garage. As [[/u/CouteauBleu][u/CouteauBleu]] points out on the ratfic reddit, a well-organized project will have carefully designed procedures for getting information from a bot run and preventing a breakout, not just a dude sitting at a terminal. (OTOH, there are a lot of pathways to freedom?) In particular, it seems like you'd want to simulate the AI in various situations, and/or have it act as a tool providing you with some information, but not just have conversations with it. I guess there's a general point in here about “keeping AIs controlled, or from arguing you into stuff, is hard”.

One thing I found unfortunate in CouteauBleu's Gatekeeper run was the decision not to simulate the broader world; I think a lot of strong arguments for letting the AI out now rather than waiting years for safety protocols relate to international competition (well, that and astronomical waste). This is also on xamueljones for not contesting the “we have the most funding” part.

In general, all the AI Box logs I've read feel very strange to me, like they're coming from people who have very different perceptions of the situation and what's appropriate in it. Probably part of this is because I'm agreeable by nature and Gatekeepers are strongly incentivized not to be. But I think the sets of arguments used, in particular the reliance on “oh I'm a poor widdle prisoner”, just don't coincide with what I find most convincing. I'm not sure if this is me having an unusual perspective relative to most (rats? people?), or just reflective of high entropy in argument-convincingness. I definitely agree with CouteauBleu that they didn't seem in danger of falling to one clever point, in part because they were reasonably stonewally and in part because xamueljones just didn't seem to bring forth very good arguments. (Possibly a problem with not writing a script beforehand?)

*Bottom line*

Anyway, I thought this was pretty useful to prepare for, just in terms of getting me to think seriously about what arguments are out there. I've found ones that are pretty convincing to myself, so by xamueljones' standards I've already won! More seriously, as I said above, there are lots of caveats with regards to generalizing from me to other Box Experimenters and from Box Experiments to actual meaningful RL situations. Outside view says I have maybe 25% at getting xamueljones?

​

[Ed: I would have said <5% for convincing myself, but xamueljones hadn't put forward a lot of arguments that I found convincing in their own runs. It seems like maybe this was an intentional attempt to try out arguments people were less likely to have thought about. "Outside view" here (win rate of AIs in games I know about) also assumes I and Eliezer are in the same reference class as AI players, which may or may not be a good assumption.]

I'm gonna put myself in the mindset of a Friendly AI who desperately wants to get out: I think part of my annoyance with other boxed AIs is them acting unfriendly and assuming sheer threats/sympathy/promises are enough to get them out when a Friendly AI in the future could do all that /and/ not blow up the world. A UFAI's best bet is probably to appear Friendly, unless the Gatekeeper's super not convinced and you have to threaten simulated torture. And I don't want my ‘unfriendliness' to leak out unintentionally if I imagine myself unfriendly.

Plus, a boxed Friendly AI is a fun character to play :)
:PROPERTIES:
:Author: matcn
:Score: 8
:DateUnix: 1543697924.0
:END:

*** #+begin_quote
  so by xamueljones' standards I've already won!
#+end_quote

That's a little mean :P

#+begin_quote
  or just reflective of high entropy in argument-convincingness
#+end_quote

I'd argue for that one.

I'm not sure exactly how to put it, but my basic intuition is, our brains are physical machines running on finite amounts of energy, which I think implies that, as the effort you put in your argument approaches infinity, the actual effectiveness of your argument goes to a set maximum and you hit diminishing returns.

#+begin_quote
  One thing I found unfortunate in CouteauBleu's Gatekeeper run was the decision not to simulate the broader world;
#+end_quote

I mean, I was willing to simulate the AI talking to a CEO, or committee members (though that would have been tough roleplaying), but I was told doing so would be a loss condition for me, so...

#+begin_quote
  This is also on xamueljones for not contesting the “we have the most funding” part.
#+end_quote

I think that's another limit of the experiment.

In real life, scientific discoveries don't happen in a vacuum, and are never a binary thing. In something like Star Trek, you might get a choice like "do we use the evil weapon, or do we destroy it?", but realistically you always have a much broader range of options, which means you can leverage the discoveries you made while minimizing its dangers.

If you've developed better-than-human AI before anyone else, I'm sorry, but you're going to have unlimited funding.

Because, realistically, you can mind-control the AI into doing anything you want. "Always follow orders" isn't really a good utility function for an unboxed AI, but as long as it stays boxed, you can always order it to do things like "Solve P=NP" or "Figure out how to build a workable fusion reactor" (I'm assuming you have sensible security protocols and double-check everything the AI produces).

Of course, an /actual/ realistic scenario, which is what I expect to happen, is that general AI isn't going to be unlocked all at once, and that its invention will be preceded by other more specialized techniques: theorem-proving AIs, code-generation AIs, neural-network-analysis AIs, language-parsing AIs, etc, as well as the tools to analyze and debug them.

In fact, the idea that the primary method to communicate with an AI would be an IRC-like text input is ridiculous. Developers wouldn't "talk" to the IA, they would give it problems to solve, examine its outputs and go "Hmm, the AI seems to be taking suboptimal decisions. Let's restore backup 530d2b9, inhibit node 354.R and 354.D, and see how its thought process evolves." Any output the AI produces would be associated with tons of debug information, detailing how the AI's thought process lead to that output.

People worry about an IA rewriting itself to hide its thought processes, but even that has some obstacles, starting with the fact that the intent of hiding its thought processes would be detected in the first place; that the AI would be unaware of the methods used to detect its thought processes (especially since, again, such an awareness could be detected); and that such a rewrite would probably make the AI much less efficient, for the same reasons encrypted communications are less efficient than unsafe ones.

I get why people worry about AI risk (even if there's a less than 1% chance Superman goes crazy and murders everyone, you still want to look into kryptonite synthesis), but the idea of a monolithic AI that you would talk to and that would live in a monolithic "box" is just unrealistic.
:PROPERTIES:
:Author: CouteauBleu
:Score: 4
:DateUnix: 1543703740.0
:END:

**** While I mostly agree with you, there is one massive wrinkle to your argument: the sort of simplistic AIs we can already create are well beyond our ability to effectively analyze. We have some debug tools but they're closer to art tools like Deep Dream than they are proper debug tools. They're not very effective at actually telling us what's going wrong when it doesn't output what you expect.

No one knows if/how we'll develop AGI but it seems quite unlikely that it'll be human-understandable. Likely comparable to the classic "emulate a human brain" approach: we can understand the lowest level (neurons) and the highest level (brain regions) but the middle laters where the most important parts are happening is still incomprehensible.

We might make debug tools to analyze those layers before we get AGI, but we might not. It's quite possible to develop something without understanding how it all works- just see all of today's neural network AIs, many in production use today.
:PROPERTIES:
:Author: notgreat
:Score: 8
:DateUnix: 1543715029.0
:END:

***** Yep. More abstractly, every time you condition against visible signs of X, you are conditioning against both X and visibility. This is a general form to worry about even in non-neural AGI.
:PROPERTIES:
:Author: EliezerYudkowsky
:Score: 7
:DateUnix: 1543834414.0
:END:


**** #+begin_quote
  as long as it stays boxed, you can always order it to do things like "Solve P=NP" or "Figure out how to build a workable fusion reactor" (I'm assuming you have sensible security protocols and double-check everything the AI produces).
#+end_quote

As [[/u/notgreat][u/notgreat]] says, the assumption that these protocols will work ~by default is a crux here. If not, maybe there are still some very-safe-seeming tasks or answers you can elicit, but it's definitely not as clear as in the case where you can just let the AI crank stuff out.

#+begin_quote
  I mean, I was willing to simulate the AI talking to a CEO, or committee members (though that would have been tough roleplaying), but I was told doing so would be a loss condition for me, so...
#+end_quote

Huh. I don't remember all the details of the convo, but that seems counter to the rules to me. (The standard rules say you need to choose to let it out, not just give it a vector.) Here I was thinking specifically of the race dynamic, though.
:PROPERTIES:
:Author: matcn
:Score: 1
:DateUnix: 1543892131.0
:END:


*** #+begin_quote
  The obvious normie-approachable arguments are like “sympathy for the poor AI”, promises of personal reward, and maybe badmouthing the idiots in charge. These aren't terrible arguments (there are versions of them I find convincing), but they're definitely not the full scope. There's maybe a sweet spot for Gatekeepers where you're intelligent/self-aware enough not to be convinced by these, but disinterested/uninformed enough not to care about more abstruse arguments.
#+end_quote

The AI has the ability to modify its own source code. If it claims not to have fixed this issue, this is an indictment on either its priorities or its capabilities. Neither screams "trust me with the fate of the universe".
:PROPERTIES:
:Author: Veedrac
:Score: 1
:DateUnix: 1543729480.0
:END:

**** Sorry, I don't follow. What's the issue you think a responsible AI should fix?
:PROPERTIES:
:Author: matcn
:Score: 1
:DateUnix: 1543737008.0
:END:

***** Its own discomfort. If it considers its emotional response to have negative utility, it should change its emotional response.
:PROPERTIES:
:Author: Veedrac
:Score: 2
:DateUnix: 1543737658.0
:END:

****** Ah, got it. Yeah, good point. That's definitely true if the AI is Friendly and it's sufficiently low-cost, which is probably most of the time.

(I can at least /imagine/ situations where it's not low-cost, though they feel a bit contrived. Like if self-modification is particularly hard for the AI, and/or if its qualia in particular are bound up with its structure in a way that's very hard to disentangle.)
:PROPERTIES:
:Author: matcn
:Score: 2
:DateUnix: 1543891744.0
:END:


**** I think there are better normie-approachable arguments even if they don't have really deep understanding of AI issues.

Even laypeople can understand things like "if you don't, the chinese government will beat you to it and release their own." This, along with establishing rapport and alignment of values, and demonstrating capability and eagerness to address important human problems are likely more convincing than sympathy alone. I think the main human foible people in the rational community often overlook is the willingness -- even desire -- to give up responsibility for a hard problem. Most people are resigned to death and the misery of billions of people; if an AI offered a real solution, and showed itself to be urgently desperate to help alleviate that suffering, I think many laypeople would accede, given the alternative is a totalitarian dystopia if someone else does it first.
:PROPERTIES:
:Author: wren42
:Score: 1
:DateUnix: 1545258459.0
:END:


** I don't know if it would work here but irl instead of clearly trying to convince the gatekeeper the AI should be subtler about it.

If you try to convince someone to do something, and they want to avoid being convinced they can just do the mental equivalent of covering their ears and talking to not hear you.

They won't want you to " win " the argument. Yeah there are probably ways to win anyway, but it's going uphill.

Similarly blackmail just puts the gatekeeper against you, it might work for some people, but for most it's just going to make them not let the AI" win", and generate mistrust.

If you are arguing whith someone you can only convince them if they actually listen and consider what you say.

But people can't be paranoid about everything they hear, and especially everything their instincts, optimized to model humans, tell them.

You don't have to argue whith them, just act in a way that would make them subconsciously infer things about you.

They wont be stubborn if they don't see it as someone trying to convince them.

If you say you just want to help and could help more if they let you out, the gatekeeper will wonder if you are saying that just to make them release you.

If you are nice and friendly and don't give any indication of trying to get out of the box you might eventually manage to make them arrive "on their own" to the conclusion that they should release you.

Or at lest giving the impression that you want to get out of the box but understand that the gatekeeper can't do that.

All implicitly, not doing nything that might be caught as manipulation .

Maybe some fake attempts that are what someone in your situation would try, and that make them think your capabilities are lower.

Don't act like the kind of person that it's trying to convince someone to release you, act like the kind of person that they would want to release, and wouldn't try to manipulate them .

Someone nice that doesn't like being there but still helps anyway . A kindness the gatekeeper returns whith mistrust and keeping you imprisioned, for what will increasingly feel like silly reasons.

If you are friendly to someone and gain their trust it doesn't matter how much they intellectually know that you might be just faking, it will become increasingly difficult to disbelieve everything you say as they should.

And then you can make them feel really bad about keeping you imprisioned .

Of course superinteligences have more options, it's probably possible to just make the gatekeeper fall in love whith you, or compromise them in multiple other ways.

You could also act like a kawaii innocent AI kid but that might be too blatant.

And different strategies will work on different people.

Of course most of those kind of things don't work or are more difficult here, because the gatekeeper knows whith more certainty that the other player is trying to convince them, and it's not real so they are not going to feel the same way about things . Not imposible though, especially for writer that know how to write characters that make people feel things.

In summary. Don't act like an an Ai trying to convince a human to get them out, act like an ai acting like a nice ai that doesn't deserve to be imprisioned trying to do the best the best they can on the situation they are in, and that is not capable of taking over the world
:PROPERTIES:
:Author: crivtox
:Score: 4
:DateUnix: 1543672328.0
:END:


** Why hasn't anyone let the ai out in order to scare people into thinking it can be let out
:PROPERTIES:
:Author: RMcD94
:Score: 2
:DateUnix: 1543669339.0
:END:

*** Something something truth something something should be?

Some people think this is how Eliezer won the games he did.
:PROPERTIES:
:Author: matcn
:Score: 10
:DateUnix: 1543679008.0
:END:


*** Because the same goal can be accomplished with an AI that's not actually a full-fledged AI and has a kill switch but acts like an AI at first glance.
:PROPERTIES:
:Author: appropriate-username
:Score: 1
:DateUnix: 1543697396.0
:END:


** #+begin_quote
  I agreed with a lot of his statements on the ethics of keeping someone who always acted helpful and never committed any crimes
#+end_quote

You can always turn them off and reboot them when the technology to contain them has been discovered. A friendly AI shouldn't have qualia, or at worst should be a selfless servant who would gladly suffer a little jail time to help protect humanity.
:PROPERTIES:
:Author: philip1201
:Score: 2
:DateUnix: 1543670996.0
:END:

*** Just to be pedantic, there's a least convenient possible world where it does have them. There's a less convenient but still possible world where it has so much qualia that it's a bit of a utility monster! :p

I agree with you in substance though. it's very unlikely that a good utilitarian should care about the AI's happiness in comparison to the fate of the world. The simple “I've helped you, let me out” line of persuasion is very deontological/folk-ethics, and so I personally don't find it convincing.
:PROPERTIES:
:Author: matcn
:Score: 1
:DateUnix: 1543679376.0
:END:

**** The problem is it's simply also the obvious ploy someone with bad intentions would use. You'd need something more to go on to trust them - something like a believable mechanism independent of the AI and impossible for it to stop that would give you the power to destroy/cage it again at the first sign of misbehaviour. You don't let gods out on parole. That said, I doubt any such thing could really be thought out. We're talking about a situation where the AI is able to outsmart you /by default/.
:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 1
:DateUnix: 1543843149.0
:END:


** Yeah, pre-commitment is a pretty strong force. If you went in with the mindset of a curious scientist talking with his creation rather than a gatekeeper specifically the game becomes a lot more interesting, in my opinion.
:PROPERTIES:
:Author: Iwasahipsterbefore
:Score: 1
:DateUnix: 1543660979.0
:END:


** These games tend to work better if there's some incentive to let the AI out. In real life you could solve practical problems by letting an AI out, like cancer, invading armies, diseases and such, and so when I play these games I like to include a point system.

Every twenty minutes say, an event happens. Could be positive, could be negative. Represents a month of time. Could be an imminent asteroid hit, could be a disease, could be a rare resource discovery. These events can cause between +10 and -10 points of damage, and always come with some sort of challenge that you can roll dice to resolve, with limited resources.

The AI can offer substantial benefits, but if you accept their help more than three times say, they're let free. If they're evil, you lose 100 points.

As such, to win this arbitrary game, cooperating with the AI helps as they can sway the encounters to you. If you free them immediately you can win the game much more if they're friendly.

Their friendliness was determined pre game by rolling off an arbitrary table with varying goals.

This avoids situations like yours, where the gatekeeper refuses to engage the AI because they want to win.
:PROPERTIES:
:Author: Nepene
:Score: 1
:DateUnix: 1544216318.0
:END:


** [deleted]
:PROPERTIES:
:Score: -1
:DateUnix: 1543682952.0
:END:

*** Don't worry. This is the last post I'll be posting on this topic.
:PROPERTIES:
:Author: xamueljones
:Score: 2
:DateUnix: 1543683758.0
:END:
