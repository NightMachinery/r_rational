#+TITLE: [Biweekly Challenge] Long View

* [Biweekly Challenge] Long View
:PROPERTIES:
:Author: alexanderwales
:Score: 10
:DateUnix: 1525413577.0
:DateShort: 2018-May-04
:END:
** Last Time
   :PROPERTIES:
   :CUSTOM_ID: last-time
   :END:
[[https://www.reddit.com/r/rational/comments/8daskl/biweekly_challenge_complexity/][Last time]], the prompt was "Complexity". Our winner is [[/u/xamueljones]], with their story, [[https://www.reddit.com/r/rational/comments/8daskl/biweekly_challenge_complexity/dxlnt5q/]["For God-like power, all I need is one bit"]]. Congratulations to [[/u/xamueljones]]!

** This Time
   :PROPERTIES:
   :CUSTOM_ID: this-time
   :END:
Our current challenge is *Long View*. The story should center around long-term thinking, ideally in the range of decades if not centuries; projects or plans that can't be completed in the lifetime of anyone but an immortal, future-proofing for a future that can't be predicted, and optimizing for extreme endurance - that sort of thing. Remember that prompts are to inspire, not to limit.

The winner will be decided *Wednesday, May 16th.* You have until then to post your reply and start accumulating upvotes. It is strongly suggested that you get your entry in as quickly as possible once this thread goes up; this is part of the reason that prompts are given in advance. Like reading? It's suggested that you come back to the thread after a few days have passed to see what's popped up. The reddit "save" button is handy for this.

** Rules
   :PROPERTIES:
   :CUSTOM_ID: rules
   :END:

- 300 word minimum, no maximum. Post as a link to Google Docs, pastebin, Dropbox, etc. *This is mandatory.*

- No plagiarism, but you're welcome to recycle and revamp your own ideas you've used in the past.

- Think before you downvote.

- Winner will be determined by "best" sorting.

- Winner gets reddit gold, special winner flair, and bragging rights. Five-time winners get even more special winner flair, and their choice of prompt if they want it.

- All top-level replies to this thread should be submissions. Non-submissions (including questions, comments, etc.) belong in the companion thread, and will be aggressively removed from here.

- Top-level replies must be a link to Google Docs, a PDF, your personal website, etc. It is suggested that you include a word count and a title when you're linking to somewhere else.

- In the interest of keeping the playing field level, please refrain from cross-posting to other places until after the winner has been decided.

- No idea what rational fiction is? [[http://www.reddit.com/r/rational/wiki/index][Read the wiki!]]

** Meta
   :PROPERTIES:
   :CUSTOM_ID: meta
   :END:
If you think you have a good prompt for a challenge, [[https://docs.google.com/spreadsheets/d/1B6HaZc8FYkr6l6Q4cwBc9_-Yq1g0f_HmdHK5L1tbEbA/edit?usp=sharing][add it to the list]] (remember that [[http://www.reddit.com/r/WritingPrompts/wiki/prompts?src=RECIPE][a good prompt is not a recipe]]). Also, if you want a quick index of past challenges, I've [[https://www.reddit.com/r/rational/wiki/weeklychallenge][posted them on the wiki]].

** Next Time
   :PROPERTIES:
   :CUSTOM_ID: next-time
   :END:
Next time, the challenge will be [[/u/blasted0glass]]' choice, *Memoir*. Take one of your memories from real life and write about it. Feel free to change names, places, and whatever else (you really should change names at least). Dramatizations are fine, as are obviously fictional elements. The question at hand is: what experiences from your own life could be a rational story?

*Next challenge's thread will go up on 5/16*. Please private message me with any questions or comments. The companion thread for recommendations, ideas, or general chit-chat [[https://www.reddit.com/r/rational/comments/8gx2z7/challenge_companion_long_view/][is available here]].


** [[https://vi-fi.github.io/Fidelity%20(A%20Darker%20Night).html][Fidelity (A Darker Night)]] (1700 words)
:PROPERTIES:
:Author: vi_fi
:Score: 7
:DateUnix: 1525437120.0
:DateShort: 2018-May-04
:END:

*** (Edit: Huh, the previous spoiler tag format isn't working any more. Beginning experimentation.)

Hypothesis: [](#s "There is only one extraordinary event, and no coincidences.")

Corollary: [](#s "By killing her, specifically, he bought his planet an indefinite length of time, not just a few weeks.")

More detailed reasoning: [](#s "His and her minds are pretty-clearly being handled on a different priority level from everything else, whether that's the category of 'things important to her' or 'human minds' or 'brains' or something else. Consider the paragraph on her capability; somewhat backed up by her portrayed mental ability throughout the text; posit that she has a surpassing-normal-specifications, very-abnormal mind. From the hypothesis that there are no coincidences, posit that her abnormality and the observed world's abnormality are causally correlated in some way. Occam's razor--assume that if her existence was a trigger to deliberately end the simulation, either a safer (for her sake) or more immediate (not for her sake) way would be used for shutdown, whether the means were pre-programmed or supervised. Note that this also applies to the phenomena if her abnormality is anthropic, just high-sigma rather than meaningfully special--if a deliberate outside decision, more plausible that it would have been carried out in a more goal-serving way (quicker or safer, given the observed higher priority level of their minds); more plausible that it's symptoms of an internal problem in a hands-off state.")

Elaborated hypothesis: [](#s "Her mind itself is what has been gobbling up the universe's resources. In one of the sillier possibilities, the system on which runs the simulation--which gives higher priority to minds or her mind at least (the higher priority being the bug itself also conceivable, as long as it also directly explains her abnormal mental capabilities)--allocates a set pool of resources to the simulation according to the reported resources needed (maybe there are many simulations, or other resource-intensive programs, running on the same system (whatever its form may be)?), and the special-priority-level-for-minds simulation reports an amount according to the world's parameters and the number of minds in existence. One mind that's using up more resources than it should be, and everything starts getting laggy, getting problems. Emergency intended-temporary resource free-up, certain processes momentarily operating at a simpler level to keep the minds operating smoothly, except the simpler operation isn't 'momentary' because the problem isn't momentary, the simpler process doesn't get to get more complex because the abnormal mind is still there and using up its freed-up resources, and then when the next resources-allocation comes the new estimation calculation is according to the persisting simpler-level process and the number of minds, the lower-amount-of-resources-to-work-with gets locked in, and the world ratchets downward, as though its required budget were being reduced in an off-by-one error each time. That explanation is actually most appealing to me, as the main other would be that her mind is exponentially growing and gobbling up all the system's available resources by itself, for which I would hope there would be additional insight/capability displayed.")

Ah, before I forget again, a weeks-related calculation: [](#s "(Known) human minds went up from 500 million to 7 billion with no apocalypse, then down to 2. If every human-mind death could actually grant a few weeks (rather than, say, a thousand dying in the same day granting the same reprieve (depending on ratchet function) as one dying in one day) before the ultimate end, then treating 'a few weeks' as at least three, 7,000 million * 3 / 52 ~ 403.8 million years before that 'ultimate end' already would have been bought. Even if that ultimate end were about to immediately arrive when the population first started dropping, that's too much time for it to arrive already by 2288.")

Elaboration/Restatement of the corollary: [](#s "Her mind able to die from physical occurrences, and her mind's abnormal operational parameters hypothesised to be the cause of the abnormal behaviour of the simulation, the death of her mind either immediately frees up all the exhausted resources, or else locally frees up the resources she was using while getting rid of the bug that caused putting-on-the-thumbscrews inaccurate resource need estimation. (Or something else I haven't thought of.) Either way, her death specifically allowing the simulation to return to normal operation--Probably, the only hope any of humanity had was for her specifically to be noticed and killed (or even just frozen!) quickly, and by her having been kept alive so long all the rest of humanity has been doomed. Then again, I can still sympathise with that course of action, particularly if she herself suspected it! The 'Even if carrying a virus that would kill everyone else on the planet, still wanting to struggle to live even if it makes everyone else on the world one's enemy'--or the similar 'I will be your ally even if the entire rest of the world become your enemies'... though at least trying a medical coma to see if that had an effect on apocalyptic changes, earlier, or staking everything on being frozen and then being reawoken once humanity had gotten advanced enough to do something about the problem, either on her side or the world's, would have been particularly tempting to me at least...")

Now we get to the confusing part. [](#s "I can't fathom why she would aim for the gut. Was explaining it to him so important to her that she would choose that method, even with how inhuman it was, rather than even the throat? Was there something else she was going to tell him, in the greater die from gut-stab death compared to throat-cut death? Did she predict he wouldn't let her do it if she explained it first--even though, with his current personality, he probably would have and not resisted--and yet not think about how he would react if taken by surprise? For that matter, why did she leave the handle in there? More death-slowing, wanting to give a really really long explanation? And then, able to kill others, yet not able to react/dodge/roll when taken by surprise? Given her displayed personality, and the timing, I can't see it as a deliberate assisted suicide to overcome difficulty in doing it herself--unless the death of her hope when the moon was gone was her despair at confirming the truth, and then only wanting to die together with her father? There's also a semi-funny possibility (though one I don't favour) of her guessing the existence of an afterlife system, trying to get around a necessity-murder-allowed-but-suicide-not rule, or just trying to save him and not herself, since if knowledge didn't interfere then she could have explained it to him first...")

Incidental note of what memory (of a fiction) most resonates with the ending: [](#s "Near the end of the Doc Future prologue. --Huh, the spoiler tag erases the link. [[http://docfuture.tumblr.com/post/34751426243/doc%5C-prologue][http://docfuture.tumblr.com/post/34751426243/doc\-prologue]] ")
:PROPERTIES:
:Author: MultipartiteMind
:Score: 6
:DateUnix: 1525575228.0
:DateShort: 2018-May-06
:END:

**** Unmarked spoilers below (I assume everyone who wants to read the story has read it by now, especially before reading the comments.)

As [[/u/eroticas][u/eroticas]] said below, I intended for them (and in fact for all humans) to have the same priority. Your theories are interesting, though :)

As for your calculation, you're indeed correct that the system ran fine with 7 billion humans, at least for a while. The loss of fidelity is a result of the simulation running out of resources, not anything happening internally. (My main inspiration for this story was Tsuki Project, a thoroughly weird anime-themed suicide cult based on a similar idea; the servers are getting shut down, and the rest of the simulation is run through at an accelerated pace and decreased resolution.)

Why did she aim for the gut? Good question. This is certainly one of these times where I could have thought things through more thoroughly. These short stories are usually planned for less than two weeks and written within a few hours, and it shows. (I should probably resolve to do more proofreading and editing for longer works, though. Would you be interested in proofreading with a focus on logical and consistent behaviour? I always enjoy your comments.)
:PROPERTIES:
:Author: vi_fi
:Score: 3
:DateUnix: 1525985665.0
:DateShort: 2018-May-11
:END:

***** I cannot guarantee consistent reddit checking, but I would be happy to read through and comment on anything I see in my inbox!

I'm happy to get the anwer about all having the same priority; I still don't have a clear mental model about how resource free-up works. In-story, what year (approximately) did humanity last have 7 billion members alive? Let 'The End' be the point at which system simplification directly causes death of any humans still alive; in the story, The End would have come for the two surviving humans at a certain point, and the death of one made enough resources 'free' to delay the ultimate date+time of The End by a few weeks. Does a human death when humans still had 7 billion members delay the ultimate date+time of The End by the same amount, or by much less? (Does the birth and then death of a human result in a negative amount of resources freed up, rather than 0?)

(...even if resource chewing-through at a faster and faster rate, unless the death of a human later somehow gives a greater amount of resources than a human dying earlier... or else, the chewing-through rate /starting/ really fast, then getting slower and slower... but no, still doesn't work if the resource cap decrease-per-unit-time is external... unless it's percentage based, the more 'free' resources there are the faster it gets consumed, in which case... yes, one way or another there has to be a way that 100 deaths (through simultaneity or timing) doesn't delay The End as much as 100 * the delay from a single death, but specifically how...)

(7 billion * 2/52 =..!)
:PROPERTIES:
:Author: MultipartiteMind
:Score: 3
:DateUnix: 1526023380.0
:DateShort: 2018-May-11
:END:

****** Regarding proofreading: I'm mostly thinking about a specific larger project, so consistent reddit checking is not as critical. I'll send you a DM once details crystallize (probably a few weeks in the future).

My mental model of the resource free-up was as follows: There are n server-hours left (let's say a year's worth of full resolution simulation, to grasp a number out of thin air.) The simulators let the next year run at half resolution (which would probably be unnoticeable, as there are a lot of details that can be abstracted away). Now they have a half-year of server time left. Proceeding as before, after ten years have passed in the simulation, we only have 0.098% of the details left, and after another year, they will be further halved. The numbers are arbitrary, of course.

Now the question is how the simulators' resource allocation interacts with the natural decline of the required resources (i.e. humanity dying out). I guess that's where I could finagle the details until the story seems reasonable.
:PROPERTIES:
:Author: vi_fi
:Score: 2
:DateUnix: 1526035810.0
:DateShort: 2018-May-11
:END:

******* Thank you very much! I think making the decrease exponential rather than linear resolves it!

Modelling (simplified) with 8 people and an extra day; near-end point, two people and one day to go, at end of day would have two people on half-capacity, both dead; one dies, have one person running on double-capacity, after day then running on full-capacity, after another day then running on half-capacity. Two days earlier, back to 8 people running on full capacity. All live, all straight die. To get the first extra day, four have to die. To get the next extra day, two have to die. ...Yes, that's it. In a situation where the available resources is (effectively!) halved in unit time T, every postponement of The End by 1T requires halving your current population. Convenient-if-unlikely if the two characters were the last surviving members of any civilisation--though, ah yes, especially with the priority level and Fermi's paradox probable/plausible that only that one planet existed.

Let time T be three weeks; (log(7,000,000,000)/log(2))*3/52 = ~1.887. Let T be 3-6 weeks to get a maximum instead of a minimum; from that, from all the dying off humanity did in the early non-critical stages, The End would have been ultimately pushed back by less than four years. *happiness*
:PROPERTIES:
:Author: MultipartiteMind
:Score: 2
:DateUnix: 1526182803.0
:DateShort: 2018-May-13
:END:


**** They both had the same priority. They were the last two or last few. She was going to kill him to live longer and buy time to escape the problem. He just wanted them both to die quickly rather than slowly abstracted away. He killed himself hours after.
:PROPERTIES:
:Author: eroticas
:Score: 2
:DateUnix: 1525700715.0
:DateShort: 2018-May-07
:END:


*** That's a file:/// URL.
:PROPERTIES:
:Author: osmarks
:Score: 2
:DateUnix: 1525437168.0
:DateShort: 2018-May-04
:END:

**** How embarrassing. Thank you for alerting me.
:PROPERTIES:
:Author: vi_fi
:Score: 1
:DateUnix: 1525438119.0
:DateShort: 2018-May-04
:END:


** [[https://kishoto.wordpress.com/2018/05/11/schism/][Schism]]

Word Count: 2292
:PROPERTIES:
:Author: Kishoto
:Score: 4
:DateUnix: 1526056507.0
:DateShort: 2018-May-11
:END:


** [[https://docs.google.com/document/d/1S0Sx3cbNpozAcMtLO3yG4L3y3tIjpNaKPgqsxxDTgww/edit?usp=sharing][The Only Thing that Proves You]] (3637 words)
:PROPERTIES:
:Author: blasted0glass
:Score: 4
:DateUnix: 1525835213.0
:DateShort: 2018-May-09
:END:

*** I wonder why the stop was programmed in. Sentimental value? Seems like an inefficient and value-less excursion if the idea was to have the ship persist as long as possible.

I'm trying to wrap my head around the mental gymnastics of programming this into priority 1, but also deleting any capability of the ship to understand or appreciate the sentimental value of visiting that system, unless it triggered something in the sentients it housed unbeknownst to the narrator/ship.
:PROPERTIES:
:Author: t3tsubo
:Score: 1
:DateUnix: 1526053924.0
:DateShort: 2018-May-11
:END:

**** Given how scatterbrained and increasingly amnesiac this narrator becomes, it could be an attempt to heal, written down by itself in a moment of better lucidity.
:PROPERTIES:
:Author: WilyCoyotee
:Score: 2
:DateUnix: 1526066203.0
:DateShort: 2018-May-11
:END:
