#+TITLE: [RT] Three Bodies At Mitanni, by Seth Dickinson

* [RT] Three Bodies At Mitanni, by Seth Dickinson
:PROPERTIES:
:Author: embrodski
:Score: 13
:DateUnix: 1453915917.0
:END:
This came out in mid-2015, but it was only available via traditional pay publication. It's been pointed out before that, generally, that isn't worth sharing here. However I consider Seth Dickinson to be a rationalist author, and this is one of his rationalist pieces, and I /really/ like it. So I got the audio rights. :) If you don't mind listening to your fiction rather than reading it, here's [[http://www.hpmorpodcast.com/?p=1647][Three Bodies at Mitanni]], free online. If you do mind, I still recommend it, and will point in the direction of the June 2015 issue of Analog magazine.


** There be spoilers below!

For realz.

OK.

I'm still into themes of consciousness, specifically as discussed in [[http://slatestarcodex.com/2014/07/30/meditations-on-moloch/][Meditions on Moloch]] and Peter Watts's [[http://www.rifters.com/real/Blindsight.htm][Blindsight]]. I find them deliciously frightening. But what I really like about 3BM is that it makes me really freakin' worried about myself.

Take [[http://lesswrong.com/lw/5f/bayesians_vs_barbarians/][Bayesians vs Barbarians]] which makes the case that, if an /actual/ Rational Society was attacked by fanatics, the Rational survival action is to delegate the amount of people and resources necessary to defeat the fanatics and convert them to war-making purposes until victory is assured. People would likely be selected based on some combination of effective fight ability and lottery. This would include whatever self-modification is necessary in order to WIN as efficiently as possible.

I completely agree. I would hope such a lottery wouldn't choose me, but if it did I would submit to warrior-modification for the good of my society. This IS what I want.

Which sounds suspiciously like the horror-punch of 3BM.

Which also leads me to realize that any time I give up any immediate pleasure for the promise of future utility (saving money to invest rather than spending on hedons; working on podcast or writing rather than going out with friends; etc) I am in effect saying "This is fine. This is what I want." And it really is! A life of pure in-the-moment hedonism would suck, I /want/ to make things that last, and to have future financial security! But, well, how far am I willing to push that before I become Mitanni-esque? Before I'm not really working for me anymore, I'm just benefitting "the system" and deluding myself that it's for me?

Basically, anytime my core values are convincingly attacked like this, I feel really creepy and shivery, and I like that feeling.
:PROPERTIES:
:Author: embrodski
:Score: 3
:DateUnix: 1453916961.0
:END:

*** Spoilers!

Must say Blindsight is really that curious kind of frightening to me too.

After putting it down, I was thinking about how we could distinguish system 2 rationalizations of automatic system 1 thinking vs actual system 2 thinking, and I don't have a good handle on the probable answer. We don't really have a pure reasoning engine per se, probably just hacks and heuristics and biases with some general algorithms to tie it together, so I'm guessing there's actually quite a bit of rationalization or at least skipping of intermediate steps by just using cached responses, while thinking we're having entirely 'new' thoughts. But that doesn't mean the causal relationships from thoughts to decisions-to-act don't also depend on at least some system 2 type thoughts. I can evaluate which car is the best one for my budget using a spreadsheet and then go out and buy it, and I'll be very explicitly determining the outcome based on an actually defined financial model and related thoughts.

It'd be nice if we were able to measure how prevalent system 1 and 2 are. I imagine system 1 wins out 80:20 of the time. But even with associative, automatic system 1, certain automated behaviors may be the result of deliberate system 2 practice. Superforecasting makes this point... certain stellar forecasters had a basically habitual tendency to think rationally, update based on evidence, seek new points of view and so on. They seem to have trained some automatic habits by just practicing certain techniques enough. So, the prospects for deliberate thinking rise quite a bit if you can deliberately train your own system 1 behaviors to some degree.

I think on balance I'm left a bit curious about all this... i should go and actually finish reading Thinking Fast and Slow...
:PROPERTIES:
:Author: tvcgrid
:Score: 2
:DateUnix: 1454388431.0
:END:


** Your voice acting is amazing. Great work!
:PROPERTIES:
:Author: ywecur
:Score: 2
:DateUnix: 1465429190.0
:END:
