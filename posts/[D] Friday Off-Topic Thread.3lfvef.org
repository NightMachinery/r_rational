#+TITLE: [D] Friday Off-Topic Thread

* [D] Friday Off-Topic Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 9
:DateUnix: 1442588883.0
:DateShort: 2015-Sep-18
:END:
Welcome to the Friday Off-Topic Thread! Is there something that you want to talk about with [[/r/rational]], but which isn't rational fiction, or doesn't otherwise belong as a top-level post? This is the place to post it. The idea is that while reddit is a large place, with lots of special little niches, sometimes you just want to talk with a certain group of people about certain sorts of things that aren't related to why you're all here. It's totally understandable that you might want to talk about Japanese game shows with [[/r/rational]] instead of going over to [[/r/japanesegameshows]], but it's hopefully also understandable that this isn't really the place for that sort of thing.

So do you want to talk about how your life has been going? Non-rational and/or non-fictional stuff you've been reading? The recent album from your favourite German pop singer? The politics of Southern India? The sexual preferences of the chairman of the Ukrainian soccer league? Different ways to plot meteorological data? The cost of living in Portugal? Corner cases for siteswap notation? All these things and more could possibly be found in the comments below!


** With trepidation, I dredge up another probably controversial opinion of mine. Downvotes are welcome as long as it generates interesting conversation.

Star Wars sucks. I don't get it, I just don't. It's a stupid predictable plot in a world with inconsistent sci-fi and paper thin characters. How did something this bad get so big? Having forced myself to watch all 6 and finding that I enjoyed maybe half of the first one (of the original three), it's beyond me how these movies got past pre-production.

It doesn't help that the universe doesn't have any depth and seems to be targeted at kids who've just discovered sci-fi. Hell, even most of the fanbase seems to hate the prequels. The world is written to be as malleable as possible, allowing for 100 comic books, 30 graphic novels, and 20 more movies.

So, why is VII generating so much hype? Aren't they just trying to milk the franchise as much as possible at this point? Why do people still love SW so much?
:PROPERTIES:
:Author: Magodo
:Score: 11
:DateUnix: 1442591454.0
:DateShort: 2015-Sep-18
:END:

*** The /original/ Star Wars trilogy harkened back to the pulp magazine era that George Lucas had grown up in (same for /Indiana Jones/). There wasn't much original about Star Wars even when it was first released in theaters. So I think if you want to ask why Star Wars was popular then, you have to ask why pulps were able to sell a million copies a week for a long period of time.

I think there are three big factors at play.

1. The pulps existed to fire the imagination. In the same way that the /Twilight/ series is often criticized for having Bella be this blank character for the audience to project themselves into, the pulps often had a world that an adventure-hungry person could project their imagination into. This is where the paper-thin characters and loose rules of the Star Wars saga work in its favor; it's not really about the story, it's about imagining your own stuff. I never hear people rave about the story of Star Wars and very, very rarely hear people saying the characters were well-constructed. Instead, they're imagining lightsaber battles or cool ships flying around exotic worlds.
2. Anything that runs for a long time is going to generate fans, even if it's crap. I haven't really figured out why this is the case, but part of it is that people gravitate to the familiar, and some other part must be that people get invested despite themselves. If you want fans, just keep churning out the same thing, week after week, or year after year, and eventually people will just ... have settled into it. Star Wars did that. At the place it's at now, it's an institution. /People like institutions./ (This was one of the basic models of the pulps.) People get done with a well-crafted story and think "I wish there were more of this" without really considering that maybe the story the author wanted to tell was done (alternately, without caring). George Lucas knew all this, which is why the series was calibrated towards repetition and familiarity.
3. It's got spectacle. When the pulps weren't firing the imagination, they were showing things that were neat and shiny, new ideas that maybe didn't have much more than surface to them but by the time you started thinking about it, they were on to the next thing. Part of this feeds back into 1, because you want to leave things to the imagination and hint at a larger world, but also because spectacle is easier if you're not giving the full details of how, say, a plasma blaster actually works. Star Wars has spectacle in spades.

Star Wars happened to come along at a time when the pulps had been dead for a while. It reintroduced that aesthetic to a new audience and hooked into those familiar patterns in the old audience who was familiar with the pulps. From there it gained institutional inertia and dug its hooks in with the expanded universe and loads of merchandise.
:PROPERTIES:
:Author: alexanderwales
:Score: 14
:DateUnix: 1442593490.0
:DateShort: 2015-Sep-18
:END:


*** I say this as someone who didn't like SW when I rewatched it as an adult, and who gave Phantom Menace a terrible review, publicly declaring that I would never give Lucas money again, because I felt at the time I was supporting a racist. I never saw the last two, although I've caught part of them on TV. So this is the original trilogy for me.

(I have softened in the last 15 years. Now I think Lucas is just tone deaf, and his donations of most of the proceeds from selling Star Wars make me think that I was far too rash in judging).

1. The Best Special Effects award the year before Star Wars went to Logan's Run. From the looks of the two movies you'd expect a few decades between them. Star Wars looked amazing at the time, and it holds up well.
2. My father took the (very young me) see it in theaters a second time. What interested him was that the movie constantly had music and themes. The music is legitimately one of the great orchestral pieces of the last century. Would it be as popular if it wasn't a movie? Of course not.
3. The world's depth is implied. The cantina scene, for example, implies dozens of other worlds, and not just "Human with a ridged forehead."
4. Having read Joseph Campbell, I'm not sure that it mattered that it tapped into the Hero's Journey architype, but there's certainly an argument to be made.

#+begin_quote
  inconsistent sci-fi
#+end_quote

1. It's a sci-fi movie based on Swashbucklers and Samurai Movies. It's a glorious hodgepodge.

That being said, it's overlong, poorly edited (sometimes deliberately so, like the pulps), and the trilogy is repetitive.
:PROPERTIES:
:Author: TaoGaming
:Score: 4
:DateUnix: 1442611631.0
:DateShort: 2015-Sep-19
:END:


*** I loved Star Wars when I first saw it, back in the '70s, right up to the point where Luke turned off his targeting computer.

Even as a teenager that pissed me off.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 6
:DateUnix: 1442592156.0
:DateShort: 2015-Sep-18
:END:


*** Most of my love of /Star Wars/ is for the Expanded Universe novels (Thrawn Trilogy, New Jedi Order, Legacy of the Force, and many stand-alone books), rather than for the movies or the cartoons. The books expand (duh) the background a lot, even if they can be inconsistent at times. I like the original trilogy, but certainly don't care enough about it to rewatch it, as I do with the books; I probably skipped half of every prequel movie when I watched it on DVD (romance? who cares?), but I loved the ship designs and visual effects.

(I may count only as "a kid who's just discovered sci-fi", though, since I read only for entertainment, rather than for any depth.)
:PROPERTIES:
:Author: ToaKraka
:Score: 2
:DateUnix: 1442595178.0
:DateShort: 2015-Sep-18
:END:

**** This is exactly where I'm at- the books just add so much depth to the universe and characters. The EU novels' vary a lot in quality (there are some really horrible or just bland books out there) but it's a very rich, vibrant universe with epic story lines. Overall I agree it's mostly entertainment sci-fi except there are some amazing books in there. I think everyone knows the Thrawn Trilogy is a staple recommendation, but for example /Traitor/ is pretty unique (and probably the closest example to a relationship like Quirrel/Harry's in HPMoR, now that I think of it).

Whereas the movies? Great special effects, some memorable scenes/actors, and that's probably all good I can say about them.
:PROPERTIES:
:Author: whywhisperwhy
:Score: 2
:DateUnix: 1442604040.0
:DateShort: 2015-Sep-18
:END:

***** Yes, yes--/[[https://www.goodreads.com/book/show/35461][Traitor]]/ is probably my favorite of all the EU books. (It's been so long since I last read [[https://www.goodreads.com/series/42348][the Thrawn Trilogy]] that I can hardly remember it, though, so I may not be comparing them properly...)
:PROPERTIES:
:Author: ToaKraka
:Score: 1
:DateUnix: 1442604599.0
:DateShort: 2015-Sep-18
:END:


*** I wouldn't say it sucks, but yeah, it's kind of dumb and not all that special or great.
:PROPERTIES:
:Score: 2
:DateUnix: 1442595338.0
:DateShort: 2015-Sep-18
:END:


*** u/FuguofAnotherWorld:
#+begin_quote
  So, why is VII generating so much hype? Aren't they just trying to milk the franchise as much as possible at this point? Why do people still love SW so much?
#+end_quote

Well, personally I think I'm hyped for two reasons. One, Star Wars got to me before I had seen enough films to start picking things apart. Fleeing through an asteroid field for example was a new and exciting way to get rid of chasers, rather than that thing any fighter pilot in a film does. Second reason is fond memories of KOTOR and KOTOR II. That's all there is too it. I see a star wars VII trailer and my mind just links it to all those hours spent happily cutting people up with lightsabers.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 2
:DateUnix: 1443004704.0
:DateShort: 2015-Sep-23
:END:


*** u/DataPacRat:
#+begin_quote
  Why do people still love SW so much?
#+end_quote

I suspect a combination of two causes: that at the time it first came out, it was, in fact, one of the best available pieces of SF on visual media; and that a lot of people watched it at an age where they imprinted on such things.

I'm still rather fond of the "Mystara" setting for Dungeons and Dragons, which I was exposed to at roughly the same time, in spite of it being based on D&D physics and having all sorts of inconsistencies and silliness that should turn the worldbuilding on its head.

#+begin_quote
  Aren't they just trying to milk the franchise as much as possible at this point?
#+end_quote

"You say that like they've ever done anything else."

#+begin_quote
  Having forced myself to watch all 6
#+end_quote

"Well, /there's/ your problem."
:PROPERTIES:
:Author: DataPacRat
:Score: -1
:DateUnix: 1442593944.0
:DateShort: 2015-Sep-18
:END:


** Since Alicorn's [[http://alicorn.elcenia.com/stories/dogs.shtml][Dogs]] story was posted here a while ago, I'm interested in knowing what you think about the following issue.

You probably know about the reducing animal suffering section in the EA movement? Anyway, [[http://qz.com/497675/to-truly-end-animal-suffering-the-most-ethical-choice-is-to-kill-all-predators-especially-cecil-the-lion/][the co-founder of Giving What We Can argued that we should start killing predators]] because they cause suffering by eating prey animals alive. Of course that was a really dumb suggestion because it's really hard to predict what the actual effects are of that kind of intervention.

As you could guess, [[http://thehsi.org/2015/09/10/animal-rights-gone-wrong/][the response to this was a bit hostile.]] [[https://www.facebook.com/permalink.php?story_fbid=10153646854454700&id=99206759699][In Facebook discussion about this]] many people suggested killing the authors. [[https://www.facebook.com/VeganDotCom/posts/10153217867161872?hc_location=ufi][People argued that nature is sacred, that we should leave it alone, that morality doesn't apply to animals]]:

#+begin_quote
  One of the most problematic elements of this piece is that it presumes to impose human moral values on complex ecosystems, full of animals with neither the inclination, nor the capacity, to abide by them.
#+end_quote

I don't think we should start killing animals to reduce suffering. Setting aside that, the question is, which is more important, the suffering of individual animals, or the health of the ecosystem or species as a whole?
:PROPERTIES:
:Score: 5
:DateUnix: 1442597279.0
:DateShort: 2015-Sep-18
:END:

*** My cumulative concern for individual animals vastly outweighs my concern for "species" or "ecosystems" or "nature" or whatever, so I regard ecosystem re-engineering or anti-conservationist destruction (probably through gradual capture, sterilization, and relocation) fairly positively. Which isn't to say that I don't value the knowledge-of-how-the-world-works represented by extant species (they're sorta important for my work in evolutionary biology and ecology, for one), nor that I don't have some purely "aesthetic" appreciation for nature shit (I've spent many thousands of hours hiking, backpacking, climbing, paddling, etc. You probably won't find an outdoorsier person than me outside a pub table of wilderness guides), nor that there aren't "practical" benefits to be found in preserving nature (e.g. medicinal herbs, though I think targeted approaches far more effective), etc. but rather that I value closing the hellish pit that is the brutal death and torture of trillions of animals per year (roughly) above the potential and current benefits that that suffering brings.

Or at least reducing it somewhat. Maybe instead of trillions of animals, keep it in the billions, or at least don't terraform future worlds to bring the numbers into the quadrillions and up. Maybe don't gently let /everything/ die, but keep some animals in pleasant, controlled, zoo-like environments in perpetuity (i.e. create a "technojainist" welfare state). And don't do this immediately, necessarily -- perhaps once the "diminishing returns of studying nature" have set in, or we have good surrogates for outdoorsy stuff, and especially once we 1) are fairly secure in our own survival as humans, and 2) have a good idea of the short and long term ecological effects (e.g. the population dynamics of mesopredator release). All keeping in mind that for every moment of hesitation and delay untold numbers of beings wail in agony, and all.

I reckon most people oppose stuff like this because they either don't value animal welfare very strongly, are very confident that non-human animals are incapable of suffering, very strongly value the preservation of nature intrinsically (at least when it can't affect them, though I'm sure plenty of people lamented the eradication of smallpox on the basis of not tinkering with That Which Man Was Not Meant To Tinker With), or have a Disney-fied view of how the natural world works.

As a moral anti-realist/subjectivist, I don't think there's a "right" answer to the value-laden bits of the above, so when you ask

#+begin_quote
  which is more important, the suffering of individual animals, or the health of the ecosystem or species as a whole
#+end_quote

I see it as ultimately a "personal" question, with the necessary qualification of /important to whom/ or /important for what/. Within my own set of values, the bit that cares about stuff like this vaguely resembles preference utilitarianism, and I'm pretty sure your average, say, field mouse cares a lot more for not starving or being torn limb from limb by a barn owl than it does about complex abstractions like the "good of the species" (with all the evolutionary misconceptions that term entails). Of course, it probably cares a fair bit about raising young and fucking (perhaps less than avoiding a painful death, though), but "ecosystem health" and "species preservation" are not on the agenda.

Of the people I know, Brian Tomasik has probably written the most about these issues (followed maybe by David Pearce). I'd start [[http://reducing-suffering.org/#Animals][here]] under "Wild-animal suffering", if you're interested in reading some essays and discussions.
:PROPERTIES:
:Author: captainNematode
:Score: 3
:DateUnix: 1442607336.0
:DateShort: 2015-Sep-19
:END:


*** I think ecosystems have some value of their own, as an interesting thing that could be permanent lost. But it's unreasonable to value them /more/ than their constituent parts, considering the suffering involved.

I don't know of any way to systematically reduce wild animal suffering; I'd suggest some sort of large-scale zoo or adoption system, possibly prioritizing prey animals and highly intelligent species somewhat. But while this might reduce suffering on the margin, it could never scale to eliminate even a noticable amount of animal suffering.

On the other hand, I'm /extremely/ dubious about the idea that animal lives /aren't worth living/. You don't even have the evidence of suicidality with animals; they demonstrably aren't suicidal. So I'm not really comfortable with attempting to euthanize or sterilize large portions of the biosphere, a task which would merely require a one-world government to accomplish.

In short, I think animal suffering is bad and should be prevented; but I don't think it's possible to bring animals as a group up to our living standard, at current levels of technology. Technology will advance, though, and we can still help individual animals to an extent.

The issue of suffering in /domesticated/ animals, however, is both far larger per individual animal and much easier to address.
:PROPERTIES:
:Author: MugaSofer
:Score: 3
:DateUnix: 1442609829.0
:DateShort: 2015-Sep-19
:END:

**** Animal lives could be worth living*, but we still wouldn't want to create any more of them (depending on your thoughts concerning stuff like Parfit's mere addition paradox and Benatar's Asymmetry, etc. Humans with tremendously unfortunate diseases might still have lives worth living but would still want to prevent creating more humans with those diseases, especially when alternatives exist). Currently existing animals wouldn't necessarily die (except, perhaps, by old age), but I don't feel as strong an impetus to let them breed. And if, for example, we can't feasibly round up the predators and let them live well without harming others, we'd have to weigh their preference for life against the preferences of all the animals they'd otherwise kill (averaged across our uncertainty in predicting the effects of any sort of ecological intervention).

I also don't know that the suffering of agricultural animals is necessarily worse than that of wild animals. Perhaps for some types of animals (esp. in factory farms), suffering induced by being kept in a tiny box your whole life compares unfavorably to an hour of bleeding out as a hyena chews your leg off ([[http://foundational-research.org/publications/importance-of-wild-animal-suffering/#How_Wild_Animals_Suffer][as one example]]), but I think definitive statements to that effect are hard to make. And certainly some domesticated animals (e.g. many pet dogs) live far more pleasant lives than exist for the majority of animals in nature.

As for addressing the issue, I agree that ecosystem reformation is far harder a question than just closing down farms or improving slaughter practices. And it's certainly far less palatable to the average person, so there'd be considerable social pushback, at least in the present social climate. But there are still practical questions to consider today, like reintroducing predators to areas where they'd previously been depopulated (e.g. the Yellowstone wolves), or replanting the rainforest, or mitigating the less desirable effects of global warming, or whatever.

edit: though the suicidality observation doesn't necessarily demonstrate this, as non-human animals might just be really bad at forecasting the future. I'm sure a gazelle would "prefer" to die painlessly just before being disemboweled by a hungry lioness, perhaps even some considerable amount of time in advance. But since gazelles can't predict well what the future holds, they might "choose" to live on in the present, even if, with perfect foreknowledge, they'd have chosen to die.
:PROPERTIES:
:Author: captainNematode
:Score: 2
:DateUnix: 1442614548.0
:DateShort: 2015-Sep-19
:END:


*** False dichotomy: ecosystems sustain individuals, and will do so until we maybe someday stop being made of meat. Then it will just be social and infrastructural systems.
:PROPERTIES:
:Score: 2
:DateUnix: 1442605220.0
:DateShort: 2015-Sep-19
:END:

**** The key question is should we spread wildlife to other planets, and the options are: no wildlife = no suffering, healthy ecosystem = loads of suffering, or some kind of artificial system with animals in it. So in that case it's not a false dichotomy.

edit. [[/u/captainNematode]] also mentioned ecosystem re-engineering which is also an example in which the question is not a false dichotomy
:PROPERTIES:
:Score: 3
:DateUnix: 1442607435.0
:DateShort: 2015-Sep-19
:END:

***** Call me a humanocentric specieist ass but the only ecosystem I'd try to emplace on a colony planet is one that benefits our colonists. And the only reason to make that complex and self-sustaining is to have a bare bones support structure in case both technology and the interstellar supply system somehow go to shit for a while.

I guess another reason would be for science. You could do all kinds of ecological and biological experiments. No one will complain about people messing with the equilibrium of nature if the whole thing was emplaced on a terraformed dirtball by human hands in the first place.
:PROPERTIES:
:Author: Bowbreaker
:Score: 1
:DateUnix: 1442618786.0
:DateShort: 2015-Sep-19
:END:

****** "Interstellar supply chain" is not a thing and can't /be/ a thing, compared to an ordinary on-world supply chain. Colonies must be almost entirely self-sustaining or they won't work.
:PROPERTIES:
:Score: 1
:DateUnix: 1442778720.0
:DateShort: 2015-Sep-21
:END:

******* Until we find that space relay near Pluto :p

In all seriousness though, if we send an unmanned transport every few months (despite the first one not having arrived yet) we could in theory supply a small colony. We'd just need a post-scarcity society for that.
:PROPERTIES:
:Author: Bowbreaker
:Score: 0
:DateUnix: 1442782636.0
:DateShort: 2015-Sep-21
:END:

******** While technically possible, it would be incredibly inefficient and wasteful. The cost of resources to move those supplies up to a fraction of light speed and back could be used to colonise entire other solar systems, or keep however many million people alive for x number of years (instead of spending the same amount of resources on a few thousand people for x years). We only have so many resources in our sun's gravity well and by extension in our universe, so it behooves us to use them efficiently.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 1
:DateUnix: 1443005159.0
:DateShort: 2015-Sep-23
:END:


*** I find it very dubious that extant animals suffer to the same extent as humans, and that I should care about animals to the same extent as humans. So forgive me if I simply don't care about this. If anyone starts to interfere with ecosystems (already approaching destabilization) that support humans for the sake of prey animals, I will oppose them in the only way I can, by posting loudly about it on the internet. And voting, if it ever comes up.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1442700350.0
:DateShort: 2015-Sep-20
:END:

**** Obviously animal suffering isn't as important as human suffering. But there's so much /more/ of it.

To justify your argument, you'd have to value animal suffering trillions of times less than human suffering (which seems rather suspicious), or simply not subscribe to utilitarianism at all - or consider animal happiness worthless.
:PROPERTIES:
:Author: MugaSofer
:Score: 1
:DateUnix: 1442755951.0
:DateShort: 2015-Sep-20
:END:

***** Lots of people don't subscribe to utilitarianism.
:PROPERTIES:
:Score: 2
:DateUnix: 1442784134.0
:DateShort: 2015-Sep-21
:END:

****** I'm not certain that utilitarianism has anything to do with it. Utilitarian moral objectivism seems to be the main argument here, while I'm more like a moral subjectivist. I may just be thinking of utilitarianism in the "shut up and calculate" sense, rather than by the philosophical tradition.

Speaking of, my ethics teacher seems to be bewildered by the fact that rational justification is not required for an individual's terminal values, while at the same time saying that terminal values (the Good) are the thing that all moral judgements are relative to. My classmates all hate me for talking too much. I think I'm just going to shut up for the rest of the semester.
:PROPERTIES:
:Author: Transfuturist
:Score: 2
:DateUnix: 1442811182.0
:DateShort: 2015-Sep-21
:END:

******* LessWrong tends to talk about ethics in extremely heterodox language, resulting in much confusion and flame-wars of mutual incomprehension when LWers encounter mainstream ethicists.

#+begin_quote
  Speaking of, my ethics teacher seems to be bewildered by the fact that rational justification is not required for an individual's terminal values, while at the same time saying that terminal values (the Good) are the thing that all moral judgements are relative to.
#+end_quote

There's no real contradiction here, but you're using extremely different meta-ethical positions. Most codes of normative ethics implicitly assume a realist meta-ethical position, in which case the Good is defined independently of people's opinions about it and moral judgements are made relative to the Good (even while an individual's own /personal preferences/ may simply /fail/ to track the Good).

Talking this way causes a whole lot of fucking trouble, because traditional ethicists have (usually) never been told about the Mind Projection Fallacy or considered that a mind could be, in some sense, /rational/ while also coming up with a /completely alien/ set of preferences (in fact, traditional ethicists would probably try to claim that such minds are /ethically irrational/), so, "The Good (as we humans view or define it (depending on meta-ethical view)) /must necessarily be causally related to/ the kinds of preferences and emotional evaluations that /humans specifically/ form" isn't so much an /ignored/ notion as one that's /so thoroughly woven into the background assumptions of the whole field/ that nobody even acknowledges it's an /assumption/.

Also, I do have to say, just calling oneself a subjectivist seems to duck the hard work of the field. If you treat the issue, "the LW way", then your meta-ethical view ought to give you a specification of what kind of inference or optimization problem your normative-ethical view is actually solving, thus allowing you to evaluate how well different codes of ethics perform at solving that problem (when treated as algorithms that use limited data and computing power to solve a specified inference or optimization problem). Declaring yourself a "subjectivist" is thus specifying very few bits of information about the inference problem you intend to solve: if it, whatever /it/ is, is about your brain-states, then which brain-states is it about, and how do those brain-states pick out an inference problem?

Whereas, in contrast, much of the work towards what's called "ethical naturalism" and "moral constructivism" seems to go to quite a lot of trouble, despite being "conventional" moral philosophy, to precisely specify an inference problem.
:PROPERTIES:
:Score: 2
:DateUnix: 1442815536.0
:DateShort: 2015-Sep-21
:END:

******** u/Transfuturist:
#+begin_quote
  If you treat the issue, "the LW way", then your meta-ethical view ought to give you a specification of what kind of inference or optimization problem your normative-ethical view is actually solving, thus allowing you to evaluate how well different codes of ethics perform at solving that problem (when treated as algorithms that use limited data and computing power to solve a specified inference or optimization problem). Declaring yourself a "subjectivist" is thus specifying very few bits of information about the inference problem you intend to solve: if it, whatever it is, is about your brain-states, then which brain-states is it about, and how do those brain-states pick out an inference problem?
#+end_quote

I don't understand this paragraph. By "code of ethics," you mean an agent's action selection process? What do you mean by "what kind of inference or optimization problem your normative-ethical view is actually solving?"
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1442816650.0
:DateShort: 2015-Sep-21
:END:

********* Picture the scenario in which your agent is /you/, and you're rewriting yourself.

Plainly, being human, you don't have a /perfect/ algorithm for picking actions. We know that fine and well.

So how do we pick out a /better/ algorithm? Well, first, we need to specify what we mean by /better/: what sort of problem the action-selection algorithm solves. Since we're designing a mind/person, that problem and that algorithm are necessarily cognitive: they involve specifying resource constraints on training data, compute-time, and memory-space as inputs to the algorithm.

If you've seen the No Free Lunch Theorem before, you'll know that we can't actually select a single (finitely computable) algorithm that performs optimally on /all/ problems in /all/ environments, so it's actually quite vital to know what problem we're solving, in what sort of environment, to pick a good algorithm.

Now, to translate, a "normative-ethical view" or "normative code of ethics" is just the algorithm you endorse as /bindingly correct/, such that when you do something other than what that algorithm says, for example because you're drunk, /your actually-selected action was wrong and the algorithm is right/.
:PROPERTIES:
:Score: 1
:DateUnix: 1442818032.0
:DateShort: 2015-Sep-21
:END:


***** I don't necessarily believe that disutility adds linearly across persons, or that it should. At the very least, I can say that my own terminal values are not calculated that way. Fifty people all being slightly depressed is much preferable to forty-nine very happy people and one person in crushing depression.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1442810771.0
:DateShort: 2015-Sep-21
:END:


** What option(s) for neuter-gender English pronouns do you prefer?

*1.* =He= universally

*2.* =She= universally

*3.* =He= and =she= alternating (by example, paragraph, chapter, etc.)

*4.* =They= universally

*5.* A new word (=ze=, =xe=, etc.) universally

*6.* =It= universally

*7.* The writer's preferred pronoun

Generally, I lean toward options 3, 5, and 6.

Feel free to suggest further options.
:PROPERTIES:
:Author: ToaKraka
:Score: 5
:DateUnix: 1442595861.0
:DateShort: 2015-Sep-18
:END:

*** I usually use, and prefer, "they" universally. I have seen he/she alternating done well but it seems harder to do well than just using "they". As far as neologisms goes, I don't really like them, and only use them in reference to those who desire them explicitly (rather than using them for everyone). My least favoured alternative is the "He or she", an awkward construction that I particularly dislike for it's inelegance.
:PROPERTIES:
:Author: Escapement
:Score: 11
:DateUnix: 1442597170.0
:DateShort: 2015-Sep-18
:END:

**** [deleted]
:PROPERTIES:
:Score: -1
:DateUnix: 1442604277.0
:DateShort: 2015-Sep-18
:END:

***** It /is/ technically correct, insofar as there's such a thing in English.

Use of the singular /they/ predates the prescriptivists who decided it was disallowed, and it continued in common usage the whole while. It's no more ungrammatical than split infinitives are.
:PROPERTIES:
:Author: MugaSofer
:Score: 6
:DateUnix: 1442609043.0
:DateShort: 2015-Sep-19
:END:


*** They, or he and she chosen randomly per person we are asked to imagine.
:PROPERTIES:
:Author: Charlie___
:Score: 9
:DateUnix: 1442596548.0
:DateShort: 2015-Sep-18
:END:


*** Depends on why I'm being gender neutral.

If it's a /hypothetical/ person I'm using in an example, I flip a coin to decide whether I'm using he or she, or otherwise just avoid using anything gendered. For example "when a Jedi draws [flip coin, tails] her lightsaber". If it's a persistent hypothetical person, I'll keep gender the same for as long as I'm talking about [flip coin, heads] him.

If it's a /real/ person whose gender I don't know, I'll generally go with "they", though sometimes to avoid this I'll make a token effort to determine gender (on reddit it's easy to check user pages).

"It" is usually considered offensive; I haven't ever met someone who liked being called an it. Pronouns like "ze" and "xe" I find jarring, though I understand why people use it.
:PROPERTIES:
:Author: alexanderwales
:Score: 5
:DateUnix: 1442596959.0
:DateShort: 2015-Sep-18
:END:


*** I find myself in a quandry. If ze or xe were common enough for me to not get looked at funny using them, then I would use them. They are not, however, so I do not.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 2
:DateUnix: 1443005576.0
:DateShort: 2015-Sep-23
:END:


*** I vote for "they".
:PROPERTIES:
:Author: TimTravel
:Score: 1
:DateUnix: 1442623133.0
:DateShort: 2015-Sep-19
:END:


*** 1, 4, 5 (ey/em/eir) after reading a story by Alicorn.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1442700478.0
:DateShort: 2015-Sep-20
:END:


*** 4 or 6, but 6 needs more cultural acceptance. You can get away with it for pets and babies I tend to find.
:PROPERTIES:
:Author: RMcD94
:Score: 0
:DateUnix: 1442792799.0
:DateShort: 2015-Sep-21
:END:


** It seems like there's occasionally going around, these "What is the most rational thing to do in X situation" threads.

They're like those "Scenario" threads from spacebattles, where someone proposes a scenario and people well go back and forth deciding what the best thing to do in it is, and the original poster will act as 'nature' and describe the responses to their actions.

I sort of like the concept as a game, but I feel like things could be done to make it more rational and interesting, otherwise the original poster is basically God/game master, and can just distort things however they want. Many any attempted munchkin will know the wrath of a vengeful GM.

At what point does it lose its value as a learning tool, and starting being a very loose form of online roleplaying? Is it even really the sort of thing we should have here? I'm not sure whether or not I'm in favour of it, because it seems pretty ill defined right now.
:PROPERTIES:
:Author: Sagebrysh
:Score: 3
:DateUnix: 1442614928.0
:DateShort: 2015-Sep-19
:END:

*** I'm generally against them. To me it's like having dessert without dinner. If you don't have a story to wrap the exploits, it's just ... meaningless? I think that they have a place here, but I'd like to see less of them, ideally in favor of workshopping stories. (I also feel like people talking about their stories before writing them makes them less likely to write them because they get some of the same hedons with none of the production.) But I know this is down to personal preference and don't really think I'm in the majority.
:PROPERTIES:
:Author: alexanderwales
:Score: 4
:DateUnix: 1442618879.0
:DateShort: 2015-Sep-19
:END:

**** For being against them I do tend to see your responses a lot in those type of threads which defeats your implied wish of minimising them.
:PROPERTIES:
:Author: RMcD94
:Score: 0
:DateUnix: 1442792899.0
:DateShort: 2015-Sep-21
:END:


** Two questions.

1. Do you ever find yourself arguing with non-rationalists, and trying to use rational arguments to convince them of something, and they refuse to accept your point? For example, I got into an argument with an older lady (I'm 22, she is about 40) about colds. She said I shouldn't go in the rain, because I would catch a cold, due to both the general cold of the water and the cold inflicted on me by the wet clothes drying on me. I said that I wouldn't, as the cold isn't caused by low temperatures, it's caused by a type of virus. She proceeded to tell me about how her mother, who was very wise, told her that growing up, and she noticed that it was indeed true. I told her that it's more likely she was either incorrectly remembering, or selectively remembering things. That it was likely she mentally disregarded the times she caught a cold without being wet, subconsciously. She proceeded to accuse me of always trusting science over the wisdom of elders. I proceeded to say "I'm not saying old people don't have knowledge from sheer time spent on the Earth. But I definitely trust facts verified by thousands of intelligent minds, over advice from a single old person, if the two directly conflict." This argument spiraled, and the rest isn't important, as it quickly became more about how I was always a "know-it-all that trusted Google more than those who know more because they are older". Anyway. So that's the question. Have you ever tried to appeal to the rational side of a non-rationalist, only to get rebuffed? And does said rebuff ever make you almost irrationally angry?

2. Do you ever find yourself feeling unjustifiably superior, on the intelligence scale? Like, obviously, you know you aren't Einstein or Hawking. But you know that you probably know more than the average Joe on a lot of topics, as a student of rationalism, or even just someone who likes to read or learn new things. It also seems to take someone smarter than the average bear (/he he he/) to really grasp some of the basic concepts of rationalism, meaning you can almost assume a budding rationalist is smarter than average (please regard the *almost*. I'm not making a concrete statement, I haven't done any formal research on this, it just seems like a sensible conclusion, based on what I've seen of many core rational elements, and the knowledge it takes to begin grasping them) So, as a result of said intelligence, do you find that, often, you're disregarding the opinions of those around you as almost lesser than your own? This isn't a good thing, as you are obviously only one person. You will be biased and/or wrong a lot, naturally. But this still seems like an easy trap to fall into, especially when you aren't in an academic setting, so you find yourself just naturally more intelligent than those around you (And yes, I know, intelligence is an abstract concept, but come on. Please infer what I mean, here.)
:PROPERTIES:
:Author: Kishoto
:Score: 3
:DateUnix: 1442646168.0
:DateShort: 2015-Sep-19
:END:

*** Neither actually....

1. [Note that these strategies are for when arguing with people who refuse to pay attention to my side of the argument] I usually do my absolute best to avoid arguments with people on how to do some task which I don't consider to be important. My reasoning is that it's easier to just go along with what they need done, since the amount of time it takes for me to change their mind is often more than just letting the results prove them wrong. If the argument is important enough (such as money is at stake, this is my domain of expertise, or we are about to start on a time-consuming project), then I often can demonstrate why I'm more likely to be right such as being educated in this field, prior experience with this task, or some way to prove why I'm one of the right people for the job. Don't appeal to *logic*, appeal to *authority* of being in a higher-level position, or appeal to the *emotion* of don't-you-trust-me?, or appealing to their *laziness* (I know better and you can just let me take responsibility for any potential shit-hitting-the-fan problems). It's dipping into Dark Rationality, but these kind of people aren't going to listen to logic, you need something else. If it's to do with facts, then if they aren't going to listen to the reasons for my answer in the first place, I usually just prove a link to some respectable source from online (this is a bit hit-and-miss at times). If that still doesn't work, I walk away. They aren't going to ever listen to me, so why spend more time with them? I am the /master/ of walking away and not letting it bother me ever again.

2. I'm surrounded by smart people and I make mistakes in a ridiculous variety of situations. I /know/ I am not smart. I have been called a genius by several people, and one person (after watching me mess up in a social situation) asked me if I was an idiot savant. However, I just feel awkward when that happens. My reasoning is that I'm lucky to like learning for its sake and I'm very well-educated. So I just chalk these comments to people seeing a well-educated person for a genuinely /smart/ person.
:PROPERTIES:
:Author: xamueljones
:Score: 4
:DateUnix: 1442721927.0
:DateShort: 2015-Sep-20
:END:

**** 1. YES. I usually try to tell myself this, especially when I'm familiar with the people I'm arguing with ("Come on Kishoto. These people have never read HPMOR. They don't know what lesswrong is, they probably only know the word fallacy from an English class from way back in school. It's irrational of YOU to try and use a rationally based argument to convince them, as they don't have the tools for it. Or are already biased in some manner due to upbringing, personality, etc. and refuse to let their viewpoints change) but I can never stick to doing it. When I get into the argument, I forget this and just default to verbal sword fighting, and a rationality-based sword shatters against ignorant defenses. I really need to take your paragraph and like save it on my phone, so I can try and enforce it on how I think. Especially since I suck at walking away from shit (despite the fact that it's easy to see when someone's hunkering down and refusing to change their viewpoint) because I love arguing, and hate willful ignorance.

2. I, unfortunately, am not surrounded by smart people. I, myself, am fairly intelligent, but I don't really hang around any one that is. Most of my friends are in other countries, so we communicate via Skype and such, and THEY are fairly intelligent. But the people I interact with at my job (a call center, where I'm in management) or in my daily life aren't. Even if schooling isn't really a factor (although I can bring that in too, depending), they can't seem to make the intuitive leaps that i'm able to (holy hell, reading that sentence sounds so arrogant. Argh.) For example, when the Malaysian plane went missing last year, and weeks passed without anything substantial being found, I heard so many people at work talking and this is how this conversaiton started:

"We have all this fancy technology. We can google people and find what street they're on, how can you not find an entire plane?"

"Well. Those people you "google" are in civilization, firstly, assuming you can find them at all, it's because there's some data being transmitted, via their phone, their workplace, etc. You don't have any of that in the ocean. Plus the ocean is much larger than people give it credit for."

"That makes no sense. I can contact someone in China and we can't find this one plane between all these countries? What about radar and shit?"

"Well, firstly. Even if we look at all of the detection methods we have, a lot of them need some form of thing to bounce back off. Something transmitting. That's how you track things. If the tracker's disabled, you're stuck trying to identify it physically. And there are huge range limitations on things like SONAR and stuff, and that's not accounting for noise."

"Well, how do we know it even crashed? Maybe it was stolen and landed on some island?"

"While technically, that may be true. Realistically, having some secret airport on a deserted island large enough to land that size of a plane is very unlikely. Plus, why would you bother? The amount of money it would take to set up said airport and sustain it would be ridiculous. And air space is heavily monitored, so it's not like you'd even be able to make much use of it. And this flight didn't have anything that important on it, human lives not withstanding."

"Well, what if they just flew it to another country, huh? Where they had all that stuff set up illegally already."

"Again. Airspace is heavily monitored, pretty much world wide. Especially with planes that size. Not to mention, fuel limitations. It couldn't just keep flying indefinitely. At best, it could've made it to like India or something. Maybe."

"Well, how would you know? You're not a pilot or smuggler or anything."

"I don't have to be to know the things I just said."

The conversation then went on for a while longer (too long, I'm ashamed to admit) and I walked away feeling as if 80 percent of what I said went over their heads, as they all went right back to puzzling out how the plane was probably stolen and landed on some super secret island.
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1442776845.0
:DateShort: 2015-Sep-20
:END:


*** [deleted]
:PROPERTIES:
:Score: 1
:DateUnix: 1442648018.0
:DateShort: 2015-Sep-19
:END:

**** u/Transfuturist:
#+begin_quote
  But I generally feel that I'm justified in feeling at least slightly more intelligent than other people simply because I've read more than them. People find it fascinating that I know random facts like Lavoisier discovered Oxygen, or that the price of Berkshire Hathaway stock is 190,000$. Come on people, read more.
#+end_quote

Trivia is generally used to feel superior to others, so I can at least understand why you feel that way. I'd like to hear about a time when that sort of information effected a beneficial outcome for you.
:PROPERTIES:
:Author: Transfuturist
:Score: 1
:DateUnix: 1442700669.0
:DateShort: 2015-Sep-20
:END:


*** 1. All the time. Recent example: My mother trying to convince me of christian end time prophecies. She asked me to do research on them, so I did, and they strike me as the same brand of bollocks as horoscopes and suchlike. However, I know that if I brought that up to her she'd get really defensive or -- well, she seems almost psychotic when she talks about them. Really not comfortable for me.

2. I do my level best not to. Just because someone is not a rationalist doesn't mean they don't have valid (or true) opinions, even if they are sourced in (un)intuitive, unexplainable judgement calls. That's not to say it doesn't happen sometimes anyway, especially with people I know to lack sound reasoning for a lot of their stated opinions -- but people have their fields of expertise and experience regardless, in which they are vastly superior to myself.
:PROPERTIES:
:Author: Cariyaga
:Score: 1
:DateUnix: 1442650559.0
:DateShort: 2015-Sep-19
:END:

**** To address 2.), I wasn't really trying to propose that rationalist > all. I moreso just was trying to illustrate that, due to the inherent complexity of rational thinking, you can expect rationalists to, on average, have a higher than average threshold of intelligence. Obviously, as a rationalist, you should be able to acknowledge a verified expert in a field, and accept that their knowledge in said field exceeds yours. I was moreso leaning towards the common man. As in, you're in a room full of coworkers (let's assume you don't work at a place that would surround you with intelligent equals, such as a university. Let's assume you work at a walmart, or a call center, or somewhere else that's stocked with average people as employees), or family members. And you're entering discussions, and you find yourself feeling so superior, just because these people, plain and simple, *aren't as smart as you are*. Inherently, there SHOULD be nothing wrong with that, but I will admit that I, personally (for a variety of complex reasons), am inclined to look down on others that I find particularly unintelligent. Not to the point of extremity, but enough to feel superior to the point that I find myself uncomfortable with how dismissive of them I am.
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1442651757.0
:DateShort: 2015-Sep-19
:END:

***** u/deleted:
#+begin_quote
  And you're entering discussions, and you find yourself feeling so superior, just because these people, plain and simple, aren't as smart as you are. Inherently, there SHOULD be nothing wrong with that, but I will admit that I, personally (for a variety of complex reasons), am inclined to look down on others that I find particularly unintelligent.
#+end_quote

Nah, it's way more alienating than that. When there's /really/ a significant gap of intellectual capability, I can't look down on other people, because I can only weakly guess at their point-of-view, and they can't even guess at mine at all.

Too large a gap just makes me feel very alone.
:PROPERTIES:
:Score: 2
:DateUnix: 1442784941.0
:DateShort: 2015-Sep-21
:END:


***** Ah, I do see what you mean. I... admittedly, do my best to avoid people I can't have intelligent discussions with. It's extremely frustrating to be around people with whom I cannot.
:PROPERTIES:
:Author: Cariyaga
:Score: 1
:DateUnix: 1442659262.0
:DateShort: 2015-Sep-19
:END:

****** I especially hate when people are stuck in the "appeal to tradition" fallacy, particularly when it comes to things in scientific fields.
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1442671245.0
:DateShort: 2015-Sep-19
:END:


*** u/deleted:
#+begin_quote
  Do you ever find yourself arguing with non-rationalists, and trying to use rational arguments to convince them of something, and they refuse to accept your point?
#+end_quote

Mostly, /no/. I try to avoid arguing when I expect it to be neither useful nor fun. There can be many circumstances in which it's fun but I'll never convince them, but of course, in those, why even try to convince rather than to troll?

#+begin_quote
  Do you ever find yourself feeling unjustifiably superior, on the intelligence scale?
#+end_quote

Yes, when I was a teenage edgelord and into early graduate school. The lesson I eventually learned is: if you find yourself feeling unjustifiably superior on the intellectual scale, you need to turn your eyes towards new role-models who are sufficiently above your level that you can still /learn/ something from admiring them.

Feelings of superiority are a distraction from real goals for foolish mortals. In fact, they're practically the /definition/ of "foolish mortal" in at least one important sense: millenia's worth of people have thought themselves ever-so-clever for finding ways to raise themselves above other people in social-status hierarchies, as a result of which they accomplished /nothing else/ with their entire lives.
:PROPERTIES:
:Score: 1
:DateUnix: 1442784801.0
:DateShort: 2015-Sep-21
:END:


*** u/FuguofAnotherWorld:
#+begin_quote
  Do you ever find yourself arguing with non-rationalists, and trying to use rational arguments to convince them of something, and they refuse to accept your point?
#+end_quote

Yeah, when I first got into rationalism I did that a lot. As time went on I cut down on how often I did it, but it's still annoying that I can't convince my mum for example of anything under the sun. Then again she believes in crystal healing and 'energy' sharing so maybe I should just stop giving a crap. It gets painful though when she's making business decisions and I can see so clearly that she's refusing to take the obviously cheap and better option that has worked better in the past out of pride because the person that sells them was snooty to her. It's like, sure you can act that way for a 5 purchase but when it's thousands of pounds and your main income stream you should really get over it.

Rational arguments just don't /sound/ convincing to people who themselves are not rational. Maybe I should learn oratory or something, because it's fairly clear that I speak a different language once I get into a discussion like that.

#+begin_quote
  Do you ever find yourself feeling unjustifiably superior, on the intelligence scale?
#+end_quote

For a while I did. I recognised it as a not-useful and counter productive way of thinking about things, so I made the decision to consciously focus on other qualities people have when I found myself thinking that. My inner monologue sounded a bit like:

/frustration/, but really that's not an accurate summation of the whole of his worth and he does try hard to help people when they're down and out.

/frustration/ okay, but intelligence is not inverse stupidity so I need to make sure to look into this properly before I decide instead of just discounting it because he's the one saying it.

/frustration/, but I'm sure that in her own sphere of knowledge she is very knowledgeable. After all, not everyone can know everything about everything, so I'm sure there are things she could still teach me.

Worked out pretty well.

#+begin_quote
  you can almost assume a budding rationalist is smarter than average (please regard the almost. I'm not making a concrete statement
#+end_quote

I recall looking at the lesswrong site survey. The average IQ was 138.25 with a SD of 15.94, which is funny because it actually made me feel a little slow compared to the rest. Now, obviously IQ is not a shorthand for everything and yada yada yada standard disclaimer but statistically speaking you can probably indeed assume that the average rationalist has a quicker mind than average.
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 1
:DateUnix: 1443007195.0
:DateShort: 2015-Sep-23
:END:

**** I didn't know a survey was taken, but that doesn't surprise me. I feel like, to even understand some of the core ideas of rationalism ensures that rationalists tend to be more intelligent
:PROPERTIES:
:Author: Kishoto
:Score: 1
:DateUnix: 1443013616.0
:DateShort: 2015-Sep-23
:END:

***** Tend, yes. Need to be, no. It is a significantly greater time investment with less frequent payoffs for people who are slower. The survey results are interesting if nothing else.

[[http://lesswrong.com/lw/lhg/2014_survey_results/]]
:PROPERTIES:
:Author: FuguofAnotherWorld
:Score: 1
:DateUnix: 1443017473.0
:DateShort: 2015-Sep-23
:END:


*** For 2 due to the nature of being even just a reddit user you are likely demographically in the smarter than average.

First, assume that education is broadstroke correlated with intelligence. As presumably a western educated younger person you get have a higher quality education than most of the world. Most old people, most children, most poor people can all be assumed to be below average intelligence.

If you did even reasonably well at school that would be another indicator of above average intelligence. At the end of the day it's not that hard to be above average.
:PROPERTIES:
:Author: RMcD94
:Score: 0
:DateUnix: 1442793166.0
:DateShort: 2015-Sep-21
:END:


** You have a death note. Who do you kill? How do you put it to best use?
:PROPERTIES:
:Author: PL_TOC
:Score: 1
:DateUnix: 1442597610.0
:DateShort: 2015-Sep-18
:END:

*** I think there are relatively few problems that can be solved by killing. One of the big mistakes that Light makes is in thinking that killing all the criminals will eliminate criminality. This is wrong for two reasons (beyond the moral ones).

First, criminals don't really respond to incentives like that - increasing punishment changes the incentives for the crime, but that's worthless if there's no response to incentives. Look at the drug war in America. Increasing the penalties for possession to the point of absurdity hasn't done a lot to curb actual use. There have been a number of studies on this.

Second, criminality is systemic. It's a result of how your society is set up. Just killing the criminals is treating a symptom, not the actual cause. If you're using such an extreme method, you want to be sure that you're hitting the root of the problem.

There's also the issue that the Death Note can only kill people who are /caught/ for their crimes, leaving behind everyone who is smart enough to get away with murder, rape, jaywalking, etc. Once people know the rules, they'll know enough to hid their face and name. You'd probably see shootouts with the police more often, because every crime is a death sentence. It just doesn't work.

This applies to dictators as well; you don't get dictators because there's this one giant asshole making everything miserable, you get them because of systemic problems that you can't just wipe away by killing enough people. It doesn't take a terribly deep reading of history to see that this is the case. What you need is to create new, stable institutions, which you can't really do with the Death Note.

So what's it good for? You can try using it to break the world somehow, either by its (nearly?) pre-cognitive powers or some other method. You could get some money, either through blackmail or by dictating actions prior to death. You could use the pre-death mind control aspects in order to build power or make some more subtle shifts in the way of world.

Personally, I would probably use it for euthanasia.
:PROPERTIES:
:Author: alexanderwales
:Score: 7
:DateUnix: 1442598747.0
:DateShort: 2015-Sep-18
:END:

**** u/Sagebrysh:
#+begin_quote
  This applies to dictators as well; you don't get dictators because there's this one giant asshole making everything miserable, you get them because of systemic problems that you can't just wipe away by killing enough people. It doesn't take a terribly deep reading of history to see that this is the case. What you need is to create new, stable institutions, which you can't really do with the Death Note.
#+end_quote

Kevin Spacey's Character from COD: Advanced Warefare sums this up well in his 'democracy' speech:

"Democracy isn't what these people need hell, it's not even what they want. America's been running around the globe trying to install a democracy in nation after nation for a century and it hasn't worked one time. Now, why do you think that is? Because these countries don't have the most necessary building blocks to support a democracy, little things like, we gotta be tolerant of those who disagree with us or we gotta be tolerant of those who worship a different god from us or, that a journalist gotta be able to disagree with a fucking president. And you think you walk into this country based on fundamentalist and religious principles, drop a couple bombs, topple a dictator and start a democracy? Give me a break."

You can't just kill Sadam Hussain and have Iraq magically transform into a bastion of liberty and freedom, there's inertial reasons that places end up with dictators, and killing the dictator isn't actually going to change the inertia of the country.

So yeah, using a Death Note to make the world better by offing dictators won't really work too well. It at the very least will lead to increased violence and instability as the result of a power vacuum.
:PROPERTIES:
:Author: Sagebrysh
:Score: 4
:DateUnix: 1442612032.0
:DateShort: 2015-Sep-19
:END:


**** You're free to choose non-criminals. To your second point, you could express your political preference pretty effectively to bring about systemic changes.
:PROPERTIES:
:Author: PL_TOC
:Score: 3
:DateUnix: 1442600033.0
:DateShort: 2015-Sep-18
:END:

***** You would gain a great deal of power - but it would depend fully on completely maintaining anonymity in the face of virtually every powerful stakeholder in the world trying to discern your identity. While people here probably have read [[http://www.gwern.net/Death%20Note%20Anonymity][gwern]] about this... I would not count on the authorities of the world, confronted by an actual Death Note, to not use google really hard and /also/ read gwern about this. L is pretty exceptionally smart, but in real life there would be a much larger amount of manpower and in attacking hard problems quantity has a quality all it's own. I am confident that I am very smart, probably 'smarter' than the average of the people who would be told to kill me as soon as they were pretty sure of my identity... but there would be thousands of them, and only one of me, and I would have to be smarter than all of them in a large, large number of fields, all the time, or my identity would slowly leak.

Another difficulty: you might be able to persuade governments, etc, by threatening world leaders... but the ones who wouldn't comply are likely to be the true patriots who want to serve what they perceive as a higher cause (e.g. their nation, the common good, etc) over their own personal self-interest, and are therefore likely to be the ones who you should want to kill the least! Also, the legitimacy of democracy is such a powerful force for good that attempting to destroy it by personal military coup through perfect assassination is probably going to cause more net problems than the improvement from your specific policies over democratically determined policies solves, and from a pure utilitarian perspective would increase future suffering and decrease future happiness.

Honestly, the whole 'you now have proof that magic really exists, that fates are in some respect foreordained, and etc.' would really confuse the hell out of me, and I am not sure how that would factor into my planning. The 'magic is real but until now the entire world appeared consistent with the no-magic hypothesis' really would tend to indicate I was in a simulation or something.
:PROPERTIES:
:Author: Escapement
:Score: 4
:DateUnix: 1442609343.0
:DateShort: 2015-Sep-19
:END:


***** I think if it worked, the costs associated with it would be way too high. I mean ... could you hold the world hostage and get specific policies implemented? Well ... even that I'm not sure about. You're depending on people responding to the pressure of death in the way that you want them to, which I don't think is safe. /Instead/ I think that you would probably end up killing a lot of politicians, then have a second generation of politicians who work from anonymity with their faces masked and identities unknown to the public. Then you'd have to start killing everyone who agreed with that policy of avoiding death, until the bodies had stacked up high enough that people agreed that politicians wouldn't be anonymous. And even that probably wouldn't work, because they would find other ways around your tyrannical rule. That's all regardless of what your specific policy proposals are.

The history of the 20th century is one of people killing their political enemies in order to enact policy changes. The track record on this has been absolutely abysmal. To argue that you could do better with the Death Note is to argue that the real problem was in not using /enough/ force, or not using it precisely enough, which I think is a bad misreading of why the violence-based approach to changing society hasn't worked.

I think if you actually wanted to use the Death Note to change some public policy, you'd want to make sure that people don't even know that any one is being killed with it. You'd make every death look like an accident and arrange so that mortality rates didn't look /too much/ different from the real world. You wouldn't take out the people in positions of power, but the people who /create/ those positions of power. Otherwise the position of power is just going to be filled again in a few days.

And /even then/ I'm skeptical that it would be effective, because as stated, people have tried the violence-based approach many, many times before all through the 20th century and morality aside, it doesn't seem to get results.
:PROPERTIES:
:Author: alexanderwales
:Score: 3
:DateUnix: 1442704446.0
:DateShort: 2015-Sep-20
:END:


**** u/TaoGaming:
#+begin_quote
  First, criminals don't really respond to incentives like that - increasing punishment changes the incentives for the crime, but that's worthless if there's no response to incentives. Look at the drug war in America. Increasing the penalties for possession to the point of absurdity hasn't done a lot to curb actual use. There have been a number of studies on this.
#+end_quote

However, there are also studies that point that swift, sure punishment does reduce recidivism. (I'm drawing a blank on the guys name and blog....).

It is reasonable to assume that criminals are rational actors, but you must assume a discount rate (A 1% chance at life is basicaly the same as 1% chance at 20-30 years?)

If Light can get the %age up to a reasonable #, people take notice.

However, you can also punish supposedly rational actors with a rule that. "The head of a corporation that causes a massive environmental disaster dies." (etc).

You could also target the leader (and families) of despotic regimes. I think that's the best bang for the buck. "People disappear in your regime, you die."
:PROPERTIES:
:Author: TaoGaming
:Score: 3
:DateUnix: 1442612058.0
:DateShort: 2015-Sep-19
:END:


*** I start measuring the world's financiers and landholders by how much they own and how widely their assets will be disbursed upon their death.

I then think hard about whether I really think these deaths in particular will save lives, whether I can "cheat" the shinigami somehow, or whether it would just reinforce the precedent that problems are solved by killing.
:PROPERTIES:
:Score: 3
:DateUnix: 1442605350.0
:DateShort: 2015-Sep-19
:END:

**** Now I know who to suspect if the world's financiers and landowners start dying disproportionately. :P

I've had these thoughts as well. I believe a systematic campaign against the wealthiest people and their soon-to-be-wealthiest beneficiaries may indeed discourage the idea of inheritance altogether. But what happens when anonymity becomes popular among them? What happens when the wealth is, instead of concentrated in personal assets, moved into corporations and shell companies? Invested assets will probably be moved into more liquid and less regulated forms. Couldn't this result in a collapse of fractional-reserve banking? Not that FRB is by any means sensible, but I doubt this sort of approach would result in anything less than massive destabilization and catastrophe.
:PROPERTIES:
:Author: Transfuturist
:Score: 3
:DateUnix: 1442701908.0
:DateShort: 2015-Sep-20
:END:

***** What, me commie? One issue is that the wealth is /already/ concentrated into corporations, shell companies, and holding companies nowadays.

But yeah, the Death Note is a near-useless power to have for non-megalomaniacs.

But ah, here's a thought: can we use it to start some well-placed fires in places where title deeds to land are kept? Erasing titles to rent-extracting assets is a much better way to destroy a system of rentiership.

Alas, the superpower I consider most useful for actually accomplishing things I really want to accomplish takes a ridiculous effort to obtain and wield controllably -- it's a Giga Slave-type thing. But on we go, day after day.
:PROPERTIES:
:Score: 1
:DateUnix: 1442704183.0
:DateShort: 2015-Sep-20
:END:


*** As I think they realized in the show, the ability to manipulate circumstances surrounding a death is more powerful than actual death itself. But let's ignore that.

I'd become a supervillain.

Send an ultimatum to the government, probably in the middle of an obviously-contrived death (a criminal escapes jail, finds me, takes my note, drives to the whitehouse, and counts down to his death by cerebral hemorrhage alongside others travelling from completely different locations across the world - for example.) If they do not comply, I'll just kill them. The same goes for large corporations and individuals with enough personal wealth or political power to make a difference, such as dictators.

/That/ is how you become God of a new world - by forcing it's existing rulers to remake it for you as a paradise. Not by killing people the justice system has already caught one by one.

EDIT: Oh, this should be kept fairly secret, too. Otherwise you just get a whole planet coming up with ways to defeat you, which is ... bad. Keep to quiet-but-dramatic blackmail and threats.
:PROPERTIES:
:Author: MugaSofer
:Score: 3
:DateUnix: 1442612793.0
:DateShort: 2015-Sep-19
:END:

**** The Grey Lord.
:PROPERTIES:
:Author: Frommerman
:Score: 1
:DateUnix: 1442713882.0
:DateShort: 2015-Sep-20
:END:


*** I'd probably save it for later use, if some Hitleresque person comes to power.
:PROPERTIES:
:Score: 2
:DateUnix: 1442598009.0
:DateShort: 2015-Sep-18
:END:


*** My first targets are the known leaders of ISIS, because I think it is entirely possible to make it look like they died by the hand of Allah, and thus to collapse the entire group.

After that, I don't know. Killing Kim Jong Un won't actually solve any problems, and there aren't any other sufficiently evil people whose deaths I conclude could be easily manipulated into positive outcomes.
:PROPERTIES:
:Author: Frommerman
:Score: 2
:DateUnix: 1442713537.0
:DateShort: 2015-Sep-20
:END:
