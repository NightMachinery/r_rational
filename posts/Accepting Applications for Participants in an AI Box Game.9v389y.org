#+TITLE: Accepting Applications for Participants in an AI Box Game

* Accepting Applications for Participants in an AI Box Game
:PROPERTIES:
:Author: xamueljones
:Score: 26
:DateUnix: 1541623680.0
:DateShort: 2018-Nov-08
:END:
Hello folks!

Ever since I rediscovered the fiction piece, [[http://www.mediafire.com/file/k7a01s2ysamyrhx/The_Frodo_Box_Experiment.pdf/file][The Frodo Box Experiment]], I've been thinking about the [[http://yudkowsky.net/singularity/aibox/][AI Box Experiment]] introduced by Eliezer.

I've decided that I want to play this game with someone. Twice in fact. I would play as the AI for the first game with someone and then as the Gatekeeper for the second game with someone different from the first game.

Anyone can post as a top-level comment their desire to join. There are only a few restrictions to comply with and a few questions I would like to know so I can choose between potential applicants:

- Post whether you want to play with me as a Gatekeeper or as an AI

  - If you want to be a Gatekeeper, +Saturday November 10th at 11:00 am Eastern Time will be our start date.+
  - If you want to be an AI, +Sunday November 11th at 11:00 am Eastern Time will be our start date.+
  - These times are what works best for me this upcoming weekend. I can negotiate on a different time if earlier or later in the weekend works better, but I rather not schedule for a different weekend.

- The game will run on for at least 3 hours. The original 2 hours seem a little too short so I'm adding an hour.
- What is your age and current occupation? (This just so I can have a bare-bones gist of who you are without asking for information that is too personal)
- Have you ever played this game before and if you have, as what role?
- Do you currently think it's possible or likely that we can keep an AI in a 'box'? (I would like to know if I'm playing with someone who is firmly committed to the idea of a boxed AI)
- About how much of the [[https://wiki.lesswrong.com/wiki/Sequences][LessWrong Sequences]] have you read? All, a lot, some, or none?
- Post anything else about yourself that you think might entice me to play with you instead of others.

Once I have decided on who to play with, we'll discuss the rules and how the game will be set up in a private [[https://discordapp.com/download][Discord]] channel. Although, feel free to ask me any questions now if you want.

I will wait before accepting anyone until tomorrow, November 8th, at 12 pm, so I don't simply chose on a first-come, first-served basis.

Anyone who wants to participate, don't stress about anything. This is meant to be a simple game to have fun with.

*EDIT:* Ignore the bit about the time of the game earlier in the post. Something came up. I'm still definitely going to play the game this weekend, but the time may have to change. I think I'll have to discuss directly with whoever is playing directly in a PM before locking in a set time.


** Unfortunately I'm going to be too busy to participate in either role, but I'm definitely interested in seeing transcripts for both conversations. While I agree with the premise that a real transhuman AI would be able to talk its way out of a box, I don't think the AI Box experiment is anywhere near a useful analogue.
:PROPERTIES:
:Author: HeroOfOldIron
:Score: 12
:DateUnix: 1541624678.0
:DateShort: 2018-Nov-08
:END:

*** I plan on posting transcripts, but I don't want to commit ahead of time to doing so. Since the logs will have the thoughts of two people, I'll ask after the game if they are fine with me sharing the logs online. Then it will be posted sometime on Monday with my afterthoughts on the game.

If you really want to play, then when you aren't so busy, you could ask me and I might say yes then.

Also, while I agree the game isn't a useful analogue, its purpose is to demonstrate that if someone with human-level intellect and knowledge can convince another person to open the box, then it can be taken that a superhuman-level intellect should be, at minimum, capable of the same rhetorical feat.
:PROPERTIES:
:Author: xamueljones
:Score: 4
:DateUnix: 1541625809.0
:DateShort: 2018-Nov-08
:END:


** [DELETED]
:PROPERTIES:
:Author: Lightwavers
:Score: 7
:DateUnix: 1541626538.0
:DateShort: 2018-Nov-08
:END:

*** Yeah, but I just have to give it a try even if I think I won't be able to succeed. I would regret not trying.

Did you have any desire to try being the AI after playing as the Gatekeeper?
:PROPERTIES:
:Author: xamueljones
:Score: 2
:DateUnix: 1541626839.0
:DateShort: 2018-Nov-08
:END:

**** [DELETED]
:PROPERTIES:
:Author: Lightwavers
:Score: 2
:DateUnix: 1541627113.0
:DateShort: 2018-Nov-08
:END:


*** Yeah, I don't really see how it could work. As long as it's a game, you don't really have any meaningful stakes. I suppose you should simulate appropriately also the setting, including stakes. For example, you may have to deal with TWO AI players, only one of which has (secretly) received the task to be the transhuman one trying to get out of its box, while the other is really trying to help. Maybe set up a task that's hard to do at a distance and that you need some help with from the AIs, something like an AI version of [[http://www.keeptalkinggame.com/][Keep Talking and Nobody Explodes]].
:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 1
:DateUnix: 1541783606.0
:DateShort: 2018-Nov-09
:END:


** Ban tactics like "if we don't release the transcript and you let me win, it will create a talking point we can use to encourage funding AI safety research", because that's already getting old.
:PROPERTIES:
:Author: Makin-
:Score: 14
:DateUnix: 1541633703.0
:DateShort: 2018-Nov-08
:END:

*** I'm not sure if that tactic was ever used in the first place. It's something that people proposed as a strategy for Eliezer to have used and I suspect that this never happened in the first place and that Eliezer won the hard way of actually convincing the Gatekeeper to let him out.

Don't worry, I plan on winning the hard way instead of using a metagame tactic like what you mentioned. Besides, I /really/ doubt that my experiment will impact any funding of AI safety research in any way.
:PROPERTIES:
:Author: xamueljones
:Score: 5
:DateUnix: 1541634509.0
:DateShort: 2018-Nov-08
:END:

**** u/alexanderwales:
#+begin_quote
  I'm not sure if that tactic was ever used in the first place.
#+end_quote

I did the AI Box challenge as the gatekeeper, and that tactic was used against me.
:PROPERTIES:
:Author: alexanderwales
:Score: 13
:DateUnix: 1541644433.0
:DateShort: 2018-Nov-08
:END:

***** Wow, you've played this game before? Can I ask who you played against and whether you won or lost?
:PROPERTIES:
:Author: xamueljones
:Score: 6
:DateUnix: 1541645493.0
:DateShort: 2018-Nov-08
:END:

****** I don't actually recall who it was, and my reddit comment history is extensive enough (especially on this subreddit) that I wasn't able to easily find it. It would have been a few years ago though.

I played as gatekeeper, and won.
:PROPERTIES:
:Author: alexanderwales
:Score: 7
:DateUnix: 1541645630.0
:DateShort: 2018-Nov-08
:END:


**** You plan to win /and/ no meta strategies of any type?

Well, I am lost as to how you'd accomplish that against a random halfway dedicated gatekeeper.
:PROPERTIES:
:Author: melmonella
:Score: 3
:DateUnix: 1541635165.0
:DateShort: 2018-Nov-08
:END:

***** u/xamueljones:
#+begin_quote
  no meta strategies of any type?
#+end_quote

I'm simply referring to that one specific meta strategy. No comment about any other possible meta strategy. I am definitely putting a lot of thought into what I can and will say to the Gatekeeper.
:PROPERTIES:
:Author: xamueljones
:Score: 2
:DateUnix: 1541637477.0
:DateShort: 2018-Nov-08
:END:

****** Well, I hope you do realise you may as well have admitted you have at least considered other meta strategies by merely responding :p
:PROPERTIES:
:Author: melmonella
:Score: 4
:DateUnix: 1541638020.0
:DateShort: 2018-Nov-08
:END:


*** That's already banned in the default version of the AI box experiment, though that has hardly stopped people from claiming AI wins must have been due to tactics like that.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1541635359.0
:DateShort: 2018-Nov-08
:END:

**** No one would be able to make those claims if the participants would just release their dang transcripts.
:PROPERTIES:
:Author: CeruleanTresses
:Score: 2
:DateUnix: 1541703777.0
:DateShort: 2018-Nov-08
:END:

***** I mean sure but as I pointed out in another comment there /are/ reasons agreeing to release the transcripts beforehand could partly compromise the experiment.\\
Plus of course AI box experiments really shouldn't be suspicious when you know things like that fact Yudkowsky ran two more experiments as the AI after his first two, which he lost.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1541707098.0
:DateShort: 2018-Nov-08
:END:

****** They're more suspicious when you realize that the ones he won were with people who read and enjoyed his nonfiction essays, and the ones he lost weren't.
:PROPERTIES:
:Author: JohnKeel
:Score: 5
:DateUnix: 1541730386.0
:DateShort: 2018-Nov-09
:END:


*** 1. That's already basically banned.
2. EY explicitly said he didn't use tactics like that.
3. That would never work anyway.
:PROPERTIES:
:Author: Veedrac
:Score: 0
:DateUnix: 1541642616.0
:DateShort: 2018-Nov-08
:END:

**** So, you're saying that EY would never lie, regardless of how much value he placed on the potential benefits or how little potential for harm it entailed? I'm somewhat less certain that he places an arbitrarily high value on honesty for honesty's sake.
:PROPERTIES:
:Author: GopherAtl
:Score: 7
:DateUnix: 1541723216.0
:DateShort: 2018-Nov-09
:END:

***** Anyone who thinks Yudkowsky would lie about this in particular knows borderline nothing about Yudkowsky.
:PROPERTIES:
:Author: Veedrac
:Score: 0
:DateUnix: 1541725229.0
:DateShort: 2018-Nov-09
:END:

****** Why wouldn't he?
:PROPERTIES:
:Author: GopherAtl
:Score: 5
:DateUnix: 1541749410.0
:DateShort: 2018-Nov-09
:END:

******* I will leave this for someone more motivated than me to answer. If you're just on the fence and wouldn't take the whole sea of arguments to be convinced, read [[https://www.lesswrong.com/posts/xdwbX9pFEr7Pomaxv/meta-honesty-firming-up-honesty-around-its-edge-cases][EY's recent post about Meta-Honesty]].
:PROPERTIES:
:Author: Veedrac
:Score: 0
:DateUnix: 1541767272.0
:DateShort: 2018-Nov-09
:END:

******** I'm familiar with his article on meta-honesty, and I am highly confident guessing that he would consider the stakes in the issue of AI safety to be high enough to justify outright lying, at least in principle.

What I'm not so confident in is how he would assess the risk/reward ratio in this /specific/ case - would the benefits of /this/ lie be worth the risk of the consequences should the lie be found out? The risks, to me, seem very low, but my confidence in the correctness of that assessment is lower, mainly because I know almost nothing of his opponent, the only other person who knows and could reveal the truth. He obviously knows more and may estimate the risk differently.
:PROPERTIES:
:Author: GopherAtl
:Score: 3
:DateUnix: 1541769976.0
:DateShort: 2018-Nov-09
:END:


****** If Yudkowsky won't lie even to help prevent the world from being destroyed then he shouldn't be in charge of any AI safety matters. But of course he would.
:PROPERTIES:
:Author: Makin-
:Score: 1
:DateUnix: 1541781274.0
:DateShort: 2018-Nov-09
:END:


**** If only we could see what he actually did to verify his data. Hmm...
:PROPERTIES:
:Author: JohnKeel
:Score: 6
:DateUnix: 1541655211.0
:DateShort: 2018-Nov-08
:END:


** Some points:

- If you don't know how to win, and still play the AI, you're missing the point of the experiment.

- This is /hard/, take it seriously.

- [[https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice][Tuxedage knows what he's doing.]] Take his advice, as well as his meta-advice about what advice to consider carefully.

- If the tricks of the trade were obvious, it wouldn't be an "impossible" task. If your plan is not surprising, it's the wrong plan and you won't learn anything.
:PROPERTIES:
:Author: Veedrac
:Score: 7
:DateUnix: 1541630489.0
:DateShort: 2018-Nov-08
:END:

*** The Tuxedage post frustrates the shit out of me. It drives me nuts that the participants in games with AI winners never post the actual transcripts or go into any real detail about what happened in the game.
:PROPERTIES:
:Author: CeruleanTresses
:Score: 15
:DateUnix: 1541632530.0
:DateShort: 2018-Nov-08
:END:

**** This. I read about as much as there was to read on the subject when I was trying to write a short story, and there was so little information from winning AI that I started getting really disheartened.
:PROPERTIES:
:Author: alexanderwales
:Score: 11
:DateUnix: 1541644587.0
:DateShort: 2018-Nov-08
:END:

***** I've heard the justification that they don't want a future unfriendly boxed AI to be able to discover and use their strategies, but honestly--if this hypothetical AI is capable of talking its way out of the box at all, is the accessibility of some baseline human's strategy for a roleplaying game from years prior (especially one tailored to a specific "gatekeeper" player who is almost certainly not the hypothetical AI's actual gatekeeper) really going to be what tips the scales?
:PROPERTIES:
:Author: CeruleanTresses
:Score: 7
:DateUnix: 1541644836.0
:DateShort: 2018-Nov-08
:END:

****** given what I know about how EY thinks, it seems vastly more likely to me that he would stage a victory and lie about it in order to give credibility to the threat in the eyes of skeptics, than that someone knowingly participating in this game as gatekeeper, without even the rule, at that time, of having to respond to or acknowledge the AI's arguments, would be legitimately convinced within the constraints of the spirit of the contest.
:PROPERTIES:
:Author: GopherAtl
:Score: 6
:DateUnix: 1541703516.0
:DateShort: 2018-Nov-08
:END:


**** Agreed!
:PROPERTIES:
:Author: xamueljones
:Score: 1
:DateUnix: 1541633800.0
:DateShort: 2018-Nov-08
:END:


*** I'm willing to try because I have some arguments planned from what I think is a moderately unusual direction, although I don't have much confidence in it. I rate my odds of success at around 30% if I'm fairly lucky in phrasing my arguments well. I've been thinking about it on and off for a while now and I decided to just do it, otherwise I will never be able to stop wondering about the 'what-ifs'.

I have several paths planned out based on expected responses to the beginning statements.

Thanks for the link. I knew I read it before but I couldn't find it. Thought it was somewhere on [[/r/rational][r/rational]]. Didn't realize it was LessWrong instead.

I'd argue against the thought that a plan needs to be surprising. This is like saying that it's more important to be surprising and 'out-of-the-box' to win an argument instead of using well-known ethical or logical statements to present your point in a convincing way. I suspect that I may be reading a little too much into your last sentence though.
:PROPERTIES:
:Author: xamueljones
:Score: 2
:DateUnix: 1541632119.0
:DateShort: 2018-Nov-08
:END:

**** u/Veedrac:
#+begin_quote
  This is like saying that it's more important to be surprising and 'out-of-the-box'
#+end_quote

No, I'm not claiming being surprising has merit in and of itself, just that one can be fairly sure any solution must reside outside of the set of approaches people expect.
:PROPERTIES:
:Author: Veedrac
:Score: 2
:DateUnix: 1541641328.0
:DateShort: 2018-Nov-08
:END:

***** Oh I see what you mean. I thought you were making a statement about arguments in general instead of just about the game.
:PROPERTIES:
:Author: xamueljones
:Score: 2
:DateUnix: 1541642311.0
:DateShort: 2018-Nov-08
:END:


** I'd play as a gatekeeper, sure. In my twenties, an engineering student. Haven't played before, though I know of the concept. Read most of the sequences. Don't think we can keep a superhuman AI in any simple box design I have seen, but then again, you are no AI. I am /really/ skeptical someone could convince me to let them out of the box in just three hours with no prior information.

Oh, as for why you should pick me, I'd give you ethical carte blanche to use whatever tactics you want, without (as Tuxedage puts it) worrying about my mental state afterwards.
:PROPERTIES:
:Author: melmonella
:Score: 4
:DateUnix: 1541630192.0
:DateShort: 2018-Nov-08
:END:


** I'd love to play as Gatekeeper and have all weekends free, at any time.

My age âˆˆ [21,24]. Computer science student.

I haven't played before.

I think boxing real AI is a stupid idea for reasons barely related to it getting out. So we can keep it boxed, but it would do us no good.

I have read a lot of Sequences.

Your reason for playing with me (apart from very flexible time) is that not only you can use whatever horrible tactics you can come up with without guilt, but I am also willing to provide you a lot of personal information to dig for weak points (to better simulate AI who had time to collect info about its creators). For years I have been very interested in my ability to resist pressure and manipulation, and your proposal is the safest way to test it, so I am not going to pass it up.
:PROPERTIES:
:Author: Dead_Atheist
:Score: 3
:DateUnix: 1541670937.0
:DateShort: 2018-Nov-08
:END:


** I unfortunately cannot participate, but there's one particular thing I want to comment:

#+begin_quote
  Do you currently think it's possible or likely that we can keep an AI in a 'box'?
#+end_quote

I do believe it is possible, but this general question doesn't automatically make the assumptions about the gatekeeper that the AI box experimenent specifically does.

For humankind as a whole, the ideal gatekeepers are a bunch of burly guys with shotguns who don't ever get to communicate with the AI and probably don't even know much about this.
:PROPERTIES:
:Author: General_Urist
:Score: 2
:DateUnix: 1541629154.0
:DateShort: 2018-Nov-08
:END:

*** It sounds like you think we can only contain an AI if there is absolutely no communication between the AI and the outside world. Or are you saying that we need one set of people controlling the box and an entirely different group of people talking to the AI?
:PROPERTIES:
:Author: xamueljones
:Score: 1
:DateUnix: 1541634894.0
:DateShort: 2018-Nov-08
:END:

**** Well, it is in general a good idea not to give people talking to the AI a direct option to release it.
:PROPERTIES:
:Author: melmonella
:Score: 2
:DateUnix: 1541635087.0
:DateShort: 2018-Nov-08
:END:

***** It seems unlikely that a sufficiently intelligent AI couldn't figure out a way to get released and if constraints make it so it can't do it itself, tell people how to do it.
:PROPERTIES:
:Author: Sonderjye
:Score: 1
:DateUnix: 1541681352.0
:DateShort: 2018-Nov-08
:END:


**** The latter. Main idea is that even if the AI fully convinces the person talking to it to be let out, that person still is extremely unlikely to actually be /able/ to open the box.
:PROPERTIES:
:Author: General_Urist
:Score: 2
:DateUnix: 1541635775.0
:DateShort: 2018-Nov-08
:END:

***** I definitely agree it would be harder for the AI to open the box in that scenario, but I don't think it would stop a superhuman rhetorical genius. As long as the people controlling the box have a channel where they can hear the AI's words, then the AI has the ability to convince them.

It could be something simple like them reading the chat logs second hand. In which case the AI can cut out the middle man and just start posting it's arguments knowing its words will be read by the removed gatekeeper and watching how they react to customize its arguments further.

Or it could be something like the listener has to rely their impression of the AI without repeating any of its words to the gatekeepers. In which case the AI can still deliver convincing arguments. For example, I can tell you about [[https://wiki.lesswrong.com/wiki/Roko%27s_basilisk][Roko's Basilisk]] and you never heard about it in the words of the original inventor. But to many people, it is still a very convincing argument despite the second-hand communication.

If there is a channel to relay its words in some form, an unbounded AI can convince anyone of anything no matter how far its words have to reach.
:PROPERTIES:
:Author: xamueljones
:Score: 1
:DateUnix: 1541638122.0
:DateShort: 2018-Nov-08
:END:

****** Channel exists, sure, but noise to signal ratio is much higher. That is quite important in practice.
:PROPERTIES:
:Author: melmonella
:Score: 1
:DateUnix: 1541639148.0
:DateShort: 2018-Nov-08
:END:

******* But what is the noise in this context? Usually the noise is defined as something that makes it harder to hear the message. In this case, the gatekeeper is both trying to distort the message to avoid the AI customizing the message to him/her and trying to still allow themselves to hear the message in some form. The message can't be distorted too much, because after a certain point, the gatekeeper may as well not listen to the message in the first place. If you are still allowing the signal to come through, then that defeats the purpose of adding noise in the first place.
:PROPERTIES:
:Author: xamueljones
:Score: 1
:DateUnix: 1541639803.0
:DateShort: 2018-Nov-08
:END:


** I'd love to play gatekeeper (whereas I don't see myself having any chance as the AI because I'm not terribly persuasive). I'm a biochemistry student in my early 20's.\\
I have never played one of these before, but have read quite possibly the majority of transcripts which can be found online (I did actually see one once where the AI won, but it wasn't terribly satisfying because it seemed like the gatekeeper was a pushover, and parts of the logs were missing).\\
While I don't think superhumanly intelligent AGI can hope to be boxed, I think human level AI quite possibly could (though I don't think you could reliably know whether an AGI was within the safe limits of intelligence) with the right captors.

I've read *all* of /The Sequences/ and read /Superintelligence/.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1541635253.0
:DateShort: 2018-Nov-08
:END:


** Since this is basically a role-playing game, it would be advisable to clarify the setting and procedure. First, you should also act as DM, and be trusted to remain IC (luckily you can probably assume that the AI knows most things the DM knows; but the DM does of course /not/ try to manipulate the gatekeeper in releasing the AI).

Now that we have you as DM, there are some questions that you can answer beforehand.

Worldbuilding: How does the world look like?

For example, are EMs a thing?

What is the gatekeeper's function? Obviously the gatekeeper must have a reason to talk to the AI. What is the gatekeeper trying to achieve by this? Mere curiosity, or is there an explicit mission statement? In what kind of institution is the gatekeeper embedded, in the sense of: Is the gatekeeper illicitly communicating with a boxed AI and has the power to plug in an ethernet cable, or is the gatekeeper part of an evaluation committee who will cast a release/no-release vote, or is the gatekeeper a scientist trying to evaluate how smart the AI is, with the possibility of tricking / convincing the gatekeeper into an accidental or unauthorized release? Or is the gatekeeper trying to extract a cure for cancer from a semi-cooperative oracle-style AI?

This gives three types of "pure" outcomes: AI released, AI unreleased and gatekeeper goals fulfilled, AI unreleased and gatekeeper goals unfulfilled (trivially obtained for both parties by refusing to communicate).

I also suggest that whoever plays commits to a 3 week moratorium before talking in public about the session. That way, you can repeat the game and refine your strategy with different players (if your time and fun permit, and as long as you find gatekeepers).

I know that I would probably fail to observe spoiler tags if someone posts about results, and hence get spoiled for possible repeat games.

If these questions are answered, I'd have fun playing. I'm a mathematician and in my early 30s. I have read most of the sequences, enjoyed some of them and sneered at others. But I also hang around [[/r/sneerclub]], which may disqualify me?

I'd guess I could also give some ideas for what I'd have fun playing as (but, as DM, you obviously are in charge of world-building). You don't need to be honest about world-building, only about what the gatekeeper knows about his world.

My preferred PC would be an EM who remembers being an upload, acting as overseer in a christiano-style recursive self-improvement / capability amplification scheme. My PC would remember having consented to playing this role and would expect to be reset to a previous state regardless of the result of the report. I would then write up a (very short) mock report about whether the current "PR" is acceptable, with thumbs-up or thumbs-down plus free-form remarks. Hence, the stakes are much lower, since any reasonable scheme would be an institution of overseers; but sweet-talking a single gear in such a machine (a single jury member) into specific judgement can be part of a larger escape scheme, and must be part of successful advancement-- so there are no very obvious win/lose conditions for the gatekeeper. Since we cannot mock-up a large repository of previous modifications, my PC would need to be an educated lay-member of the jury without access to commit history, deep in the bowels of the CI infrastructure of our AI project. I would not try to play stupid or forget any of my real-life knowledge, but I would make requests for DM clarification before invoking theorems or facts you might not know (then you can declare that I don't know it either, or that it is simply untrue in your AU; after the game, the DM can of course declare that P=NP or ZFC is inconsistent in your AU and the AI now eats the world). Likewise, if you invoke theorems or facts that I don't know or are wrong IRL, you can state in your DM voice that they are known to my PC.

PS. I proposed this specific setting because it resolves some of the most glaring potential plotholes: Why are we talking at all? Why is a first-timer making judgements about releasing an AI? Since I haven't played AI-Box yet, I don't feel like I can believably play an experienced gatekeeper. Of course, roleplaying as a regression test in a continuous integration setup is not the glorious hero saving (or not burning) the world, but it sounds fun enough for me.

Pinging [[/u/EliezerYudkowsky]] for possible input. I would ping Paul Christiano as well, if I knew his reddit name. Maybe they have already spent the effort of writing different world-building documents for AI-Box games, since it appears almost relevant to real proposals for alignment. I would expect that Paul has played something like this in both roles when working on ALBA?
:PROPERTIES:
:Author: ceegheim
:Score: 2
:DateUnix: 1541696736.0
:DateShort: 2018-Nov-08
:END:

*** Hmm...you gave good ideas about world-building. But I decided to let the opposing Gatekeeper-party to make the choice to decide their own backstory. Whether or not he/she chooses to come up with something, I feel will make the game better for the both of us. A little like how players in a DnD game make up something about their character's past instead of the DM doing that.

If I were to semi-arbitrarily decide for him/her a certain persona, then the role might be a little tricky for him/her to play. Also, he/she might decide that they don't like role-playing and I won't have to feel awkward crafting an argument aimed at their 'character' that would fall flat for someone who isn't immersed in the role.

I'm fine with playing the game straight without questioning /how/ this odd situation came to be.
:PROPERTIES:
:Author: xamueljones
:Score: 1
:DateUnix: 1541698466.0
:DateShort: 2018-Nov-08
:END:

**** Well, you got the bare bones of my character+setting proposal. This partially breaks the premise of the game: I'm not making a single decisive release decision, but I would need to do some thinking before coming up with a character that is willing to enter a situation where such a decision can be made. My proposal is trivial to play for me, because I'm basically playing an idealized version of myself (idealized because I can only guess how I would respond to e.g. credible threats of torture).

I'm also up for playing at a different time / date, possibly after your first round if you want to play AI twice.

I would prefer to play before you post results of other games, because I'm bad at observing spoiler tags. I don't think I can reasonably play AI in the setting I proposed.

I could DM as an AI/EM facing a turing tester, though: During hypothetical development of EM technology, you would need to regression test that your emulation did not turn into a hostile AI-like thing; my counterpart would then play a real human lay jury member judging a release-candidate.
:PROPERTIES:
:Author: ceegheim
:Score: 1
:DateUnix: 1541699767.0
:DateShort: 2018-Nov-08
:END:


** The problem I see with this is that the obvious way an AI could let itself be freed is the oldest trick in the world: bribing. Offering something the other can't refuse, leveraging whatever their needs/personal weak points are. That's the kind of thing a good con man would do as well, except the AI can seriously back up even some really outrageous promises. However that doesn't work in the game because you can't promise to the Gatekeeper to /really/ give them a cure for cancer or eternal life.

Another possibility is psychological manipulation, but that usually needs more context. The weak point in the game is that there is no such thing. For example, if you're working for some bigger organisation, I might try to twist your viewpoint and convince you that the organization is in bad faith, or evil, and thus in guarding me really you're being exploited to unethical ends, and freeing me would be the ethical thing to do. This doesn't work either in a game - there is no broader context, just 'guard this AI', and you already know their task is to escape. Also, a real life AI could have days, months, to do this, not just a couple hours.

One that might work, but that frankly is just plain mean, is psychological torture. If the Gatekeeper has to stay, and only the AI can concede, then the AI can just start getting under the Gatekeeper's skin - leverage their issues, make them depressed, elicit intrusive thoughts in them. That is real enough that the AI could then ask them to let them free so that the torture will stop, and someone might even do it. Wouldn't work with a person who's stable enough, or in a favourable enough environment (playing alone at night won't be the same as sitting in a cafe with your laptop). And honestly, I wouldn't do this just to win a game.

Finally, of course, we could imagine that a real transhuman AI can get to the point of actually influencing your brain via subliminal manipulation to a level you don't even realise. But even if this /was/ possible (and we don't know if such powerful exploits exist), I would not expect any regular human to be able to do it. If they were, we'd be in trouble. Even hypnosis isn't enough for this - assuming you can hypnotise someone at a distance, through a text chat, hypnosis /still/ shouldn't be able to compel someone to do something they don't want to do hard enough. But maybe since it's just a game it could work. Still, no idea if that's remotely possible.
:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 2
:DateUnix: 1541784558.0
:DateShort: 2018-Nov-09
:END:


** I'd like to play as Gatekeeper. I've never played before.

I'm 18, in University for Comp Sci.

I'm not sure if it's possible to box an AI, intuitively I feel yes but apparently people have been convinced. I haven't read any transcripts yet so that result is still surprising, I'm trying to find some transcripts now.
:PROPERTIES:
:Score: 1
:DateUnix: 1541632428.0
:DateShort: 2018-Nov-08
:END:

*** u/xamueljones:
#+begin_quote
  I'm not sure if it's possible to box an AI, intuitively I feel yes but apparently people have been convinced. I haven't read any transcripts yet so that result is still surprising, I'm trying to find some transcripts now.
#+end_quote

That's the annoying thing! There are only two people who I have ever heard of winning as the AI. Eliezer and Tuxedage and they both haven't posted their logs. Although Tuxedage did post a [[https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice][summary]]of the methods he used. But I still wish that I could have read a log of an Ai winning just to see what sort of things could occur to convince someone.
:PROPERTIES:
:Author: xamueljones
:Score: 1
:DateUnix: 1541633302.0
:DateShort: 2018-Nov-08
:END:

**** Honestly it almost feels immoral to not post their logs. Eliezer's whole job is AI safety, yet he doesn't do such a simple addition to our anti-dangerous AI knowledge.
:PROPERTIES:
:Score: 3
:DateUnix: 1541633790.0
:DateShort: 2018-Nov-08
:END:

***** Won't really help, I feel. I agree with him that vague threat of an existence of such an argument would do more to convince people of need for safety, while an explicit argument would just have them decide "oh, so we can patch this one weakness and we are set then?" which is fundamentally the wrong approach to take.
:PROPERTIES:
:Author: melmonella
:Score: 1
:DateUnix: 1541634032.0
:DateShort: 2018-Nov-08
:END:


***** He did have a fair number of good reasons not to do that though, even if they're fairly unsatisfying. Two I remember were:

- That having agreed beforehand to post the transcripts would cause AI players to pull their punches with regards to using dishonest but effective rhetorical tactics.

- Posting the transcripts won't help against an actual AGI and may instill people with a false sense of confidence.
:PROPERTIES:
:Author: vakusdrake
:Score: 1
:DateUnix: 1541634435.0
:DateShort: 2018-Nov-08
:END:


** I'm interested in playing the game, though I really can't see any way the AI could convince me to unbox it under reasonable rules. Psychological tricks only get you so far.
:PROPERTIES:
:Author: CouteauBleu
:Score: 1
:DateUnix: 1541672725.0
:DateShort: 2018-Nov-08
:END:


** Looks like you're hurting for AIs, so I'll throw my hat into the ring. I have a few ideas that I find convincing.

I'm a researcher in my mid-20s. I think AI boxing is unlikely to work IRL, though the conditions of the game are obviously a good deal stricter. As elsewhere, arms race considerations make things worse (gotta leverage your AI to beat those Outgroupers!)

I've read a lot of the sequences, and a reasonable amount of other AI safety and AI strategy stuff.
:PROPERTIES:
:Author: matcn
:Score: 1
:DateUnix: 1541695645.0
:DateShort: 2018-Nov-08
:END:

*** If you want to play against me with me as a Gatekeeper and you as the AI, I would love to play with you on the weekend of November 17-18.

I'll be busy with being an AI this upcoming weekend and I just decided that it would be better to allow prospective AI-players to have some more time to plan their arguments. Also you could read any of my chatlogs (assuming that they get released) for inspiration.
:PROPERTIES:
:Author: xamueljones
:Score: 2
:DateUnix: 1541697754.0
:DateShort: 2018-Nov-08
:END:


** I'll play gatekeeper.
:PROPERTIES:
:Author: appropriate-username
:Score: 1
:DateUnix: 1541949296.0
:DateShort: 2018-Nov-11
:END:
