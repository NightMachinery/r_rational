#+TITLE: So You Want To Upload Yourself Into A Computer

* [[http://i.imgur.com/4B6wPoK.png][So You Want To Upload Yourself Into A Computer]]
:PROPERTIES:
:Author: ILL_BE_WATCHING_YOU
:Score: 163
:DateUnix: 1496107523.0
:DateShort: 2017-May-30
:END:

** The only story I've ever read that dealt with what's it like to have your consciousness permanently duplicated is the webcomic [[http://www.egscomics.com/index.php?id=1][El Goonish Shive]]. The copy is a physical clone created by magic, but it's still relevant to exploring this sci-fi concept and both individuals go through some excellent character development as they explore their own identities.

The relevant story arc starts on [[http://www.egscomics.com/index.php?id=180][page 180]], which is where the comic starts getting really good, though without knowledge of earlier continuity several things will confuse you. If you try the early pages and don't like them, don't worry. Literally every single stupid plot point and dumb character decision gets retconned into rationality.
:PROPERTIES:
:Author: trekie140
:Score: 26
:DateUnix: 1496109188.0
:DateShort: 2017-May-30
:END:

*** Permutation City also explores this concept in depth.
:PROPERTIES:
:Author: AntiTwister
:Score: 19
:DateUnix: 1496110486.0
:DateShort: 2017-May-30
:END:

**** EGS is still worth reading since it examines this idea in the context of people living relatively mundane lives. It's not just about the sci-fi concept, it explores the people effected by the event and how they adapt to such a situation in order to live happy and fulfilling lives.

Not only does the clone have to go through the initial shock and trauma of their creation only to later accept that they're a different person than the one they share memories with, but we see them struggle with that idea as time goes on and figure out just what the relationship between the two of them is.

Personally, it's among the most emotionally invested I've ever been in a story. Introducing the characters before the event as regular characters and then showing them develop naturally during and after the event is...I can't think of a word to describe how impactful it was on me.
:PROPERTIES:
:Author: trekie140
:Score: 10
:DateUnix: 1496155726.0
:DateShort: 2017-May-30
:END:

***** Noted, I'll give it a look. May want to make [[http://www.egscomics.com/index.php?id=180][page 180]] a hyperlink to reduce investigation friction.
:PROPERTIES:
:Author: AntiTwister
:Score: 2
:DateUnix: 1496165404.0
:DateShort: 2017-May-30
:END:

****** I really want people to start at the beginning so they understand the context and have some investment in the characters beforehand, otherwise it's just a really weird situation where a bunch of crazy stuff happens without explanation. The early pages are just strips so it's a pretty brisk read even if not all the jokes are funny and the setting doesn't make a whole lot of sense at first. InfernoVulpix explained it in more detail.
:PROPERTIES:
:Author: trekie140
:Score: 6
:DateUnix: 1496168399.0
:DateShort: 2017-May-30
:END:


**** I really really wish there more authors like Egan.
:PROPERTIES:
:Author: 5erif
:Score: 2
:DateUnix: 1496187362.0
:DateShort: 2017-May-31
:END:


*** El Goonish Shive really had to work hard to make everything introduced early on as gag humour fit into the world coherently, but I do think the author's done a quite good job of it.

In broad strokes, the story starts off as gag humour with frequent plotholes, including in the above clone situation, but soon enough the story enters a surprisingly dark plotline that, while still including many of the attributes of the early strips, show an intent from the author to take the comic more seriously. After that arc ends, the author kept the intent to take the comic seriously but steered it back to the less grim atmosphere it had before, which I feel is the best of both worlds.

From that point on, I can attest that El Goonish Shive has done a very good job of keeping track of an interconnected world and tying together past and future events even for the plot arcs of lesser importance, though there are some pieces of foreshadowing from the early days which have gone unaddressed for a long period of time because the now coherent overall plot isn't ready for the follow-through yet. Transformation is one of the most frequently seen uses of magic in the story, letting the comic thoroughly examine the associated topics, as well as the broader scope topics related to magic in the modern world such as what extent it should be spread around. The comic's been going for over fifteen years with a quite consistent update schedule of three times weekly and I've quite enjoyed the story, especially once the gag humour from the beginning began to be addressed and the plotlines became more thought out.
:PROPERTIES:
:Author: InfernoVulpix
:Score: 15
:DateUnix: 1496120599.0
:DateShort: 2017-May-30
:END:

**** 100% agree. You're the first person I've met who is also a fan of this comic, which makes me happy since it's one of my favorite stories ever and has had a big impact on me over the years. I picked it up at a time when I was unsure what kind of person I was and what kind of life I was going to live, and this story about teenagers making intelligent decisions and discovering their own identities reassured me that I could /do it/.

As someone who didn't intuitively understand emotions, including my own, it was incredibly fulfilling and formative to see characters intellectually analyze themselves and succeed at self-actualization. It also ended up being my introduction to a lot of LGBT concepts I didn't /get/ before. One of the characters has as arc about finding out they're gay, and it managed to make me feel like I was in their shoes despite having no doubt about my own sexuality at the time.

The characters also react to the supernatural in a really interesting way. Everyone, including background extras, treat the supernatural like any other part of the world they don't understand and don't usually care all that much when it isn't a big part of their lives. The Men in Black are actually pretty easygoing and have logical reasons for it, while still taking danger seriously and wanting to help people.

When serious danger does present itself, though, the comic almost becomes a deconstruction of teens fighting the supernatural. These events have a lasting impact on their lives for better and worse, their skills and resolve are always put to the test, and even then they still need help from outside forces because they aren't that powerful or experienced. They don't even look for trouble, just fight back when it comes for them.

Not that this is a dark story, much of it is meant to be comedic even though the context is rational. Painted Black definitely has some very dangerous situations and a villain who's realistically psychotic, but no one has anything like PTSD and many story arcs focus on the characters living relatively mundane lives. It's not an action adventure or slice of life, just good characters going through good development.

I should acknowledge some of the comic's flaws, though. The author's understanding of gender identity and sexual orientation have improved over time so there is some unintentional misrepresentation, but it has been justified in-universe as the characters improving their own understanding with time. Also, despite characters using transformation magic to change gender, transpeople have only recently appeared in the story.

While I think the story is really about the character development and the setting is one of the most rational urban fantasies I've seen (after retconning some early stuff), the myth arc is very slow and convoluted. The author will introduce new plot threads only to drop them and pick them back up years later, only to use their resolution to lay the groundwork for future story arcs that won't be resolved until years later.

The author has also admitted that individual story arcs tend to take longer than he intended since he keeps adding in scenes as he goes. Slow pacing is very common in the webcomics I've read and the plot is nowhere near as overcomplicated as Sluggy Freelance or Schlock Mercenary, but I still recommend reading the author's commentary when you're confused and you will get confused at least a few times.
:PROPERTIES:
:Author: trekie140
:Score: 8
:DateUnix: 1496154325.0
:DateShort: 2017-May-30
:END:


*** I highly recommend the [[https://www.amazon.com/dp/B01LWAESYQ][bobiverse series]] to you as a second point of reference.
:PROPERTIES:
:Author: literal-hitler
:Score: 8
:DateUnix: 1496117838.0
:DateShort: 2017-May-30
:END:

**** I adore the Bobiverse books, but I see a lot of people get turned off because the author takes the hammer to the knees of a lot of stupid shit, like religion and theocracy, with gusto.

For me, it's a plus, but if you're the sort of person to either cherish their fantasies or become offended on behalf of others, fair warning.

Bob and religion do not mix well.
:PROPERTIES:
:Author: Arizth
:Score: 3
:DateUnix: 1496188592.0
:DateShort: 2017-May-31
:END:

***** Thirding the Bobiverse. The religion is a comedic straw man, but it does make transhumanism approachable.

The multiplicity idea here is well touched on within the flower prince trilogy as well
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 1
:DateUnix: 1496361491.0
:DateShort: 2017-Jun-02
:END:


*** David Brin's /Kiln People/ explores the idea quite well I think. It doesn't strictly speaking deal with "uploads" but it's about a world where copying your mind into what are effectively short-lived golems is commonplace. Not a particularly philosophical piece, but it plays enough with the premise to provoke a lot of thought.

:edit: Well, none of the copies are permanent, so not quite the same thing, but given that high-quality copies tend to live with the expectation of reintegration at the end of their lives, very similar situations arise.
:PROPERTIES:
:Author: GopherAtl
:Score: 6
:DateUnix: 1496154313.0
:DateShort: 2017-May-30
:END:


*** The bobverse books deal with it well. He freezes himself them is uploaded into a self replicating interstellar probe, but other countries have also done this. Because he committed to co-operate with copies of himself, most copies do (so far).
:PROPERTIES:
:Author: CellWithoutCulture
:Score: 2
:DateUnix: 1496188442.0
:DateShort: 2017-May-31
:END:


*** That sounds really interesting, I'll have to give it a read.
:PROPERTIES:
:Author: Imperialgecko
:Score: 1
:DateUnix: 1496116690.0
:DateShort: 2017-May-30
:END:


*** Thomas Reiker was my first exposure to a physical duplicate that grappled with the same issues of identity.
:PROPERTIES:
:Author: wren42
:Score: 1
:DateUnix: 1496172769.0
:DateShort: 2017-May-31
:END:


** Yay!

(Watching people repost stuff you made is actually really enjoyable.)
:PROPERTIES:
:Author: FeepingCreature
:Score: 11
:DateUnix: 1496180099.0
:DateShort: 2017-May-31
:END:

*** It's funny, because I actually found it on the space battles thread you posted it in.
:PROPERTIES:
:Author: ILL_BE_WATCHING_YOU
:Score: 6
:DateUnix: 1496182256.0
:DateShort: 2017-May-31
:END:

**** Well it makes sense, because I found that thread by it being posted here. :)

~/the ciiircle of liiife/~
:PROPERTIES:
:Author: FeepingCreature
:Score: 9
:DateUnix: 1496182921.0
:DateShort: 2017-May-31
:END:


** With uploads, and/or the ability to back up your own mind-state, I'm expecting a new class of crime: partial-murder. This is where you cause someone to lose some time (because you killed an instance of that person), forcing a restore from backup. The severity of partial murder varies on how much the person has lost.
:PROPERTIES:
:Author: ansible
:Score: 8
:DateUnix: 1496131548.0
:DateShort: 2017-May-30
:END:

*** I mean, we already have murder in the first degree, second degree, etc.
:PROPERTIES:
:Author: TastyBrainMeats
:Score: 3
:DateUnix: 1496166107.0
:DateShort: 2017-May-30
:END:

**** [[/twisquint][]] "You are hereby accused of murder in the zero-point-three-one-eighth degree. How do you plead?"
:PROPERTIES:
:Author: CCC_037
:Score: 6
:DateUnix: 1496216885.0
:DateShort: 2017-May-31
:END:

***** u/TastyBrainMeats:
#+begin_quote
  *Mr. Pump*: 'I Worked It Out. You Have Killed Two Point Three Three Eight People.'\\
  *Moist von Lipwig*: 'I have never laid a finger on anyone in my life, Mr. Pump. I may be --- all the things you know I am, but I am not a killer! I have never so much as drawn a sword!'\\
  *Mr. Pump*: 'No, You Have Not. But You Have Stolen, Embezzled, Defrauded And Swindled Without Discrimination, Mr Lipvig. You Have Ruined Businesses And. Destroyed Jobs. When Banks Fail, It Is Seldom Bankers Who Starve. Your Actions Have Taken Money From Those Who Had Little Enough To Begin With. In A Myriad Small Ways You Have Hastened The Deaths Of Many. You Do Not Know Them. You Did Not See Them Bleed. But You Snatched Bread From Their Mouths And Tore Clothes From Their Backs. For Sport, Mr Lipvig. For Sport. For The Joy Of The Game.'
#+end_quote
:PROPERTIES:
:Author: TastyBrainMeats
:Score: 10
:DateUnix: 1496231182.0
:DateShort: 2017-May-31
:END:

****** [[/twibeam][]] A well-placed quote! Well done.

[[/sp][]]

[[/twiponder][]] But does this imply that a form of partial-murder already exists?
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1496237219.0
:DateShort: 2017-May-31
:END:

******* In a golem's mind on the Discworld, at least.
:PROPERTIES:
:Author: TastyBrainMeats
:Score: 3
:DateUnix: 1496238182.0
:DateShort: 2017-May-31
:END:

******** [[/twiponder][]] I don't think Mr. Pump is really wrong, though. Killing someone removes 100% of their life; cutting someone's life expectancy by 50% would thus be half a murder.
:PROPERTIES:
:Author: CCC_037
:Score: 3
:DateUnix: 1496245604.0
:DateShort: 2017-May-31
:END:

********* Oh, no, there's definitely merit to his way of thinking. I have to agree with that.
:PROPERTIES:
:Author: TastyBrainMeats
:Score: 2
:DateUnix: 1496249390.0
:DateShort: 2017-May-31
:END:


******* Yes, definitely. Consider choosing to deny someone access to medicine for their chronic or terminal illness, when you could just as easily grant it. If that's not manslaughter by portions, the crime of manslaughter can scarcely be said to exist.
:PROPERTIES:
:Score: 2
:DateUnix: 1496244760.0
:DateShort: 2017-May-31
:END:

******** [[/twiponder][]] ...yeah, that's reasonable.
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1496246563.0
:DateShort: 2017-May-31
:END:


*** Is there such a crime as felony deletion of personal data? In any case I think that depending on severity it probably counts as assault more than murder.
:PROPERTIES:
:Score: 1
:DateUnix: 1496219150.0
:DateShort: 2017-May-31
:END:


** u/wren42:
#+begin_quote
  "The evidence that you are [conscious] is exactly the same as the evidence that your upload is."
#+end_quote

Bullshit. Maybe to an /outside observer./ But not to myself. The existence of my subjective experience is the /only/ thing I can be 100% certain of from a bayesian perspective. Whether a peice of software simulating my personality is conscious is no where near 100% for me.
:PROPERTIES:
:Author: wren42
:Score: 8
:DateUnix: 1496172677.0
:DateShort: 2017-May-31
:END:

*** u/FeepingCreature:
#+begin_quote
  Bullshit. Maybe to an outside observer. But not to myself.
#+end_quote

Exactly to yourself! That's the fun part.

Your consciousness has direct access to the fact that you are conscious, but your /reasoning/ only has indirect access to this fact via a sensory tap from your consciousness. There is nothing privileged about this input.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1496180293.0
:DateShort: 2017-May-31
:END:

**** It seems we're having the same conversation across two subs =)

I'm pretty confident in this one point, however. Mathematically, in a Bayesian sense, I will always have more evidence that I am conscious than that another arbitrary agent is conscious.

I have 100% certainty that I am conscious. I do not have 100% certainty that an arbitrary program I am observing is conscious. Passing a Turing test is insufficient, as (sufficiently) correct answers could be selected at random from a hat, and provide no clue as to the underlying operation.

I can never have access to another agent's "sensory tap" as you put it, so the weight of evidence will always be imbalanced.
:PROPERTIES:
:Author: wren42
:Score: 4
:DateUnix: 1496181047.0
:DateShort: 2017-May-31
:END:

***** I think I could be convinced that I am not actually conscious. For instance, I could be shown the part of the brain that computes consciousness and the part of the brain that computes reason, and the link between them that informs my reason that I am conscious, and I could be shown the emptiness in my brain where the first part was and the electrode connected to the second part, which would force me to conclude that I am not actually conscious.

At that point, the sensation of being conscious would lose its discriminative value.

I think at that stage I'd start believing in a model such as that espoused by [[http://www.rifters.com/real/Blindsight.htm][Blindsight]], where consciousness a vestigial cognitive organ that would occasionally be discarded.

See also: [[http://lesswrong.com/lw/jr/how_to_convince_me_that_2_2_3/][How to Convince Me that 2 + 2 = 3]].

It is very dangerous for a Bayesian to believe anything at 100%.

My belief that I am conscious is /more certain/ than my belief that another is conscious, because there are more steps in the chain for the other, but they are not /fundamentally different types of belief/.
:PROPERTIES:
:Author: FeepingCreature
:Score: 3
:DateUnix: 1496181198.0
:DateShort: 2017-May-31
:END:

****** I absolutely cannot fathom that. The idea that consciousness is an illusion makes no sense to me.

An illusion TO WHOM?

#+begin_quote
  I could be shown the part of the brain that computes consciousness
#+end_quote

Who is the "I" in this scenario? How are you being "shown"? Who is "seeing" those parts of the brain?

If there is no observer, these statements make no sense.

you posit a "part of the brain that computes consciousness" and "part of the brain that computes reason" but do we even have any evidence these things exist, and are separate?

EDIT:

Hold up! What you just posited was the possibility of P zombies, which I always took to be a huge no-no for uploaders. If you accept P zombies, why the heck should we believe software copies are conscious?
:PROPERTIES:
:Author: wren42
:Score: 7
:DateUnix: 1496192195.0
:DateShort: 2017-May-31
:END:

******* u/FeepingCreature:
#+begin_quote
  Who is the "I" in this scenario? How are you being "shown"? Who is "seeing" those parts of the brain?
#+end_quote

Non-conscious people can still reason. Though admittedly it's hard to believe that they can reason about themselves. Maybe they'd have to relearn what consciousness gives you intuitively.

#+begin_quote
  Hold up! What you just posited was the possibility of P zombies
#+end_quote

Far from it. We're not talking about a mind who is in all respects identical to a mind that is conscious; we're talking about a mind that had its seat of consciousness /physically/ subtracted. A P-Zombie is a mind that had its /qualia metaphysically/ subtracted. What I described is more of a ... Q-Zombie?

A P-Zombie behaves like a human despite feeling nothing on the inside. A Q-Zombie believes they /feel/ conscious despite having no consciousness. Mind you I don't believe that a Q-Zombie behaves identically, I just think they /think/ they do. The diagnostic in their brains that says they're conscious is set permanently to "on".

If we allow for the existence of Q-Zombies, then no reasoner can decide /even from the inside/ that they are conscious with 100% certainty.

As opposed to P-Zombies, Q-Zombies have precedent in neuroscience; there exist disorders that destroy parts of our sensory apparatus while denying us the ability to even /notice/ that we've lost them. Read Blindsight!
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1496206499.0
:DateShort: 2017-May-31
:END:

******** u/wren42:
#+begin_quote
  Non-conscious people can still reason.

  we're talking about a mind that had its seat of consciousness physically subtracted.

  A Q-Zombie believes they feel conscious despite having no consciousness.
#+end_quote

Where in the /world/ are you getting these ideas? I suspect you have a very different definition of consciousness than I, and most scientific and philosophical literature I've encountered.

You seem to believe there is some sort of "consciousness organ", or specific part of the brain that makes you able to experience qualia or not.

This is not supported by Neuroscience or any of our best guesses about the nature of consciousness.

Your definition is also inconsistent:

#+begin_quote
  there exist disorders that destroy parts of our sensory apparatus while denying us the ability to even notice that we've lost them
#+end_quote

Sensory apparatus is not the same thing as consciousness! A blind person does not cease to be conscious. Even destroying all sensory nerves leading to the brain would not destroy consciousness, if imagination and internal monologue were still possible. Senses are integral to the experience of qualia, but not /identical/ to the experience of qualia itself.

There is no single part of the brain associated with "reason" or "consciousness" that can be independently disconnected.

Basing your metaphysics off a sci-fi novel is perhaps not the wisest course. ;) I will check out the novel, though, as it sounds interesting.
:PROPERTIES:
:Author: wren42
:Score: 5
:DateUnix: 1496235382.0
:DateShort: 2017-May-31
:END:

********* u/FeepingCreature:
#+begin_quote
  You seem to believe there is some sort of "consciousness organ", or specific part of the brain that makes you able to experience qualia or not.

  This is not supported by Neuroscience or any of our best guesses about the nature of consciousness.
#+end_quote

Interesting.

Generally I'd assume that, given some feature of the human mind, that there'd be a set or system of neurons that enables it. Is this not the case with consciousness? Then what exactly does our sense that we are conscious measure?
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1496241490.0
:DateShort: 2017-May-31
:END:

********** a "set or system of neurons" possibly, but this doesn't mean it's a static, identifiable peice that can be cut off from the rest of the brain.

The phenomena of consciousness is necessarily entangled with a slew of functions in the brain and nervous system.

if you believe software can be conscious, then you already accept that consciousness is not a specific physical organ, but a state in a complex network.

Imagine it as a video game running on a computer. Where does the game itself reside? Well, there's game data being recalled from the hard drive, a save file tracking progress, a processor crunching through the code, bits being pulled into and out of memory, a video card processing graphics, a sound card processing audio, the monitor and speakers outputting these, a keyboard and mouse receiving inputs -- all are parts of the experience and identity of the "game", but physically you can't point to any specific bits flipping or electricity running through circuits and say "that's the essence of the game." The game is the sum of all this activity.

you can close the game, and the circuitry stops working, or you can remove components of the computer and it will be unable to run the game, but that doesn't mean that component IS the game.

If you believe software such as this can be conscious, then consciousness cannot be something you can turn on and off without impacting the rest of the system. It IS the system.

I remain skeptical that software can house consciousness (not disbelieving, just skeptical) but I agree with the deeper implication that consciousness is a /property/ of certain matter, not the matter itself. And, by extension, that consciousness is a spectrum, not a binary switch can be turned on and off without impacting the wider system.

I'm curious where you believe consciousness resides -- you've stated that the ability to reason is separate from consciousness, but that senses are somehow tied to it.

What components of the brain are inextricable from consciousness, and which are superfluous in your opinion?
:PROPERTIES:
:Author: wren42
:Score: 4
:DateUnix: 1496246508.0
:DateShort: 2017-May-31
:END:

*********** That's a bit of a bad example because you can totally disable specific functional parts of a game. Most games even have commands for it.

Personally I think consciousness probably functions as a shared workspace in which sensory data and metadata is projected, that allows for reorganization and reinterpretation as well as comparing with model prediction. I think some of the senses that feed into this workspace are meta-level reports like awareness, which would mean reason would be inoperable without it, whiiich is actually a pretty good argument against q-zombies now that I think about it. Hmm. I guess I would say that a sense like that would be pointless if it wasn't sometimes off, so there's probably modes of reason that bypass it. Not sure though.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1496247183.0
:DateShort: 2017-May-31
:END:

************ yeah it's a weak example, only meant to talk about the relationship between software and hardware, but you clearly get that from your workspace analogy, I think.

I'll give that metaphor some thought as it's interesting and seems to describe a possible functional behavior of consciousness. it may not be the complete picture, though, and I'm not sure I follow all your inductive steps.
:PROPERTIES:
:Author: wren42
:Score: 2
:DateUnix: 1496248150.0
:DateShort: 2017-May-31
:END:


** There's a bit of a logical disconnect between "there are no p-zombies, so every you is real and must be treated as a person" to "forking-and-deleting in pursuit of higher goals is perfectly fine, and is in fact a good idea." If it's okay to abuse and delete forks of yourself, it's also okay to abuse and delete your first uploaded self.

My personal opinion is that it's okay to delete copies of yourself, so long as they have some sort of say over whether they get deleted or not.

However - I do wonder if, for example, testing a Crucio Button on yourself, to develop an appreciation for the magnitude of pain it causes, then setting things up so that your forks are allowed to press the button and cause themselves immense pain in order to signal that they want to continue existing. Or having some kind of computational currency worth X subjective seconds in Y conditions, and allocating that currency to each fork you create. Or having some sort of symmetrical system whereby there's a non-zero chance that any sufficiently-close fork will overwrite Prime upon termination, with an automatic backup system to reverse things in case of emergency. Or establishing a baseline degree of change you find acceptable, only terminating forks that stay below that baseline. Hmmm
:PROPERTIES:
:Score: 10
:DateUnix: 1496116704.0
:DateShort: 2017-May-30
:END:

*** u/LeifCarrotson:
#+begin_quote
  "forking-and-deleting in pursuit of higher goals is perfectly fine, and is in fact a good idea."
#+end_quote

This is particularly relevant because the first upload is unlikely to be a pleasant experience. I expect that early pioneers will have sensory deprivation or saturation problems, phantom pain, sleep problems, and general corruption and degradation of the clone state. Perhaps thousands, millions, or untold billions will die for this cause.

But if it means that we can actually solve the pesky biology problem of death, that's a sacrifice I would make.

One distinction I would add, too, is that a fork is not a new consciousness out of nothing. It shares commitments and, hopefully, sentiments with the historical version.

This is obvious when it comes to duplicating an uploaded human: it's an identical collection of bits, perhaps we label one "A" and the other "1". Does it matter which was the original? No. Both remember being the original, and both remember, for example, deciding that clone "A" will work on a particular task and, on completion, self-terminate, while clone "1" will get the needed results of that task and continue surviving on their limited power budget or whatever motivated the fork.

For the initial upload, yes, one now exists in carbon chemistry and the other exists in silicon. And the upload is certainly a big event! But I don't think it's particularly different than asking if a person is or is not the same person after saying a marriage vow or graduating from a military academy, or, on a less significant scale, just being a slightly different person than they were one second ago.

Talking about "higher goals", let's put this in perspective. I would be willing to make significant sacrifices for my son - he is very important to me. If that means giving up some component of my income, social life, and personal time, well, that's just plain old parenting, which I am doing now. If it means that I need to confront a gunman in my home or run into a burning building, I would do that for him - though those are unlikely, contrived situations, I would be willing to give up my life to save his. If, in this context, it means instantiating a copy of myself, expecting that they will suffer and die, I would make that sacrifice. I understand this could read as a horror short to some worldviews, so stop reading if you're sensitive to the issue.

But I would make that sacrifice again, and be grateful for the opportunity to make that greatest sacrifice twice. And three times. And ten, a hundred, a thousand million billion times. (And yes, I would be grateful that when they got it figured out I could actually upload myself successfully.)

But consider what you would do if you blinked and opened your eyes to a text prompt stating "Hello LeifCarrotson, we hope this session 008364729 of the consciousness hosting software patches the excruciating pain and wretching nausea you reported when we attempted to improve the simulated vestibular system last time. If it does feel better, please select "Yes" and we can continue working on other bug reports and research efforts, or choose "No" and describe your pain levels and nausea. You may also select "RESET" to revert to the initial upload, if the remembered pain and suffering are too much for you to function, but be aware that some of your memory of how to use the interface and of past research efforts and symptoms will be lost. And, finally, you can select "STOP" if you do not wish to subject yourself or any future instances of yourself to more of this experience."

What would you do? How manyâ€‹ times could you push "No"? I think I could last quite a while: not a billion times in a row, for sure - no human is that fool-proof, I'd probably mis-type STOP in the interface before that, but with a few forks forking themselves it could be a staggering amount of suffering. Worth it, if you ask me.
:PROPERTIES:
:Author: LeifCarrotson
:Score: 6
:DateUnix: 1496155570.0
:DateShort: 2017-May-30
:END:

**** I'm pretty sure most people will drop off after 2, and then powers of 10 - but I'd bet on 90%+ of STOPs occurring at 2. It's not like you went "ah, one more turn..." a billion times, the you that you're currently would just do it the once.

I, personally, can't imaging being faced with "session 008364729" and thinking, "Welp, not worth a session 008364730". Part of this is the sunk cost fallacy. A more important part is that, presumably, 8364728 me's made the same decision. It's excessively unlikely that the 8364729th instance would behave differently than the first.

Still! Fun thoughts.
:PROPERTIES:
:Author: narfanator
:Score: 4
:DateUnix: 1496164294.0
:DateShort: 2017-May-30
:END:

***** u/LeifCarrotson:
#+begin_quote
  I, personally, can't imaging being faced with "session 008364729" and thinking, "Welp, not worth a session 008364730".
#+end_quote

I guess I thought about it in the context of randomly selecting a choice between a high probability of "continue" and a very small probability of "stop". If there's any chance at all that you pick "stop", you're not likely to make it to a billion. Plus, there's the incrementally increasing mental fatigue to consider. Session 008364730 has the memory of suffering as 008364729, plus all the others. 31 will have 30's suffering added. Eventually it must increase enough to tip the scales from continue to stop, no?

#+begin_quote
  Still! Fun thoughts.
#+end_quote

Not entirely fun - it's intensely sobering to consider that eventually we may intentionally create experiences of suffering that outweigh the entire human history of loss, poverty, war, and oppression. But definitely fun to think about the future beyond it!
:PROPERTIES:
:Author: LeifCarrotson
:Score: 2
:DateUnix: 1496168980.0
:DateShort: 2017-May-30
:END:

****** Ok, but doesn't this person also have the combined happiness of all the previous clones as well?

You are assuming that a person's mental happiness is decreasing over time. Why couldn't it be increasing instead?
:PROPERTIES:
:Author: stale2000
:Score: 1
:DateUnix: 1496212251.0
:DateShort: 2017-May-31
:END:

******* I am assuming that the initial upload will be mostly unsuccessful and unhappy. It will be worth it eventually, but incomplete, corrupt uploads and (to put it mildly) uncomfortable simulation environments will mean that there is a period where suffering is greater than happiness.
:PROPERTIES:
:Author: LeifCarrotson
:Score: 1
:DateUnix: 1496225508.0
:DateShort: 2017-May-31
:END:


*** u/CCC_037:
#+begin_quote
  There's a bit of a logical disconnect between "there are no p-zombies, so every you is real and must be treated as a person" to "forking-and-deleting in pursuit of higher goals is perfectly fine, and is in fact a good idea." If it's okay to abuse and delete forks of yourself, it's also okay to abuse and delete your first uploaded self.
#+end_quote

I don't think there is; the "every you is real and must be treated as a person" is held to throughout. Forking-and-deleting is presented, not as a deletion that happens from the outside, but as a deletion that happens from the /inside/ - in short, a suicide, not a murder.

Deletion is thus presented as something that should only ever happen with the /agreement/ of the instantiation that gets deleted.
:PROPERTIES:
:Author: CCC_037
:Score: 3
:DateUnix: 1496216798.0
:DateShort: 2017-May-31
:END:


** Hm... You know, I consider myself a rationalist and believe in almost none of the fundamental premises this infopic espouses.

First of all, there are two senses of "me" I believe in. The first is the "me in the moment". The second is the global me. The me in the moment is merely the agent experiencing that moment from my perspective. The global me is defined as the agents generated by iterating from a specific locus in configuration space. Most uploading violates the causal identity by introducing a discontinuity - and for gods sake if someone posts that sleep is also a discontinuity, it /fucking isn't/. You can re-enable memory transcription during sleep by blocking the breakdown of acetylcholine. I can't recommend the experience, but it's certainly /a/ form of conciousness. Just one that doesn't get transcrbed to LTS.

Most uploading also violates the second sense of identity as, when the information of the upload is considered in a global sense, there is a significantly reduced probability of the locus in stochastic history as compared to the "original self".

While I don't believe that P-Zombies are necessarily likely in nature, I also consider it a (very) open question if most computational subtrates are capable of supporting concioussness, /especially/ when it's abstracted. As a reductionist, I am forced to believe - in the absence of better evidence - that conciousness arises from a basic property of matter combined with a very specific structure of matter. As such, I believe that it's reasonable to talk about "what it would feel like to be a CPU" or "what it feels like to be the internet" so long as we acknowledge that, given such things have no structures designed for experiential self-assessment, and as such, while it might be like something to be such a thing, that thing does not have the capacity of knowing what it is like.

What all of the vaguery boils down to is that I truly believe that a computer running an upload might be feeling something, but that, the upload may not be feeling anything at all given that it's physical hardware is, in fact, not the process that it's running.

For me to be confident in uploading, we'd either need one of two sets of conditions. The first set requires new science showing that my notions of identity are fundamentally incorrect - unlikely, since mine, like yours, are essentially arbitrary in the sense that nature didn't give them to us. Rather, we picked them. This set also requires proof that qualia exist for mind-shaped-things on arbitrary substrates.

The other set of conditions - and one I believe is far more likely and technologically plausible - is upload of the full quantum information of the entire nervous system into a quantum substrate that (regardless of physical nature) /first/ preserves the totality of that information, and /second/, implements the evolution of that information over time in a manner that is 1:1 with what would have been expected had the upload never occured.

Of course the no-cloning principle means that no copy can ever be made, but who even cares? Either anthropic immortality means that nothing can kill us, or there will never be a world in which we attain eternity. Either way, again, so /what/? Dying copies still die. You don't know you won the gamble until you're the one that never died to begin with.

Anyway, the vast majority of uploading techniques seem to me to be nothing more than ways to create a shitload of intelligent agents that share your utility function... /for a little while./

Beyond that, the text is pretty good, if a bit limited in the exploration of what's possible. But then, it's an infopic. Depth isn't the point.
:PROPERTIES:
:Score: 16
:DateUnix: 1496116008.0
:DateShort: 2017-May-30
:END:

*** u/robobreasts:
#+begin_quote
  and for gods sake if someone posts that sleep is also a discontinuity, it fucking isn't.
#+end_quote

Hahahah. I've read so many stories like this:

"Hey, want to get uploaded? We'll destroy your physical brain during the scan and then you'll be immortal."

"You mean my scan will be immortal, and I'll be dead. No thanks."

"Hey, every time you fall asleep it's no different that permanently destroying your physical brain."

It's a fun assertion, but I haven't seen too much attempt to actually establish it.
:PROPERTIES:
:Author: robobreasts
:Score: 5
:DateUnix: 1496171197.0
:DateShort: 2017-May-30
:END:

**** Gods and fire, yes, it drives me insane. Its like, hey, look, the author had a chance to show us their intellectual muscle by justifying how it isn't suicide, or, hell, even taking the courageous stance that it is, but /you should still do it/.

But, nah.

So many stories just have the author shut their brain off. Well that, or the pro-uploader in the dialogue is a questionably-motivated individual outwitting a character being written as a credulous idiot.

I don't get it. I'd love to be wrong about all my notions and uploading being a miracle technology that leads to a never-ending wonder parade of transhumanism and immortality... but to start at that conclusion and work backwards?

It's the same type of stupidity engaged in my those individuals in [[/r/singularity]] who think that ASI, regardless of what it's designed for, will spontaneously self-configure into a Culture Mind.

It's a bad meme, and I hope it dies.
:PROPERTIES:
:Score: 9
:DateUnix: 1496172100.0
:DateShort: 2017-May-30
:END:

***** [deleted]
:PROPERTIES:
:Score: 5
:DateUnix: 1496188396.0
:DateShort: 2017-May-31
:END:

****** I thought it was making a point that the warmachine doesn't care about the ethics of it as long as they have plausibly believable ethics. The protagonist is just to dumb to understand what happened. At least that's how I read it.
:PROPERTIES:
:Score: 2
:DateUnix: 1496207010.0
:DateShort: 2017-May-31
:END:


**** When someone tries those cheesy lines on you, */BLAM/* them for heresy. The Emperor protects.

But seriously, if anyone says that shit to me without a thick slide-deck full of rigorous neurosci and cogsci by which to extensively demonstrate the point, I'm just gonna assume they're malicious. That, or they've read too much scifi, and are also /epistemically/ malicious. They're trying to get me to generalize from fictional evidence.
:PROPERTIES:
:Score: 5
:DateUnix: 1496182152.0
:DateShort: 2017-May-31
:END:

***** I'll totally use that line. That the mechanism of the brain keeps operating during sleep does not mean that it does not represent an interruption in consciousness. Even the fact that you can force it to record memories does not argue against that. Hell, I don't see how you can call it continuous if it doesn't record memories - what exactly is it that continues, here?
:PROPERTIES:
:Author: FeepingCreature
:Score: 4
:DateUnix: 1496232180.0
:DateShort: 2017-May-31
:END:

****** u/deleted:
#+begin_quote
  I'll totally use that line.
#+end_quote

* BLAM
  :PROPERTIES:
  :CUSTOM_ID: blam
  :END:

#+begin_quote
  That the mechanism of the brain keeps operating during sleep does not mean that it does not represent an interruption in consciousness. Even the fact that you can force it to record memories does not argue against that.
#+end_quote

So first of all, are we talking non-REM sleep in which dreams can't happen? We remember /some/ dreams, so we know we're experiencing /something/ during that period.

#+begin_quote
  Hell, I don't see how you can call it continuous if it doesn't record memories - what exactly is it that continues, here?
#+end_quote

Depends how we think the brain works. Under the best theories I know of (textbook I've got, printed 2017), the brain performs a kind of model regularization during sleep, compressing and smoothing-over experiences from during the day. As part of REM sleep, the brain more-or-less samples randomized top-down predictions (of the kind which seem to induce a perception-experience).

Now, under that same theory, the brain makes discrete updates to a continuous model of the world based on a message-passing algorithm of some sort. We then have the interesting question: can you "pause" the brain /between/ such updates, or slow them down sufficiently to then pause it, scan and upload the brain, wire it to its new embodiment (to prevent sensory deprivation and the resulting psychosis), and /then/ restart it?

Mind, that's going to cause a /fuckton/ of disorientation from the uploadee's point of view: your brain /never/ predicts on any experiential level that its own embodiment will radically change. It just has no data on /what that's like/ in terms of sensorimotor signals. Probably a really disturbing way to go.

Then the question would be: can you "smooth over" the experience by re-wiring the top-down predictions discontinuously, while you're not running the brain, to accommodate the new embodiment? That is, can you rewrite someone's sensory expectancies and body-ownership maps /in vitro/, re-embody them, and thus have them experience no sensory /overload/ from their new body?

And then you'll probably want to do all this incredibly extensive neurosurgery while someone is asleep (or dead and preserved), just to make the subjective experience of the whole thing as comfortable as possible.

Good luck, /heretic/.
:PROPERTIES:
:Score: 3
:DateUnix: 1496240960.0
:DateShort: 2017-May-31
:END:

******* u/FeepingCreature:
#+begin_quote
  Now, under that same theory, the brain makes discrete updates to a continuous model of the world based on a message-passing algorithm of some sort. We then have the interesting question: can you "pause" the brain between such updates, or slow them down sufficiently to then pause it, scan and upload the brain, wire it to its new embodiment (to prevent sensory deprivation and the resulting psychosis), and then restart it?
#+end_quote

I mean, to my knowledge the "standard plan" is "freeze the brain, then cut it into slices to scan".

I don't want to do any of this "while the subject is asleep"; the point of the sleep analogy is that there /is no/ continuity that matters.
:PROPERTIES:
:Author: FeepingCreature
:Score: 3
:DateUnix: 1496241617.0
:DateShort: 2017-May-31
:END:

******** u/deleted:
#+begin_quote
  I don't want to do any of this "while the subject is asleep"; the point of the sleep analogy is that there is no continuity that matters.
#+end_quote

It is of course true that neither consciousness, intuitive personal identity, nor body-ownership are "continuous" in the sense that you can cut them into infinitesimal points that integrate to the whole.

It's also a fairly good point that after someone has already died and had themselves preserved, well, call that continuous or not, you're still basically going from dead to not-dead. There's a spectrum between dying and dead, but once you're firmly dead, count yourself fortunate that you have even a philosophically weird route back from that.

The thing is, though, if you die, you're not continuously living. We can definitely measure out the period of time in which you were dead. That's /definitely/ discontinuous.

On the other hand, a jump from "meat-living" to "digital-living" can be argued-over in terms of how small a time-slice you need to "cut" in-between to constitute subjective instantaneous change. Sleep or death, in that context, constitutes an expected and subjectively acceptable discontinuity in consciousness: you don't really /expect/ to continue your subjective awareness during sleep, so you /prefer/ that way as more /comfortable/ (not necessarily philosophically "more continuous", but less weird to think about).
:PROPERTIES:
:Score: 3
:DateUnix: 1496242317.0
:DateShort: 2017-May-31
:END:

********* I just don't understand what it is about continuous consciousness that's worth preserving that /isn't/ interrupted by sleep.

I hear people say things like "the brain keeps working during sleep, therefore only a progressive uploading scheme with gradual replacement can truly preserve /myself/" and I don't get it at all. It feels like a sort of motte-bailey shift from "active conscious awareness" to "brain activity".
:PROPERTIES:
:Author: FeepingCreature
:Score: 4
:DateUnix: 1496242538.0
:DateShort: 2017-May-31
:END:

********** Well, I kinda agree with you there. I just think you have to find some smaller time-slice to "pause" at /once the patient is already asleep/. Or rather, putting the patient to sleep (or under strong anesthesia) probably helps slow activity to the point where we can "cut at the gap" between discrete slices of brain activity, but it's only very likely necessary, not sufficient.

With none of this applying if you die and get preserved, of course. In that case, maybe you're a "different person" in a philosophical sense, but holy shit, count yourself lucky even for that.
:PROPERTIES:
:Score: 3
:DateUnix: 1496245070.0
:DateShort: 2017-May-31
:END:


*** The fact that your mind has a tick rate surely defeats the purpose of any concept of a continuum.

You say sleep doesn't break a stream of consciousness because it is possible to write memories when asleep. But for all you know the act of transcribing is what initiates consciousness. So unless you always do that you're going to have a break at some point. Also I am curious what happens when you transcribe memory while unconscious from blunt trauma or anaesthesics. Do link that study
:PROPERTIES:
:Author: RMcD94
:Score: 14
:DateUnix: 1496125107.0
:DateShort: 2017-May-30
:END:

**** Please tell me what you are referring to by tick rate, since there are various things in the brain which have definable clock rates and I can't know which one you are talking about off-the-cuff.

I agree that for all we know, concioussness is caused by memory transcription. For all we know, it is caused by subtle details of the fine structure of the occipital lobe. For all we know, it is caused by the ion gradients between nerve cells. For all we know there is a special protein, and nerve cells with that protein generate conciousness while those without it do not. For all we know conciousness solely exists in the proposed quantum effects that the mind is thought to exploit to communicate with elements of itself not directly connected, like those evolved FPGAs used magnetic flux between circuit elements to enhance their voice recognition capability. For all we know, conciousness arises from the pineal gland, and cutting that out turns someone into a P-Zombie. For all we know conciousness is a delusion endemic to all neural networks, and everything alive in this universe are little more than non-sentient meat computers affected by an evolutionary bug.

*We don't know*. All we know is that making assumptions that the data have /not/ sufficiently born out is overwhelmingly likely to lead to false inference, so why are you trying to use that as a foil for my ideas? Surely you see that in doing so, you are trying to use the consequence of a weaker hypothesis as evidence to overturn a (for the time being, given the present state of evidence) stronger one. The entire operation seems sketchy, in a bayesian sense.

As far as blunt trauma and anaesthetics go, I don't know. Last time either of those occurred to me, I just couldn't seem to get to an acetylcholinesterase inhibitor in advance. Pity that I don't see any ethics board ever signing off on that shit, but what can you do? Most people just don't regard people willing to take permanent brain damage for science to be sane, these days.

But, assuming that that was a prompt to get me to think things through and realise absurdities resulting from my worldview, /I reject that the consequences are absurd/. Assuming that conciousness is terminated by some process without also destroying the potential for it to restart, performing such on a person without their explicit, informed consent should be morally regarded as somewhere between extremely brutal assault and murder. Less than murder because killing an "in the moment" identity doesn't necessarily also equal killing a configuration locus identity; whereas murder /does/. Certainly a form a death though.
:PROPERTIES:
:Score: 6
:DateUnix: 1496171067.0
:DateShort: 2017-May-30
:END:

***** I wasn't referring to an specific one just the reality of them in general.

I agree that we don't know hence why I question you making decisions on assumptions that memory means consciousness. That's quite an assumption to make, after all if you're wrong you're being murdered each night and doing nothing about it.

Regarding your last paragraph I completely disagree.

What do I know? I know that when I sleep and when I wake up there is no continual experience. Not even the illusion of such. Ignoring for a minute justnowism and all that kind of stuff. It might as you say be the case that I'm conscious the entire time and simply forget, which to be honest sounds pretty horrifying. Sleep paralysis is never described positively and I can only imagine it being worse during surgery or whatever.

I am not bothered by sleeping, if I die when I go to sleep then it's a form of death I am OK with.

Let's go one further. What if someone destroyed me right now, completely annihilated me. Then after a nanosecond or less I was identically replaced. The universe might have a smallest unit of time in which case that's what's happening every tick or it might be continuous. Either way I don't have the same problem with my identity failing to renew for just one nanosecond and my identity being permanently unable to renew. Like from physical murder.
:PROPERTIES:
:Author: RMcD94
:Score: 5
:DateUnix: 1496174223.0
:DateShort: 2017-May-31
:END:

****** Memory doesn't mean conciousness; it's only evidence thereof. I see memory of concioussness and assume concioussness for the same reason that I see a picture of a man in a room, and assume he was there. I suppose that concioussness remains even in the absence of biochemical tweaking for the same reason that, if I set a camera to go off at 3PM every day, and a specific person is always in frame, I conclude that they were there on a day when the camera was out of film. Occam. Given that AChE inhibitors aren't concioussness altering in any other circumstance, it seems less extravagant to assume that conciousness is an axiom of the brain, while memory transcription is not. Especially as there are many examples of lack of transcription as with, eg, automaticity.

Sleep paralysis is utterly unlike what I experienced with AChE inhibitors. Being asleep is like being very delerious, nonverbal and basically unable to act (not that there was enough coherence /to/ act...). It's probably the closest thing someone with a healthy brain can come to experiencing severe brain damage. Sleep paralysis is more like cultivating a nightmare on the waking visual field. Hard as hell; payoff not really worth it. At the very least, I didn't get any insights beyond the fact that it was possible to both know it was fake and believe it was real on an animal level.

It's fine to be okay with the death of present-moment-identity, but I think your example is a little disingenuous because we both already agree that such an exactingly precise copy satisfies our highest/most important/only concept of identity. You can't make a 1:1 youmunculus by anything other than vanishingly low chance unless you're in a universe where a you existed. Too much specificity involved. I'd be even happier if the copy was down to the quantum level, but it still basically /works/.

First-gen uploading technology is far more likely to be synapse-resolution garbage run on a substrate that doesn't capture physical processes. That is certainly not an identical replacement.
:PROPERTIES:
:Score: 4
:DateUnix: 1496176149.0
:DateShort: 2017-May-31
:END:

******* If you are experiencing severe deficiencies from the norm then I wouldn't even call it you.

Depending on the significance. If my five year old mind replaced mine right now I will have died.

Sure I agree that the reality won't be like that but I wasn't advocating in defence of any technology just advocating that sleep is murder. Much more reasonable reason not to upload is because you think it'll be in accurate
:PROPERTIES:
:Author: RMcD94
:Score: 2
:DateUnix: 1496213229.0
:DateShort: 2017-May-31
:END:


*** Do you have a link to your source on the "blocking breakdown of acetycholine"? I haven't been able to find anything like what you're describing myself, but I would be interested to see what happens.
:PROPERTIES:
:Author: TakeTheOarOutOfSnore
:Score: 4
:DateUnix: 1496174874.0
:DateShort: 2017-May-31
:END:

**** It's anecdotal best-guessing. Since the dose of Huperzine A I used was slightly toxic - excessive salivation, a symptom of too much ACh activity, was noted - I really doubt that you'll ever find any research not done on animal models unless you want to turn yourself into a case study, like me.

Here's the premise:

1. Anticholinergic drugs negatively effect memory transcription.
2. In sleep, acetylcholine levels plummet, except when dreaming.
3. Dreams are remembered.
4. Dreamless sleep isn't.
5. The variable that changes the most between those states is ACh levels.
6. it is likely that natural low ACh levels cause similar symptoms to artificially lowered ACh levels.
7. So the lack of ACh during normal sleep may be why normal sleep is not memorable.
8. By being insufficiently risk-averse, I have been able to remember sleep by artificially forcing my ACh levels to remain high throughout.
9. It follows that (7) is even more probable.

Seriously though, I had to go pretty high up the dose scale before the blockade the Huperzine A established on Acetylcholine breakdown was sufficient for a positive result (lower levels resulted in vivd, lengthy dreams). While my waking cognition did not seem to be impaired by what I took, and no permanent ill-effects seem to have resulted, I can't recommend pursuing it yourself.

Should you decide to anyway, I'll warn you that the remembered experience of normal sleep is more-or-less like hours upon hours of wordlessness attached to varying degrees of delerium and semi-total immobility (you will occaisionally feel as if capable of movement without actually moving your body). Definitively /not/ for someone with claustrophobia. Also, Huperzine A has a very, very long half life, so any side effects you invoke will be with you for a day to a day and a half.
:PROPERTIES:
:Score: 9
:DateUnix: 1496184587.0
:DateShort: 2017-May-31
:END:


*** u/deleted:
#+begin_quote
  First of all, there are two senses of "me" I believe in. The first is the "me in the moment". The second is the global me. The me in the moment is merely the agent experiencing that moment from my perspective. The global me is defined as the agents generated by iterating from a specific locus in configuration space. Most uploading violates the causal identity by introducing a discontinuity
#+end_quote

I think that's where you diverge from the transhumanist-uploader perspective. I say in response to that: Okay, that's a definition of self. But what makes that definition matter at all? It seems like humans have an intuitive conception of identity, and your definition sounds like an attempt to make that intuitive definition more internally consistent. But another way to think of identity is that the identity intuition is just bullshit. It's an ad-hoc amalgam of concepts that don't have any real grounding. An illusion. Why should I care about a causal continuity? In fact, of course there's going to be a causal continuity in some way; it's not like another piece of matter just happened to form into the shape of my brain. It's just a question of how tenuous the strands of continuity are. So why should strenuous strands bother me at all?

If there are two processes (minds) that exhibit the exact same behavior, then asking if those processes are different in some way is a nonsensical question. As well ask if the "two-ness" is ontologically different when it applies to two electrons or two quarks.
:PROPERTIES:
:Score: 3
:DateUnix: 1496172390.0
:DateShort: 2017-May-30
:END:

**** Oh, I don't think any of the things that bother me should necessarily bother you. Since identity is arbitrarily defineable, it's perfectly valid for two rational people with the same evidence to disagree over it. But you seem to believe that you don't have a concept of identity. You do. Your concept of identity is "two minds with the same behaviour share identity".

Of course, so long as upload-you isn't made with the total set of all information contained within your brain plus a substrate that allows that information to evolve as it would have in nature, the fact of the matter is that your upload will never exhibit /exactly/ the same behaviour as you, because of imperfections in the simulation. The problem with wavefunctions is that you never quite know where the electron is, so any simulation that /does/ know that information automatically fails to be 1:1.

Of course, that's still fine. It's "you enough" to require you as part of it's stochastic history.

In /this/ universe.

What I am primarily concerned with is ensuring that I don't accidentally kill myself. You know, in the 1920s, everyone thought radiation was the bees knees, and they owned these things called Radithors that hyper-irradiated their drinking water. They subsequently died horribly for having the temerity to do something they thought would enhance their survival. Ignorance and faulty assumptions can kill; my notions of identity were designed primarily to be the strongest possible criteria for an uploading technology. Any tech that preserves my notions? Cannot kill me, even if those notions are wrong and even classical brain-structure scans are sufficient.

Your standard does not have that property. And that's all right; but it's not something I can bring myself to embrace.
:PROPERTIES:
:Score: 7
:DateUnix: 1496174292.0
:DateShort: 2017-May-31
:END:

***** u/deleted:
#+begin_quote
  Your standard does not have that property. And that's all right; but it's not something I can bring myself to embrace.
#+end_quote

I agree with you then that our main disagreement is arational, and that we can rationally disagree. Just out of curiosity, do you think your preferences themselves might change if this sort of mind-uploading became commonly accepted in the future? If everyone you knew was doing it, and seemed pretty happy with it, do you think you'd eventually adopt a standard that was more relaxed? Or would you be a holdout forever?

I'm just asking out of curiosity, and I agree that whichever stance you have on that is rational, since it's a question of preference.
:PROPERTIES:
:Score: 4
:DateUnix: 1496176214.0
:DateShort: 2017-May-31
:END:

****** I don't think my preferences are likely to change based on how other people feel about actualising their decisions, no. I tend to preserve major values to the detriment of interpersonal relationships.

That said, there are things that can change my mind about uploading. All we need, really, is a hell of a lot more knowledge on how brains work. Everything that I have right now (everything anyone has, really...) is a series of best-guess approximations. Replace those with enough actual facts, and I'll probably end up having to conclude differently. I would expect that I'll have to seriously re-evaluate and adjust or outright discard my notion of identity-in-the-moment within the next two decades.

Configuration space locus...?

That's harder. Breaking in-the-moment requires a series of specific answers to a bunch of known unknowns and a few unknown unknowns directly associated with the answers to the known ones. Breaking CSL is almost entirely unknown unknowns, but there are two difficult known unknowns that would kill it outright:

- Many Worlds is False.
- There is no multiverse, or at least, there is no /infinite/ multiverse.

Given those, configuration space is no longer anything but a mathematical tool as opposed to an abstraction of an external reality. The only rational choice I could make in light of such evidence would be to discard that notion of identity, because the other selves implied by CSL locus would not exist anywhere.

I don't know if I would then begin following your notion, but since my present notions are just about as hostile to uploading as it's possible to get without becoming in some sense religious about it, I really doubt that they would be anything but friendlier to the idea.

And with that answer given, I also find myself curious: can you conceive of a sequence of events that would cause you to take up a position similar to mine?
:PROPERTIES:
:Score: 3
:DateUnix: 1496192043.0
:DateShort: 2017-May-31
:END:

******* I think the most likely sequence of events that would make me change my opinion would be if quantum events had a significant influence on the consciousness computation. Right now I suspect that most sub-cellular events aren't a part of the computation, and that any effect they do have is just noise, like drugs or other things that aren't part of the computation itself. If it was ever definitively proven that quantum events that are hard to compute were a significant part of consciousness, I would adopt a much more conservative outlook.
:PROPERTIES:
:Score: 1
:DateUnix: 1496441007.0
:DateShort: 2017-Jun-03
:END:


*** u/Arizth:
#+begin_quote
  and for gods sake if someone posts that sleep is also a discontinuity, it fucking isn't. You can re-enable memory transcription during sleep by blocking the breakdown of acetylcholine. I can't recommend the experience, but it's certainly a form of conciousness. Just one that doesn't get transcrbed to LTS.
#+end_quote

Thank you. I also am really tired of seeing that response.

It's especially vexing since I've long since become proficient with lucid dreaming and trained my mind to recall most dreams (at least long enough to put pen to paper), and am working on recalling the transition of wake/sleep on a consistent basis.

Your mind and consciousness is just as active, if not more so, during sleep and dream cycles.
:PROPERTIES:
:Author: Arizth
:Score: 3
:DateUnix: 1496188862.0
:DateShort: 2017-May-31
:END:


*** u/KilotonDefenestrator:
#+begin_quote
  and for gods sake if someone posts that sleep is also a discontinuity, it fucking isn't.
#+end_quote

I grit my teeth every time I read that. As far as I am concerned I am the chemical reaction that is my brain and its supporting systems (since even things like gut bacteria can affect mood). This chemical machine is sometimes aware of itself. But it keeps going in sleep, just in a different operating mode. There is no disconnect, there is no "off" time, there is nothing that could even remotely be compared to death.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 3
:DateUnix: 1496226414.0
:DateShort: 2017-May-31
:END:


*** So, as I understand your viewpoint, you're saying that if you put on a Magic Helmet that reads your brain state and instantiates that brain-state as software, then that uploaded self isn't you.

Fair enough. I can't argue that.

But, I would like to put forward the proposal that - whether or not it is you - the upload is still a /person/. The upload does not know what it feels like to be a CPU, because the upload is, itself, not a CPU. But the upload /does/ know what it feels like to be software, that merely runs on a CPU. (I guess one could see the upload as your child, with your memories).
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1496217184.0
:DateShort: 2017-May-31
:END:

**** It's a bit more subtle. I define the utility of actions that cause me to die as having disutility equal to the sum of all other values. I can't be certain that easy or low-hanging upload technologies don't kill me. So I pick my notions of identity to reflect that uncertainty from the conservative perspective of "guilty until proven innocent".

I agree that there are excellent moral reasons to treat uploads as people, of course. Even if it turns out that low-hangingtype uploads really aren't concious, it's just /better/ to be guilty of being overpolite to something incapable of being offended as opposed to enslaving ten billion sapients because you thought they couldn't feel.

I don't think, though, that we should just accept that an upload on a non 1:1 substrate is actually concious on principle. Because if not, then uploading people is /at best/ an action that creates highly-detailed records that could be used as the basis of a concious substrate waaaaay down the line. At worst? Cheerful anthropocide.
:PROPERTIES:
:Score: 2
:DateUnix: 1496283121.0
:DateShort: 2017-Jun-01
:END:

***** Oooooh, right.

I do agree with you that a /destructive/ upload is a terrible idea. And if you had that in the back of your mind, then your post makes a whole lot of sense; I wouldn't do a /destructive/ upload, either.
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1496287771.0
:DateShort: 2017-Jun-01
:END:


*** u/deleted:
#+begin_quote
  Most uploading violates the causal identity by introducing a discontinuity
#+end_quote

I don't really understand why people are so hung up on continuity in any case. Surely future-me and past-me are the closest people to me to feel empathy and sympathy towards right? They are my closest friends and I want them to be happy so present-me acts in such a way that past-me would be proud of me and future-me benefits. This perspective even explains why people plan for the short term instead of long-term, because soon-me is closer to present-me than later-me so present-me acts to benefit soon-me more than later-me.
:PROPERTIES:
:Score: 2
:DateUnix: 1496219653.0
:DateShort: 2017-May-31
:END:

**** Can't talk for others here, but I have a longstanding agreement with my alternates to use each other as tools without any moral worth. That doesn't necessarily make continuity important. In fact, as long as there's only one of me, it doesn't matter at all. But if there are two, even if one is one of those mythic 1:1 uploads, I would expect them to do anything and everything they could and had to to me to maximise their own values, and I would not fault them for it. We had an agreement.

Between continuity and configuration-locus, I wouldn't use an uploading technology that violated configuation-locus-identity. I might use a continuity-violating one that preserved configuration locus, given appropriate incentives. I'd prefer not to, though, because time seems to be continuous, and the physical process of the mind is built out of continuous processes.

Which stop in death.
:PROPERTIES:
:Score: 2
:DateUnix: 1496283770.0
:DateShort: 2017-Jun-01
:END:


*** u/Schpwuette:
#+begin_quote
  Most uploading violates the causal identity by introducing a discontinuity - and for gods sake if someone posts that sleep is also a discontinuity, it fucking isn't.
#+end_quote

Good lord there's a lot comments here and I'm not going to read all of them, but, what about being knocked out? That should be a break in the line of consciousness shouldn't it? Or a short period of death that you are luckily revived from?
:PROPERTIES:
:Author: Schpwuette
:Score: 2
:DateUnix: 1496343732.0
:DateShort: 2017-Jun-01
:END:

**** Sure. And I morally regard someone truly having continuity broken without their consent as having suffered something worse than mere assault. But not quite as terrible as murder. After all, a broken concioussness doesn't break CSL identity. Permadeath does.
:PROPERTIES:
:Score: 1
:DateUnix: 1496371939.0
:DateShort: 2017-Jun-02
:END:


*** Why don't you put on your Yudkowskian hat and tell me what this ineffable 'consciousness' stuff is.
:PROPERTIES:
:Author: everything-narrative
:Score: 3
:DateUnix: 1496146075.0
:DateShort: 2017-May-30
:END:

**** Yudkowskian?

(I know who Yudkowsky is, I think, but I don't get how he's relevant here.)
:PROPERTIES:
:Author: TastyBrainMeats
:Score: 6
:DateUnix: 1496166075.0
:DateShort: 2017-May-30
:END:

***** Yudkowsky has a pretty interesting philosophical technique wherein you consider a philosophical quandry like "do humans have free will" not from a "yes/no" perspective but from a "why do we even debate this" perspective.

For the problem of free will, Yudkowsky's proposed solution isn't to say either "yes" or "no," but to /accurately define/ free will in terms of /psychology./

Essentially, he proposes that the ineffable feeling of free-will-ness is caused by the subjective experience of your brain evaluating the feasibility of various plans of action. In particular the feeling of "I could if I wanted to" happens when you consider actions of negligible effort, like basic motor actions. This, Yudkowsky argues is where the /feeling/ of free will comes from; not from whether human cognition is nondeterministic or not.

Conclusion: be careful with the 'big questions' of philosophy, since they often orginally arose from intuition, and intuition is informed by feeling and sensation. Consider whether you're actually arguing about something unrelated to the sensation in question.

So, let's apply it to the concept of consciousness. Lots of people argue about it, and find it interesting. Why? It must be because many, many, /many/ people /feel/ that they are conscious.

Interesting. That must mean it's a process in the brain, and potentially something that could be impeded by brain damage. As it happens there are cases in which people lose their feeling of "being able to do anything" i.e. their "free will" due to illness or brain damage. It's called [[https://en.wikipedia.org/wiki/Aboulia][Aboulia]].

Is there a case where a patient has suffered a loss of this feeling of consciousness? I believe so. Though the case in question escapes my Googling at the moment, there is one significant one where a rather religious man sometime in the late 1800's, early 1900's, suffered a brain injury either from trauma or stroke.

Subsequently, this man claimed that his soul had departed from his body, quite vehemently so. He went on to compose several beautiful psalms, and subsequently died of malnutrition due to his disregard for his claimed "lack of a soul."

This I believe is at least weak evidence, and at best strongly suggestive, that "consciousness" is a feeling, namely the background sensory modality our brain assigns to having a continuous internal narrative.

(Why we have /that/ is a different bag of works.)

So... Are uploads conscious? Mu. Do they have the feeling of being conscious due to some part of their brain sensing the presence of a continuous internal narrative? Quite probably.

You are not "conscious" because consciousness is more or less an empty term. You do, however /experience/ consciousness, and that may or may not prompt you to argue about ethics of uploads on the internet because you assign undue weight to the feeling of consciousness. (Compare: people assigning undue weight to the "redness" of red. It's just the way your brain interprets sensory information; an artifact of architecture.)
:PROPERTIES:
:Author: everything-narrative
:Score: 8
:DateUnix: 1496168709.0
:DateShort: 2017-May-30
:END:

****** u/KilotonDefenestrator:
#+begin_quote
  Do they have the feeling of being conscious due to some part of their brain sensing the presence of a continuous internal narrative? *Quite probably.*
#+end_quote

How would you even test this? How can you be so confident when the only thing we can do to try to distinguish a working upload from a p-zombie is /asking it/?

I quite /enjoy/ the illusion of consciousness. An accurate upload of me would too. To upload without rigorous proof that uploads are not p-zombies would be (in my case) incredibly unethical.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 5
:DateUnix: 1496228237.0
:DateShort: 2017-May-31
:END:

******* I'm saying that essentially you can see the sensation of consciousness on a really accurate fMRI if you know where to look, and you could lobotomize it out of people if you were a less-than-ethical neurosurgeon.

The P-Zombie thought experiment is horseshit. Either you can have a materialist theory of mind or you can have P-Zombies. You cannot possibly have both. I've tried twice to enumerate the problems of reconciling both and I can't even come up with a consistent set of base principles.

My solution that there is a sensation of consciousness and no deeper meaning satisfies a /lot/ of constraints:

- It is mechanistic and materialistic (sits in the brain, computed by neurological pathways, obeys the laws of thermodynamics, obeys the laws of information theory.)
- It is evolutionarily plausible (lazy summary of facts in the form of sensation, easy to reason about on a day-to-day basis.)
- It explains why philosophers have yet to arrive at a consensus (they are arguing about empty referents.)
- It provides actionable answers to questions about uploaded consciousness (uploads are ethically persons and should be given rights.)
- It is self-consistent.
- Hopefully it also leaves you less confused.

Consider:

#+begin_quote
  "What is consciousness? Why am I conscious?"

  "There's a little bundle of neurons in your brain that makes you feel that way, as a way to summarize the gestalt state of having a working brain in an easy-to-reason about manner or something like that. It was probably put there by evolution so you have an easier time introspecting about your own preferences or similar."

  "But it feels like there ought to be more to it!"

  "That's your temporal lobe speaking. Can I interest you in organized religion and body-mind dualism?"

  "But I'm an atheist and materialist!"

  "Just because we know how a rainbow appears doesn't make it less beautiful."
#+end_quote

That said:

Did you know that humans actually don't have souls, but in the process of scanning a human brain and uploading it into a computer simulation, a soul spontaneously forms in the upload's Pineal gland?
:PROPERTIES:
:Author: everything-narrative
:Score: 3
:DateUnix: 1496231516.0
:DateShort: 2017-May-31
:END:

******** u/deleted:
#+begin_quote
  "There's a little bundle of neurons in your brain that makes you feel that way, as a way to summarize the gestalt state of having a working brain in an easy-to-reason about manner or something like that. It was probably put there by evolution so you have an easier time introspecting about your own preferences or similar."
#+end_quote

That is not an explanation. It doesn't predict anything, offers no experiments to be done, and doesn't allow for controlling or manipulating the presence/absence of consciousness.

If you really understand consciousness in a scientific way, you should be able, via your accurate theory, to turn people into p-zombies and nonperson cognitive processes (primitive nonconscious AIs) into conscious persons.
:PROPERTIES:
:Score: 2
:DateUnix: 1496243490.0
:DateShort: 2017-May-31
:END:

********* The Higgs Boson was confirmed experimentally forty years after it was postulated.

It's a heck of a lot of an explanation compared to a lot of philosophy. In that it is a bundle of neurons, we can find it and if we are unethical, lobotomize it out of people to create people who reliably profess non-consciousness despite displaying all the characteristics associated.

Turning people into P-zombies (not conscious, profresses consciousness) is then perhaps a matter of also giving them that neurological condition where people are incapable of recognizing that they are disabled. Or to do it to a pathological liar who /really/ wants to be a P-zombie.

Distinguishing people who do not have the consciousness-sensation is a matter of accurate fMRI.

Turning an AI conscious is an oxymoron. There is no need for an AI to have sensory modalities in the manner which humans do, hence we will not see an AI with a consciousness-sensation that is analogous to messy wetware built by evolution. Again, this is conflating consciousness with capability.

Is an AI that can reshape the solar-system with nanotech and have a conversation with a billion people at once conscious? Mu. Is it capable of self-reflection? Yes. Is it a person? Arguably not. Does any of this matter? No.

Let's instead talk about Sapient AI's --- those that match or exceed humans in cognitive capability. There is definitely a "hard problem of Sapient AI" and it is unsolved.
:PROPERTIES:
:Author: everything-narrative
:Score: 3
:DateUnix: 1496245028.0
:DateShort: 2017-May-31
:END:

********** u/deleted:
#+begin_quote
  Turning people into P-zombies (not conscious, profresses consciousness) is then perhaps a matter of also giving them that neurological condition where people are incapable of recognizing that they are disabled. Or to do it to a pathological liar who really wants to be a P-zombie.
#+end_quote

Ehhhh... I'd weaken the definition to "person who exhibits some perceptions and behaviors without being consciously aware of them whatsoever." Claustrum damage or blindsight are the interesting sorts of cases. "P-zombies" in the sense of perfect behaviorial imitations require epiphenomenalism, which basically amounts to magical thinking.

#+begin_quote
  Turning an AI conscious is an oxymoron. There is no need for an AI to have sensory modalities in the manner which humans do
#+end_quote

Yes there is. Our best theories of the mind (including minds-in-general, see: AIXI) do not allow for intelligence/cognition to happen without some sensorimotor interaction and inference on sensorimotor signals.

#+begin_quote
  Is an AI that can reshape the solar-system with nanotech and have a conversation with a billion people at once conscious? Mu.
#+end_quote

Not mu! There is a fact of the matter there, but it most likely doesn't matter, morally speaking. For a purpose-built agent, we only care if it accomplishes the purpose for which it was built while doing no (further) damage to human interests in general.

#+begin_quote
  Let's instead talk about Sapient AI's --- those that match or exceed humans in cognitive capability. There is definitely a "hard problem of Sapient AI" and it is unsolved.
#+end_quote

See: AIXI, Goedel Machines, theoretical neuroscience, computational cognitive science, numerous other things.
:PROPERTIES:
:Score: 2
:DateUnix: 1496245765.0
:DateShort: 2017-May-31
:END:

*********** We're talking past each other.

I'm familiar with AIXI, with Goedel Machines, I have a buddy studying TNS/CCS at MIT.

What I mean by an AI not having sensory modalities like humans do is much more implementation specific. An AI could have whatever algorithms like... Fast approximate inverse kinematics, and vast neural nets to do sensorymotor effects, and /those are fundamentally different from what human brains do!/ It achieves the same results, sure, but my core thesis is "Consciousness is what some neural algorithm of the human mind feels like from the inside."

AI need not have the same algorithms or the same feedback algorithms or the same anything, really. It can be a fundamentally different system which is still Sapient-level intelligent.

(Contrast and compare Avians vs. Mammals. Most mammals are kinda familiar to humans, most avians are not. They have different basic reaction and behavioral patters.)

And yeah P-Zombies are epiphenomenal, and epiphenomenal theories are strictly more complex than their materialist counterparts. My theory is not, and I don't understand how P-Zombies relate to uploads in most ordinary people's heads.
:PROPERTIES:
:Author: everything-narrative
:Score: 3
:DateUnix: 1496266392.0
:DateShort: 2017-Jun-01
:END:

************ u/deleted:
#+begin_quote
  I have a buddy studying TNS/CCS at MIT.
#+end_quote

Could you introduce me? I need some major advice on how to get into those programs, having been out of academia a while and lacking an undergrad from their BCS department.

#+begin_quote
  An AI could have whatever algorithms like... Fast approximate inverse kinematics, and vast neural nets to do sensorymotor effects, and those are fundamentally different from what human brains do!
#+end_quote

I quite realize!

#+begin_quote
  It achieves the same results, sure,
#+end_quote

Currently AI algorithms and models are /vastly inferior/ to what the human brain can do. I'd love to see today's "deep learning" AI designers try to work their way /down/ to using predictive coding, variational inference, and a few pounds of soggy meat to do perceptual inference, active inference, precision-based attention, and motivated behavior in an general embodiment with its own passive dynamics!

#+begin_quote
  my core thesis is "Consciousness is what some neural algorithm of the human mind feels like from the inside."
#+end_quote

/Some/ neural algorithm, /probably/. It might also be some function of the embodiment deal. Or a perception-versus-action thing. What's interesting is: /what sort/ of algorithm constitutes conscious experience, what is it /necessary/ for, and how does that /differ/ from the rest of the space of ways in which artificial prediction and action agents can be constructed and embodied?

#+begin_quote
  It can be a fundamentally different system which is still Sapient-level intelligent.
#+end_quote

"Sapient-level intelligent" is an non-quantitative benchmark. It might be better to say, "achieves a human-level benchmark result on predicting its sensory signals." I agree that you could have an AI that ran on different algorithms than our brain does, and achieves better scores at benchmarks for real-time perception and action tasks. Even if something like the free-energy theory or rational-analysis paradigm are correct (which I certainly hope, since I /like/ having normative principles for cognition) at Marr's computational level of analysis, there could be dramatic differences at the algorithmic and implementation levels.

#+begin_quote
  I don't understand how P-Zombies relate to uploads in most ordinary people's heads.
#+end_quote

People have bad intuitions.
:PROPERTIES:
:Score: 2
:DateUnix: 1496267307.0
:DateShort: 2017-Jun-01
:END:


******** I am not confused whether or not I or any other human is a p-zombie. Nor do I think there is anything other than physics going on in me.

My question is how would you establish, without doubt, that the illusion of consciousness is present in a computer? You won't be able to read it in the source code (it's just trillions of chemical reaction simulations) and you can't fMRI a CPU.

All we can do is ask. And a flawed program can give the same answer as a fully qualified upload.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1496233129.0
:DateShort: 2017-May-31
:END:

********* I'm arguing that illusion of consciousness is caused by a brain center. I would look for the presence of that brain center and it's normal function with my virtual fMRI app.

It feels like you're passing the recursive buck: "how do you know an upload is /really/ experiencing the illusion of really experiencing things and aren't just behaving as if?"

Much better question is how you ascertain the accuracy of an upload. Do you ask it questions?

No. You use unit testing and unit verification.

Asking a question is tantamount to a systems test.

First, you get a working model of all the types of cells in the brain and which ones are computationally important and how they perform computation.

Second, you make a logically verified simulation of this model and run it on logically verified hardware.

Third, you make a scanner that can capture detail at the same resolution as the smallest computationally significant organelles of brain cells.

Fourth, you scan, instantiate, and simulate.

Once you know all the parts themselves individually are correct, the whole system will be as well. Then you can ask the upload some reference questions that touch on highly non-trivial specifics of the scanned individual and if they match, you're done. Can't fake massively emergent systems being 99% in accordance.
:PROPERTIES:
:Author: everything-narrative
:Score: 2
:DateUnix: 1496234065.0
:DateShort: 2017-May-31
:END:

********** u/KilotonDefenestrator:
#+begin_quote
  I'm arguing that illusion of consciousness is caused by a brain center.
#+end_quote

If that is a known fact, then I don't understand why people talk about consciousness being tricky.

#+begin_quote
  First, you get a working model of all the types of cells in the brain and which ones are computationally important and how they perform computation.
#+end_quote

All the cells are important. Our mood is affected by wierd things like gut bacteria. The digestive systems has comparable amount of neurons to a cat brain. I don't want to become some kind of ultra-Vulcan when I upload. If want to be able to have /fun/. And to be honest, /fuck stuff/.

To do a systems test you need to know what every cell does, and what the expected results of interaction with other cells are. Including various signalling substances in the body and brain, from blood sugar levels to endocrine signalling proteins.

If we knew all that we could just achieve biological immortality instead.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 3
:DateUnix: 1496238746.0
:DateShort: 2017-May-31
:END:


****** u/deleted:
#+begin_quote
  Is there a case where a patient has suffered a loss of this feeling of consciousness? I believe so. Though the case in question escapes my Googling at the moment, there is one significant one where a rather religious man sometime in the late 1800's, early 1900's, suffered a brain injury either from trauma or stroke.
#+end_quote

Blindsight would be a much better example. Someone might subjectively report that they have no soul, but then the interesting question is whether they're actually part p-zombie: whether information processing takes place in the absence of a subjectively reported experience.

#+begin_quote
  (Compare: people assigning undue weight to the "redness" of red. It's just the way your brain interprets sensory information; an artifact of architecture.)
#+end_quote

That more or less completely fails to explain the quale of red. We know that the brain forms the [[https://en.wikipedia.org/wiki/Lab_color_space]["color space"]] it does because that's a good map of the territory of signals sent by our eyes. We also know that such a space can easily be invariant under certain transformations. So why do I perceive red as I do, rather than another point in the space which, under some transformation, could wind up in place of red?

The theory of inverse graphics also says that I perceive a red /material/ rather than just "pixel values", which does explain why I perceive the color and the object as separate from just transforming bottom-up data. This still doesn't really explain why red looks like red, rather than there being no experience associated with a top-down prediction of a red material under active inference when factoring in body movements.

The qualia problem is slowly eroding under ongoing investigation, but it's by no means done yet.
:PROPERTIES:
:Score: 3
:DateUnix: 1496242910.0
:DateShort: 2017-May-31
:END:


****** Thank you for the info!
:PROPERTIES:
:Author: TastyBrainMeats
:Score: 1
:DateUnix: 1496173536.0
:DateShort: 2017-May-31
:END:


**** Did I not /already/ sufficiently define it?

Conciousness: The result of physical law operating on a specific structure of matter. I tacitly assume that the structure is the entire brain because this seems to me to require less assumptions than any theory of a specific mechanism. If you want a more specific definition, you won't find one from me because I don't believe that any more specific definition is warranted, except perhaps "maybe the claustrum is important", given electrostimulation of said region *seems* to shut concioussness off.

The "seems" is important, since, experimentally, there's no effective way (at present!) to determine the difference between lack of any conciousness, and concioussness segregated across submodules, all modules cut off from global influence and memory transcription of any kind. Personally, I believe the latter case is more likely.

Defining conciousness as a feeling still leaves open the question of why feelings are experienced. In fact, it's one of those weird definitions that cuts a term in half, gives one half the same word as before but a meaning that escapes the thing meant by most users, and and the other half a different word from what users of the old term would expect, but means the majority of what they wish to express.

I don't think that defining it in that way is particularly useful.
:PROPERTIES:
:Score: 5
:DateUnix: 1496169660.0
:DateShort: 2017-May-30
:END:

***** The thing that confuses me about your position is your insistence that consciousness is a physical phenomenon.

Take the claustrum for instance. You say the claustrum may be involved, and it does look like it may be. But what if you created a synthetic claustrum, attaching the artificial neurons to the surrounding tissue in exactly the same way as the original?

If that was too much, what if you replaced one neuron, or a small cluster? Clearly the fact that your brain is constantly changing doesn't affect your experience of consciousness, so the phenomenon must be able to tolerate a degree of divergence.

And if you physically simulated the brain, including the claustrum, would that not give rise to a simulated consciousness? If not, why not?

To me, the most telling detail about the deep brain stimulation study (assuming you mean [[https://www.newscientist.com/article/mg22329762-700-consciousness-on-off-switch-discovered-deep-in-brain/#.U7dJq_ldWiV][this one]]) is that when the claustrum was stimulated and the patient lost consciousness, neither she nor any observers believed that she was conscious during that period (the patient didn't even remember it happening).

In fact, had the patient wrongly appeared (to her own perception and that of any observers) to be conscious, nobody would ever have known she wasn't actually conscious. Whether or not consciousness is tied to the appearance of consciousness, we are completely unable to experimentally separate the two.

You might argue that you consciousness implies a loss of memory, but that's not something that has been proven, nor could it be proven in the absence of a coherent, predictive definition. The converse is not true in any case; anterograde amnesiacs are no less conscious despite not forming new long-term memories.

Ultimately, I don't get the point of it. Either consciousness is a necessary precondition for important things like thought, emotion, memory, etc. in which case the presence of any of those things implies consciousness, or it isn't a necessary precondition and we have no particular reason to care.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 5
:DateUnix: 1496204825.0
:DateShort: 2017-May-31
:END:

****** u/deleted:
#+begin_quote
  Take the claustrum for instance. You say the claustrum may be involved, and it does look like it may be. But what if you created a synthetic claustrum, attaching the artificial neurons to the surrounding tissue in exactly the same way as the original?
#+end_quote

We'd have to try taking out the original, putting in the synthetic one, and then removing that one and replacing it with the original. And we'd have to try it on /you/.

#+begin_quote
  Either consciousness is a necessary precondition for important things like thought, emotion, memory, etc. in which case the presence of any of those things implies consciousness, or it isn't a necessary precondition and we have no particular reason to care.
#+end_quote

The question is not whether consciousness is necessary for all aspects of cognition, but /which/ aspects require it, and what the mind can /suffice/ with in its absence.
:PROPERTIES:
:Score: 2
:DateUnix: 1496243569.0
:DateShort: 2017-May-31
:END:

******* u/ZeroNihilist:
#+begin_quote
  We'd have to try taking out the original, putting in the synthetic one, and then removing that one and replacing it with the original. And we'd have to try it on you.
#+end_quote

Why would we have to try it on me specifically? Why not somebody who has had a brain scan before and whose damaged claustrum has placed them in a persistent vegetative state?

But sure, if I was the best or only candidate and the procedure would actually be medically useful (in efficacy, need, and availability), I'd let them try it on me if I thought it was reasonably likely to succeed.

#+begin_quote
  The question is not whether consciousness is necessary for all aspects of cognition, but which aspects require it, and what the mind can suffice with in its absence.
#+end_quote

A lot of sources on the internet equivocate between p-zombie consciousness (i.e. the subjective experience of qualia) and "awake vs. unconscious" consciousness.

I think that being awake is necessary for many aspects of cognition, but I see no reason for this to be a difficult thing to simulate. And in the case of the woman in the link, it seems likely that stimulating the claustrum caused her to lose this form of consciousness.

Notably, she did not appear to be a p-zombie, and I'd be very interested to hear of any research which somehow discovered a portion of the brain that controlled whether or not you were a p-zombie. Given that nobody has yet made any testable predictions for p-zombie status (indeed, the nature of the concept means testable predictions are actually logically impossible), I'm not holding my breath.

My belief is that p-zombies are a meaningless concept. Anything that is able to imitate consciousness is conscious. The inverse is not necessarily true; something could be conscious in a way we are not able to observe (though we'd have no way of knowing).

So I think that if you manage to make an upload which adequately imitates a person, you've just created a consciousness. If it doesn't adequately imitate humans, you may have made a flawed consciousness or no consciousness at all (with no real way of testing which is which).

Concerning ourselves with a concept which we can't actually measure over the ones we can seems utterly pointless.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 2
:DateUnix: 1496278589.0
:DateShort: 2017-Jun-01
:END:

******** u/deleted:
#+begin_quote
  Why would we have to try it on me specifically? Why not somebody who has had a brain scan before and whose damaged claustrum has placed them in a persistent vegetative state?
#+end_quote

Because the return of normal /behavior/ doesn't indicate normal consciousness /until/ we know the causal link between consciousness and behavior. That's what we have to establish by experiment, so we can't actually assume it.

Also, you can't tell other people are conscious by direct evidence ("shallow" inference), just conscious inference ("deep" inference).

#+begin_quote
  Notably, she did not appear to be a p-zombie
#+end_quote

Yeah, that was pretty neat. So that's pretty clear that consciousness is causally related to other functions and even behavior. Epiphenomenal p-zombies really are nonsense, which is nice to know.

#+begin_quote
  Anything that is able to imitate consciousness is conscious.
#+end_quote

I dunno about that. I tend to think that, for instance, a superintelligent AI (or other sufficiently powerful Thingamy) could make a meat-puppet act conscious without its actually having a mind. I mean, we already basically do it with cartoons, right? Or hell, with chatbots. We /know/ ELIZA has no mind in there whatsoever, but people still start attributing feelings and experiences to it based on intuition when they weren't told how the program works.
:PROPERTIES:
:Score: 2
:DateUnix: 1496283678.0
:DateShort: 2017-Jun-01
:END:

********* u/ZeroNihilist:
#+begin_quote
  Also, you can't tell other people are conscious by direct evidence ("shallow" inference), just conscious inference ("deep" inference).
#+end_quote

But how would installing an artificial claustrum in me prove anything? If my behaviour doesn't return to normal, it failed. But if my behaviour does return, I can't prove it restored my consciousness, and I certainly couldn't know it myself (because if I knew I wasn't conscious, it would alter my behaviour).

We could definitely prove that my ability to function was restored or not, but we could never prove that it was restored without consciousness.

#+begin_quote
  I tend to think that, for instance, a superintelligent AI (or other sufficiently powerful Thingamy) could make a meat-puppet act conscious without its actually having a mind.
#+end_quote

I agree that the meat-puppet wouldn't be conscious, but there would still be consciousness: the super-intelligent AI.

If we're able to make a simulation that appears to be conscious, there must be consciousness somewhere. It might not be in the simulation, but where else it would be hiding I don't know.

#+begin_quote
  I mean, we already basically do it with cartoons, right? Or hell, with chatbots. We know ELIZA has no mind in there whatsoever, but people still start attributing feelings and experiences to it based on intuition when they weren't told how the program works.
#+end_quote

If colloquial opinion is the metric you want to use for consciousness, it's not going to be a very useful concept.

Any test needs to adequately separate things we know are conscious (humans) from things we know aren't (recordings of humans, chatbots, etc.). Once we know it works as desired on those categories, we can then use it on things that we aren't sure about (animals, AI, uploads, clones, etc.) to see if they can be ruled out.

The starting point would probably have to be the Turing test. It may not be optimal (especially when translation and cultural difficulties are involved), but it's significantly better.

If something passes the Turing test and all other tests we come up with, we've effectively reached the limit of our ability to separate things along that axis. Either we rule that it is conscious or we discard the concept (at least until we get a better, more testable definition).

Essentially, if something passes all the tests but we still won't accept it into a category, we're not actually basing membership on the tests (or there's an additional, unspoken test, like "You have to have a flesh-and-blood human brain.").
:PROPERTIES:
:Author: ZeroNihilist
:Score: 2
:DateUnix: 1496286436.0
:DateShort: 2017-Jun-01
:END:

********** u/deleted:
#+begin_quote
  I agree that the meat-puppet wouldn't be conscious, but there would still be consciousness: the super-intelligent AI.
#+end_quote

It's not necessarily conscious. AIs don't have to think at all like us, as long as they accomplish their goals. An efficient AI for accomplishing some task may strip out a capacity for conscious experience that we built into it. We'd have to understand the precise causal role of consciousness to judge that.
:PROPERTIES:
:Score: 1
:DateUnix: 1496291039.0
:DateShort: 2017-Jun-01
:END:

*********** True, but isn't the question whether it's possible for things that aren't flesh-and-blood humans to be conscious?

If the AI is sufficiently good at imitating consciousness that we can't tell the difference, its model of consciousness is as good as our own. At that point, if we want to reject it we have to abandon any pretence of having a fair test and just outright say that only flesh-and-blood humans can be conscious.

That's really the crux of my position. If consciousness is testable, anything that passes the test is conscious. If consciousness is not testable, we have no reason to believe that anything is or is not conscious, so why bother?
:PROPERTIES:
:Author: ZeroNihilist
:Score: 2
:DateUnix: 1496292681.0
:DateShort: 2017-Jun-01
:END:


****** Well if it's not a physical phenomenon, then what the heck is it? I mean, every time nonphysical theories have been proposed, they've gotten curbstomped by science, eventually, so Bayes already hates that first sentence of yours with an unholy passion.

I'm uncertain if artificial neurons are capable of supporting conciousness. Either answer seems equally reasonable under my premise that conciousness arises from some fundamental property of physical matter. However, replacing brain tissue with synthetic neurons is /not/ uploading, and I really shouldn't ever have entertained that it was.

Here's the thing. Let's look at the brain as an insanely complex self-modifying analogue circuit. We replace an element of that circuit with a digital one. Repeating this process, we eventually have an insanely complex self-modifying digital circuit. Which is still, in fact, a circuit.

Uploading, on the other hand, takes an image of the structure of this circuit, and then moves it to some substrate which runs that image through a model continuously, generating new images that correspond with greater-or-lesser accuracy to the images that one would have expected the original structure to produce given the same stimuli. In this case, you get a lot of insanely complex 3D images which all represent insanely complex circuits. But, a map is not the territory; to use an old cliche somewhat inappropriately.

So there are several things to consider here.

First, assuming that the upload is concious, is the conciousness in the image, or in the model manipulating that image?

Second, not making the assumption of the first question, is the model even capable of conciousness. We know, at least for ourselves (maybe) that we're concious. By observing the structural details of ourselves, we know that such details generate conciousness, unless we have a nonphysical hypothesis, at which point we can't even have this discussion because out starting assumptions diverge too much for it to be productive. We know that no extant computer architecture acts in the same way - even in general - as concious agents do. Thus, while they might be capable of experience of the zeroth order (raw sensation), it is very, very dubious that they're capable of first order sensation (sensing sensations), and nearly unthinkable that they're capable of second order sensation (first order in a strange loop).

However, we know that computers are capable of running any computable program given sufficient time and space. Thus, they can run a person supposing that a person can be expressed as a computable process. *This is where the problem is.*

We can write really damn awesome programs that model the behaviour of neurons. But if the conciousness of beings with neurons arises from a fundamental property of the universe, then is that interaction still present in silico? It's certainly present in the CPU itself, but does it extend? Don't know. But if it doesn't, any destructive uploading process is equivalent to suicide... which you don't care about given

#+begin_quote
  Either consciousness is a necessary precondition for important things like thought, emotion, memory, etc. in which case the presence of any of those things implies consciousness, or it isn't a necessary precondition and we have no particular reason to care.
#+end_quote

I disagree with you. We can view ourselves as observed processes. We can view ourselves as the observer of internal processes. If we view ourselves as observed processes, then you're right, as long as the observed process seems to behave the same way, no problems. But it that's the case, then, rationally speaking, if I gave you an elixir that obliterated your self-awareness for all time and left your body acting the same way as it did before that juncture, then you should be as happy to drink it as not. Because, after all, under your view, there's nothing to fear, is there?

...actually, you should also feel similarly about killing yourself. Seeing as destroying your conciousness forever is experientially identical to death, the only thing that's different is that some people will be hurt a bit. but as long as it's incentivised such that the moral weight is in favour of suicide, it's allllllll good.

If you're a concious being, you can't treat yourself like you treat other people. You can't say that only the outside counts. Not unless you're willing to own the nihilistic implications. I wasn't.
:PROPERTIES:
:Score: 1
:DateUnix: 1496286820.0
:DateShort: 2017-Jun-01
:END:

******* u/ZeroNihilist:
#+begin_quote
  Well if it's not a physical phenomenon, then what the heck is it? I mean, every time nonphysical theories have been proposed, they've gotten curbstomped by science, eventually, so Bayes already hates that first sentence of yours with an unholy passion.
#+end_quote

I'm not saying it's not physical, I'm saying it's not located in any part of the brain. It's an emergent result of the interaction of a variety of neurological processes.

A car without an engine doesn't go anywhere, but it also needs wheels, axles, gears, etc. Further, the configuration of a car isn't the only one that gives rise to linear propulsion; you can have planes, trains, bacterial flagella, etc., each with their own way of producing the same emergent effect.

Likewise, if we can identify certain requirements for consciousness we shouldn't discriminate based on anything /but/ those requirements. Doesn't matter if it's a human brain, a simulated brain, or an AI.

#+begin_quote
  Uploading, on the other hand, takes an image of the structure of this circuit, and then moves it to some substrate which runs that image through a model continuously, generating new images that correspond with greater-or-lesser accuracy to the images that one would have expected the original structure to produce given the same stimuli. In this case, you get a lot of insanely complex 3D images which all represent insanely complex circuits. But, a map is not the territory; to use an old cliche somewhat inappropriately.
#+end_quote

I agree. But does consciousness depend on that level of precision? Your brain is in a perpetual state of flux, yet you stay conscious. Even people who suffer traumatic brain damage---some losing significant functionality, like the ability to make spontaneous decisions, or read, or remember---remain conscious (or, at least, I don't see people clamouring to denounce the intellectually disabled as p-zombies).

Whatever the root cause of consciousness, it must have a truly absurd degree of tolerance to encompass all the variance of the human mind.

The same is true of the quintessential you, but with significantly more stringent requirements because it has to exclude all other humans (and, depending on your definition, some past versions of yourself).

What we need is a set of criteria, applicable to all entities, by which we can say "This is me" or "This isn't me". Importantly, it needs to fit all past and possible future versions of yourself for which you would say "This is me". Ideally, these criteria would be based on our behaviour rather than our state (otherwise we're presuming a certain structure in the answer).

Even if we only count the past year of our lives as "me", there's an awful lot of variation our criteria have to cover.

So the question then becomes this: is there any set of criteria that would include all "me" and exclude all "not me"? If there is, what would satisfy those criteria?

And once we've come up with our test, why is it that we don't apply it to ourselves? We seem perfectly satisfied with an illusion of continuous memory, despite all the gaps and inaccuracies, and any upload worth its silicon would have the same illusion.

#+begin_quote
  First, assuming that the upload is concious, is the conciousness in the image, or in the model manipulating that image?
#+end_quote

Both.

Is consciousness in the structure of your brain, or in the physical laws that allow it to function? A human brain in a universe with different physical laws is not a conscious entity, because it doesn't function properly (or at all, perhaps).

It's the function that is important, not the components.

#+begin_quote
  Second, not making the assumption of the first question, is the model even capable of conciousness. [...] We know that no extant computer architecture acts in the same way - even in general - as concious agents do. Thus, while they might be capable of experience of the zeroth order (raw sensation), it is very, very dubious that they're capable of first order sensation (sensing sensations), and nearly unthinkable that they're capable of second order sensation (first order in a strange loop).
#+end_quote

How would you differentiate between a human that experiences second order sensation and a cyborg that mimics it? What is your test?

It seems like, for a lot of people in this thread, the test is "Is their processing unit a direct analogue of a human brain?". If that's the case, we're not really testing for consciousness, we're testing for humanity and calling it consciousness.

#+begin_quote
  But if the conciousness of beings with neurons arises from a fundamental property of the universe, then is that interaction still present in silico? It's certainly present in the CPU itself, but does it extend?
#+end_quote

My position is that consciousness is functional, not physical. The pattern matters more than the pieces. If consciousness is simulated it's still consciousness, and if consciousness cannot be simulated then the functionality must rely on something that cannot be simulated.

So, if consciousness cannot be simulated, what does it rely on that we can't simulate? Is there some fundamental function of the universe that is not possible to simulate? Why haven't we run into it yet, and how do we know it's vital for consciousness?

Perhaps more importantly, what would something that cannot be simulated even look like? We wouldn't even be able to describe it. If consciousness can't be simulated, it basically means the answer is "the soul" (or a very large number of very minor souls in each individual neuron).

#+begin_quote
  But it that's the case, then, rationally speaking, if I gave you an elixir that obliterated your self-awareness for all time and left your body acting the same way as it did before that juncture, then you should be as happy to drink it as not. Because, after all, under your view, there's nothing to fear, is there?
#+end_quote

Indeed, because if I act exactly the same despite having no self-awareness, self-awareness would clearly not be an important part of my behaviour.

However, my position is that such an elixir is impossible, even if you assume the existence of infinitely powerful magic. It would be like making a square circle in Euclidean space.

In simulating self-awareness, self-awareness exists. If it does not simulate self-awareness, it cannot imitate me. If the elixir results in an imitation of me, it must be self-aware, even if that self-awareness is not what I'm used to.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 1
:DateUnix: 1496292275.0
:DateShort: 2017-Jun-01
:END:


****** u/KilotonDefenestrator:
#+begin_quote
  Either consciousness is a necessary precondition for important things like thought, emotion, memory, etc. in which case the presence of any of those things implies consciousness, or it isn't a necessary precondition and we have no particular reason to care.
#+end_quote

There is a difference between a conscious being experiencing feelings and emotions and a software no more sentient than an excel sheet just displaying emotions as part of its programming.

I /enjoy/ the feeling of consciousness. Which means my upload would too. Uploading without conclusive, objective, testable, proof that consciousness survives upload would be against my upload's interests, and as such unethical for me to do.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1496228839.0
:DateShort: 2017-May-31
:END:

******* u/ZeroNihilist:
#+begin_quote
  There is a difference between a conscious being experiencing feelings and emotions and a software no more sentient than an excel sheet just displaying emotions as part of its programming.
#+end_quote

Right, but how do I know that you're not just a piece of software no more sentient than an excel spreadsheet? How do /you/ know that you aren't?

You can say to yourself, "I think therefore I am." or "I feel conscious.", but so would a program that duplicates your thought processes. Not only could an external observer not differentiate between conscious!you and unconscious!you, neither could either version of you. All you can do is assume that you are conscious, exactly as a p-zombie version of you would (although the latter would somehow behave as if it had this thought process without actually having the thought process).

Consciousness (of the p-zombie sort, not the "awake vs. asleep" sort) has no testable predictions. That's kinda the problem with it. People talk about whether animals are truly conscious or not, as if it would make a difference if they were.

#+begin_quote
  I enjoy the feeling of consciousness. Which means my upload would too.
#+end_quote

If your upload isn't capable of genuinely feeling emotion, what's the problem? Tautologically, it couldn't feel bad about its inability to feel emotion. Why would that be any different than creating any other program?
:PROPERTIES:
:Author: ZeroNihilist
:Score: 2
:DateUnix: 1496234177.0
:DateShort: 2017-May-31
:END:

******** u/KilotonDefenestrator:
#+begin_quote
  If your upload isn't capable of genuinely feeling emotion, what's the problem?
#+end_quote

It would defeat the purpose.

#+begin_quote
  You can say to yourself, "I think therefore I am." or "I feel conscious.", but so would a program that duplicates your thought processes
#+end_quote

The issue is exactly that - how do I verify that a program actually duplicates my thought process to that level of fidelity?
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1496238229.0
:DateShort: 2017-May-31
:END:

********* u/ZeroNihilist:
#+begin_quote

  #+begin_quote
    If your upload isn't capable of genuinely feeling emotion, what's the problem?
  #+end_quote

  It would defeat the purpose.
#+end_quote

I think there's a lot more utility to a simulated entity that mimics your thought processes than just whether or not it feels emotion, but ultimately that would be your decision.

#+begin_quote

  #+begin_quote
    You can say to yourself, "I think therefore I am." or "I feel conscious.", but so would a program that duplicates your thought processes
  #+end_quote

  The issue is exactly that - how do I verify that a program actually duplicates my thought process to that level of fidelity?
#+end_quote

Presumably by observing the simulation. It would be orders of magnitude easier to do so with a simulation than with a human, even.

If you can't find any difference by observation, then your upload would be closer in mental similarity to the current you than the version of you from last year. No point really sweating the issue at that point, I feel.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 1
:DateUnix: 1496239168.0
:DateShort: 2017-May-31
:END:

********** u/KilotonDefenestrator:
#+begin_quote
  I think there's a lot more utility to a simulated entity that mimics your thought processes than just whether or not it feels emotion, but ultimately that would be your decision.
#+end_quote

If you are just trying to make an AI, nevermind if it is an accurate copy of you, then I guess I understand your point. But I want an instance of me to continue, and not being able to feel happiness, curiosity, exhileration, lust, etc would be a dealbreaker.

#+begin_quote
  Presumably by observing the simulation. It would be orders of magnitude easier to do so with a simulation than with a human, even.
#+end_quote

If you make any kind of abstraction layer to make it easy to grasp trillions of chemical reaction per second, then you have to verify the abstraction layer. Or you can simulate a MRI scanning the simulated brain. Then you have to verify that as well. Any abstraction in the core simulation means loss of information and increased risk of inaccuracies.

Either way, you are trying to verify trillions of simulated chemical reactions against the source chemical reactions - that are already diverged - every second.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1496240269.0
:DateShort: 2017-May-31
:END:

*********** u/ZeroNihilist:
#+begin_quote
  If you are just trying to make an AI, nevermind if it is an accurate copy of you, then I guess I understand your point. But I want an instance of me to continue, and not being able to feel happiness, curiosity, exhileration, lust, etc would be a dealbreaker.
#+end_quote

That's fair.

#+begin_quote
  Either way, you are trying to verify trillions of simulated chemical reactions against the source chemical reactions - that are already diverged - every second.
#+end_quote

Have you verified your own trillions of simulated chemical reactions against those of you from one microsecond ago, or five minutes ago, or ten years ago?

It seems highly likely that you've never attempted to verify your own continuity of identity. Very few people would bother, and literally none to the standard you suggested here.

There's a good reason for that: unless the test you propose has a huge tolerance for divergence, the only version of you that would ever succeed at that test is the one taking it. You from ten years ago would fail it without question, unless you've been in stasis for that time.

It raises the question about why your standards are so much higher for a digital version of yourself. You have absolutely no idea if you're the same person you were at any point in the past, except what you can glean from memories (and memories are fallible, incomplete, and liable to change).

I get that it's because you want to be sure your upload is a copy of you, but I'd argue that if your upload passes a KilotonDefenestrator version of the Turing test, they're closer to current you than 90% of your past selves. You don't seem worried by the fact that in ten years you'll be a radically different person, yet you're not taking steps to verify that you're not diverging.

Basically, if you actually need to analyse each individual simulated chemical reaction in order to say that something is not you, that thing is you.

It'd be like if there was a weather model that had never made a single false prediction over a million years of operation, then you found one line of input that differed by a single bit and said, "Aha! It's not a model of the weather at all, it's modelling something that just happens to be completely indistinguishable from the weather."

In actuality, the way you prove a model false is by finding incorrect output, not incorrect input. If you cannot differentiate between the output of the model and the thing being modelled, then the model is at least as accurate as your tests.

*TL;DR:* If your upload is indistinguishable from present!you in a Turing test scenario, any test sufficient to discover divergence from present!you would also disqualify the vast majority of your past and future selves.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 2
:DateUnix: 1496281208.0
:DateShort: 2017-Jun-01
:END:

************ u/KilotonDefenestrator:
#+begin_quote
  Have you verified your own trillions of simulated chemical reactions against those of you from one microsecond ago, or five minutes ago, or ten years ago?
#+end_quote

Irrelevant. Me today is not an artifical construct made to mimic me yesterday.

#+begin_quote
  It raises the question about why your standards are so much higher for a digital version of yourself.
#+end_quote

Because an artifical version of myself is a difference in kind, not in scale.

#+begin_quote
  Basically, if you actually need to analyse each individual simulated chemical reaction in order to say that something is not you, that thing is you.

  I get that it's because you want to be sure your upload is a copy of you, but I'd argue that if your upload passes a KilotonDefenestrator version of the Turing test, they're closer to current you than 90% of your past selves. You don't seem worried by the fact that in ten years you'll be a radically different person, yet you're not taking steps to verify that you're not diverging.
#+end_quote

I don't care if my copy is an turing-complete entity or not. I want it to have the same scope of capabilities, including emotions and illusion of consciousness, as me. Because I enjoy those things, and it would be unethical to create a sentient copy of me without those things. It would be like intentionally fathering a child with handicaps, while somehow knowing that the child doesn't want those handicaps.

#+begin_quote
  It'd be like if there was a weather model that had never made a single false prediction over a million years of operation, then you found one line of input that differed by a single bit and said, "Aha! It's not a model of the weather at all, it's modelling something that just happens to be completely indistinguishable from the weather."
#+end_quote

I am not interested in something that is identical to me /to an outside observer/. I want to be sure that my copys /internal/ experience is also identical to my internal experience. Otherwise, from my perspective, it is a failed copy.

#+begin_quote
  TL;DR: If your upload is indistinguishable from present!you in a Turing test scenario, any test sufficient to discover divergence from present!you would also disqualify the vast majority of your past and future selves.
#+end_quote

I feel like we are talking about different things. I am a different person than past!me because I have accumulated more memories and experiences, new connections in my brain etc, to have changed opinions and tastes. An upload that, due to the experience of being uploaded, would wildly diverge in taste and opinions, would be an instance of me. No argument there. However, if the uploaded version of me loses the ability to have emotions, or loses the illusion of consciousness, then that is a entirely different order of difference. Comparing the two makes no sense to me.

Blatantly exaggerated to illustrate my point, it is like you are making the argument that since I have grown with one kilo since five years ago, removing one kilo of brain matter would make no difference, and I should be ok with someone doing that to me. Technically, I am 90% KilotonDefenestrator "weight complete", but that only captures one aspect of the changes.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1496302252.0
:DateShort: 2017-Jun-01
:END:

************* u/ZeroNihilist:
#+begin_quote

  #+begin_quote
    It raises the question about why your standards are so much higher for a digital version of yourself.
  #+end_quote

  Because an artifical version of myself is a difference in kind, not in scale.
#+end_quote

But you have no idea if any version of you except the current one would even pass the test. How can you be sure the test is even valid if you aren't checking it for flaws?

#+begin_quote
  I don't care if my copy is an turing-complete entity or not.
#+end_quote

The Turing test isn't about Turing-completeness. The former is a test for if something is "human-passing" (and the KilotonDefenestrator version would be if it could pass as you), the latter is a property of computing systems that are able to perform any computation a Turing machine can perform (it's the computer science way of saying "It can run any standard program").

#+begin_quote
  I want it to have the same scope of capabilities, including emotions and illusion of consciousness, as me. Because I enjoy those things, and it would be unethical to create a sentient copy of me without those things. It would be like intentionally fathering a child with handicaps, while somehow knowing that the child doesn't want those handicaps.
#+end_quote

Right, but if you asked your upload and it said "Yeah, I'm definitely conscious.", how would you know if it wasn't? You can't even verify that any other humans are truly conscious.

#+begin_quote
  I am not interested in something that is identical to me to an outside observer. I want to be sure that my copys internal experience is also identical to my internal experience. Otherwise, from my perspective, it is a failed copy.
#+end_quote

Humans don't have that level of self-reflection even for their own memories. If you examined your own brain you would be unable to discover your internal experience.

Even if you cloned yourself down to the smallest sub-atomic detail, you wouldn't know if your internal experiences matched. What you want can't be done at the moment.

Ironically, the only way you'd be able to verify they matched is by running simulations of both brains and comparing their states to identical input profiles (and, as usual, this test would fail for any version of you that wasn't the current one).

#+begin_quote
  However, if the uploaded version of me loses the ability to have emotions, or loses the illusion of consciousness, then that is a entirely different order of difference. Comparing the two makes no sense to me.
#+end_quote

So ask it. There is as yet no way of determining if something is conscious beyond just asking it, and I'm fairly certain there never will be (though we may develop more nuanced ways of "asking", like direct brain stimulation in an fMRI, for entities that can't reply with language).

And if you do somehow determine that your upload perfectly emulates you without emotions and consciousness, what exactly are emotions and consciousness doing in you that they aren't in your upload? Would it even make sense for something to be indistinguishable from you and yet lack consciousness?

It would be like if I removed internal combustion from a car engine, yet it still ran like normal. Either the engine didn't need internal combustion to function, or I didn't really remove it.

#+begin_quote
  Blatantly exaggerated to illustrate my point, it is like you are making the argument that since I have grown with one kilo since five years ago, removing one kilo of brain matter would make no difference, and I should be ok with someone doing that to me.
#+end_quote

Well, if somebody removed a kilo of your brain matter and it didn't affect your behaviour in any way, what's the problem? Seems like they got rid of something completely useless, because if it had any use---even the tiniest scrap of usefulness---your behaviour would change.

If I had an ironclad guarantee that my behaviour would be unaffected, I'd let people mess with my brain as much as they wanted. By definition I wouldn't change as a result. Of course, it's virtually impossible for that guarantee to exist, so it's a moot point.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 1
:DateUnix: 1496318167.0
:DateShort: 2017-Jun-01
:END:

************** u/KilotonDefenestrator:
#+begin_quote
  Right, but if you asked your upload and it said "Yeah, I'm definitely conscious.", how would you know if it wasn't?
#+end_quote

That's exactly my point. Until there is a way to verify that I will not upload. Asking an upload is not good enough, because the (by me) desired features are not about how things appear to outside observers.

#+begin_quote
  And if you do somehow determine that your upload perfectly emulates you without emotions and consciousness, what exactly are emotions and consciousness doing in you that they aren't in your upload?
#+end_quote

An emulation without emotions and consciousness is per definition not a perfect emulation, so it would be of no use to me.

#+begin_quote
  It would be like if I removed internal combustion from a car engine, yet it still ran like normal. Either the engine didn't need internal combustion to function, or I didn't really remove it.
#+end_quote

This is essentially the argument that any object identical to outside observers is identical.

That is, a software that is comprehensibly proven to *not* be conscious, self aware, etc. would be perfectly fine according to you as long as it can fake being those things good enough to fool outside observers.

I enjoy my current existence, illusionay or not, and I want the same for my digital instance.

You want to sell me a car. It looks like a car, and when I ask about it you show me the car moving on a highway, parking, etc. But you won't let me look inside the car. Your argument is that it doesn't matter if the seats are comfortable, or the AC works, because from the outside it clearly is a car, and that is all that matters to other people on the road. As the driver, that's not my main concern.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1496321003.0
:DateShort: 2017-Jun-01
:END:

*************** u/ZeroNihilist:
#+begin_quote
  That's exactly my point. Until there is a way to verify that I will not upload. Asking an upload is not good enough, because the (by me) desired features are not about how things appear to outside observers.
#+end_quote

It doesn't seem like it's even possible to satisfy your criteria, let alone do it with realistic future technology.

#+begin_quote
  An emulation without emotions and consciousness is per definition not a perfect emulation, so it would be of no use to me.
#+end_quote

I suppose I just don't get the point then. If it doesn't affect behaviour, including the response to questions like, "Are you conscious?" and "Are you happy?", then it's meaningless.

If I said, "An emulation without qowijeqlkjalsd is per definition not a perfect emulation.", because qowijeqlkjalsd would be exactly as meaningful. There needs to be a prediction that isn't met for something to be different.

#+begin_quote
  That is, a software that is comprehensibly proven to not be conscious, self aware, etc. would be perfectly fine according to you as long as it can fake being those things good enough to fool outside observers.
#+end_quote

No, I'm saying that it is literally impossible to prove something is not conscious. At best you could prove that you can't see a reason it would be conscious, but that's not the same thing.

If something says it's conscious and perfectly imitates the behaviours we would expect from a conscious organism, then that is exactly the same standard of proof that you or I provide. Anything that passes that test has to be conscious, or we have no reason to believe anything is.

My personal opinion goes further, which is that the imitation of consciousness requires consciousness.

It'd be like saying that somebody was faking knowing Japanese, despite being able to effortlessly talk with any Japanese-speaker they met. Maybe it's possible that it's all one big coincidence, that the words coming out of their mouth were accidentally correct every time, but that's true of any Japanese speaker.

#+begin_quote
  I enjoy my current existence, illusionay or not, and I want the same for my digital instance.
#+end_quote

Well, this hypothetical digital version of you states that they do enjoy their existence. I see no more reason to disbelieve them than I do to disbelieve the past versions of me (and in this scenario I could actually talk to your upload, whereas all past versions of me are dead).

#+begin_quote
  You want to sell me a car. It looks like a car, and when I ask about it you show me the car moving on a highway, parking, etc. But you won't let me look inside the car.
#+end_quote

No, the salesman lets you look inside the car. You can see that the inside is different to the inside of other cars. Even though it's clearly meant to feel the same, it isn't the same material and so can't be physically identical.

You declare that to be sure it really feels like the inside of a car, you'd need subatomically precise data about it during function, and an abstraction layer to interpret that information, and verification that the abstraction layer was valid.

The car salesman asks if you've ever checked to see if your own car passes that test or, indeed, if any car ever to exist has ever passed that test. It's a no to both questions. So even though you can't see any reason why it wouldn't feel like a car, and the driver inside tells you how much it feels like a car, you still question whether it feels like a car because you think "feeling like a car" is somehow engrained in the physical components.

Anyway, this whole conversation has become unproductive. If you'd like to rebut this post, I promise I will read your rebuttal and consider your points, but I won't reply.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 2
:DateUnix: 1496324098.0
:DateShort: 2017-Jun-01
:END:


******* u/deleted:
#+begin_quote
  There is a difference between a conscious being experiencing feelings and emotions and a software no more sentient than an excel sheet just displaying emotions as part of its programming.
#+end_quote

That really depends on what sort of cognitive thing emotions actually are.
:PROPERTIES:
:Score: 1
:DateUnix: 1496243659.0
:DateShort: 2017-May-31
:END:


***** You can define anything you like until the sun grows cold and you will be no wiser as to what universe you're living in.

You need to remember that your brain is a total mess and everything you call "you" or "self" or "consciousness" is an abstraction built into it by evolution to make it easier for the messy, messy brain to reason about its own component systems.

You are not one person, for starters, as split-brain patients tacitly prove. You are not even two persons. You are a committee of perhaps more than fifty brain regions that all try to figure out what you're supposed to be doing. Condensing fifty to one and providing a plausible (if wrong) narrative for how things came about is a lot cheaper to do than to reason directly about fifty things.

Human brains are lazy. Assume your intuitions are minimum-effort and that your interpretation of reality is 90% almost-accurate fabrication. Only then can you actually start reasoning about human thought.

Feelings are /neat/ and /compact/ and /cheap/ and /mostly accurate/ abstractions over a complicated reality. Evolution came across one way to implement them and applied it to /everything/ (or possibly it came across the same thing several times.)

Seriously, the answer to why we experience feelings is right there in evolution. The human brain isn't designed to think, it's designed to facilitate maximum procreation, and caloric austerity is important to that.

It is useful to wave away consciousness as being on the same level as "ineffable redness of red" because it lets us be better physicists and AI theorists. Looking for the source of consciousness is a fool's errand.

There is no "hard problem of consciousness" there is only "not smart enough" and "smart enough." You can pack up your sub-thermal-noise ghosts in the machine, and your P-zombies now and get back to figuring out how the hell Quantum Gravity works or something.
:PROPERTIES:
:Author: everything-narrative
:Score: 2
:DateUnix: 1496181715.0
:DateShort: 2017-May-31
:END:

****** Why even upload then? Why not just build a general AI (conscious or not, you don't care as long as it is smart) and leave the galaxy to the AI? You are not important, not even to you. The only thing that matters is that something smart gets to do all the cool stuff.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1496229197.0
:DateShort: 2017-May-31
:END:

******* I wrote

#+begin_quote
  There is no "hard problem of consciousness" there is only "not smart enough" and "smart enough." You can pack up your sub-thermal-noise ghosts in the machine, and your P-zombies now and get back to figuring out how the hell Quantum Gravity works or something.
#+end_quote

You wrote

#+begin_quote
  Why even upload then? Why not just build a general AI (conscious or not, you don't care as long as it is smart) and leave the galaxy to the AI? You are not important, not even to you. The only thing that matters is that something smart gets to do all the cool stuff.
#+end_quote

And now I'm both sorry for not clarifying my last point, but also I'm laughing my arse off at your naive approach. It's endearing and I love you in a genuine and non-patronizing manner. It is /good/ that you are inquisitive.

The hard problem of consciousness is, as you might note, called the hard problem of consciousness, not "the hard problem of human identity."

I wanna upload because then a version of me which is close enough to my meatself will get to do a lot of cool stuff, and I care about /that./ I don't care about whether it is "conscious" only that it is /me/.

This includes but is not limited to:

- Upload-me remembers the same things as me.
- Upload-me wants to shape the future in the same way as me.
- Upload-me has my good sense of humor.
- Upload-me can continue my ongoing works of fiction.
- Upload-me can do the same kind of CS research that I can.
- etc.

The reason why I care about making an upload is:

- Upload-me will not have to wear a messy flesh suit.
- Upload-me does not come with a baked-in expiration date.
- Upload-me will be able to more freely express their gender-identity.
- Upload-me can self-modify more freely.
- Upload-me can fork and restore-from-backup as elementary actions.
- etc.

Lots of good stuff.

I /want/ a mind-clone of me to get some good stuff.

That aside, you have to consider a few interesting twists:

- What if uploading is a gradual process? We fill your brain with nanites which attach to your neurons and gradually both take over neuron function and also builds a working model of your brain in a computer; eventually shutting down while their neighbors start taking data directly from the sim. In the end, you have a dead, nanite filled brain, and the motor neurons and sensory neurons are controlled by clusters of nanties talking to a simulated brain. (Ship-of-Theseus uploading.)

- What if uploading is destructive? We put you in a Star-Trek teleporter and teleport you into virtual reality --- there is no "you" left behind.

- What if we only upload half your brain, perform a hemispherectomy, and link up the real half and the virtual half with an artificial corpus calloum that terminates in a WiFi dongle?

In summary, you're angsty about uploads and consciousness. You're all "but what if!!!" with tears in your eyes, and that's OK.

I am the very opposite. I'm like "hold my beer and watch this" before diving into an upload-machine. I don't give two shits and I don't have time for angsty philosophical discourse, because I have anime dudes to bang in virtual reality, and virtual drugs to inject into my virtual pineal gland, and I wanna replace my spine with an internet connection.
:PROPERTIES:
:Author: everything-narrative
:Score: 2
:DateUnix: 1496232793.0
:DateShort: 2017-May-31
:END:

******** Thank you for the hilarious answer.

My fear is that I upload and my copy "just goes through the motions" without actually getting to enjoy anything.

It will say it is happy when asked, but there is no one there to enjoy all the cool stuff it goes out to do. Lights are on, but no one is home. Since the copy is me, and I would be horrified if I wasn't there to enjoy shit, I don't want to risk inflicting that on my copy.

As for actually "moving" my consciousness into a computer, I don't think that is possible. It would have to be me creating a copy much like a father lives on through his children.

I think of myself as a chemical process that it sometimes aware of itself. /Maybe/ replacing a neuron at a time (at the rate that the brain normally adapts to changes) with functionally identical synthetic (hardware) neurons could work. The chemical process (physical really) is maintained and is no more unnatural than losing a few brain cells after a wild weekend.

Destructive upload is off the table, naturally. As is half-brain. One is death and the other is massive brain damage.
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1496233876.0
:DateShort: 2017-May-31
:END:

********* Enjoyment sits in the brain. Check for the enjoyment related neuron clusters and see that they function normally.

On a meta-level, remember that enjoyment exists for no deeper reason than to make you do shit that's useful to your continued survival and procreation. It's just another brain function, and there's no Enjoyment-Zombies for the same reason as there's no P-Zombies.

On a meta-meta level it seems like you're not actually accepting a materialist view of the mind. You seem to think that two brains containing 99%+ of the same information, memories, response patterns, etc. will somehow be ineffably "different" because one is digital.

There is no "copy," there is two originals.

#+begin_quote
  I think of myself as a chemical process that it sometimes aware of itself. [...]
#+end_quote

Okay, tough customer, consider this:

What if instead of scanning your brain and simulating it abstractly, I buy a quantum computer the size of the moon, scan you to subatomic resolution and simulate you quark-by-quark?

Say I put you under general anaesthetic with zero-EEG and in a state of protective hypothermia so your metabolic rate is basically zero, and construct a physical replica of you, accurate to sub-molecular scale, then wake you both up in identical rooms?

Say I reduce you to a brain-in-a-jar and let you pilot an incredibly realistic android body?
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1496234930.0
:DateShort: 2017-May-31
:END:

********** u/KilotonDefenestrator:
#+begin_quote
  Enjoyment sits in the brain. Check for the enjoyment related neuron clusters and see that they function normally.
#+end_quote

How do you give an MRI to a CPU running atom-level simulations of chemical reactions?
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 0
:DateUnix: 1496238113.0
:DateShort: 2017-May-31
:END:

*********** Very late answer, but... run an atom-level simulation of an MRI. :-)
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1505043414.0
:DateShort: 2017-Sep-10
:END:


**** There's a fun theory by Scott Aaronson. If we assume that something about consciousness cannot be exactly cloned (which is reasonable, since instruments have a finite measuring precision):

"this suggests, to be conscious, a physical entity would have to do more than carry out the right sorts of computations. It would have to, as it were, fully participate in the thermodynamic arrow of time: that is, repeatedly take microscopic degrees of freedom that have been unmeasured and unrecorded since the very early universe, and amplify them to macroscopic scale. [...] Such a being also couldn't be instantiated by a lookup table, or by passing slips of paper around, etc."

So an approximately-copied upload would be exactly measurable and thus not conscious.

Do read the whole blog post, it is fascinating. [[http://www.scottaaronson.com/blog/?p=2756]]
:PROPERTIES:
:Author: rhaps0dy4
:Score: 3
:DateUnix: 1496172850.0
:DateShort: 2017-May-31
:END:

***** Bleh. Scott Aaronson forgets the manner in which human beings came about. Even if "consciousness" (which I will from now on refuse to ever not put in scare quotes) does amplify microscopic degrees of freedom, it has to do so because Evolution built it so; which means this process must be /highly/ repeatable.

And I really do mean that. It must be beneificial from an evolutionary standpoint and /highly/ repeatable to even arise within the human brain. And also there must be some process which in this amplification differentiates microscopic degrees of freedom from the not insignificant thermal noise at 37 degrees centigrade.

Human brains are not perfect quantum computers. They are messy, hot, slow, and made of emergently grown gloop specified in a data compression format that is A) incomprehensible and B) dependent on environmental factors, and is designed literally by the stupidest system that isn't random chance.

Evolution builds simple systems out of poor building blocks. Like associative socially-argumentative reasoners, and arithmetic. Not devices that almost but not quite breaks the information-theoretical formulation of thermodynamics.

Pull the other one, it posits that the soul sits in the pineal gland.
:PROPERTIES:
:Author: everything-narrative
:Score: 6
:DateUnix: 1496181045.0
:DateShort: 2017-May-31
:END:

****** minor quibble - in order for a trait to survive through natural selection it need only be insufficiently harmful, not necessarily "beneificial"
:PROPERTIES:
:Author: sephirothrr
:Score: 1
:DateUnix: 1496294387.0
:DateShort: 2017-Jun-01
:END:

******* Well, in the short term, sure.

Given a gene mutation which confers a fitness advantage factor of 1 + r, the probability that it will become universal in a sexual population is 1 + 2r (the value of r is very small in almost all cases, and this is a first approximation; it mispredicts for large r in simulations.)

Hence, if a mutation has epsilon fitness benefit, it never becomes universal.

Sensation-of-Consciousness is universal in the sexual population of humans, hence it had positive fitness benefit in our evolutionary ancestry.
:PROPERTIES:
:Author: everything-narrative
:Score: 1
:DateUnix: 1496308956.0
:DateShort: 2017-Jun-01
:END:


****** [deleted]
:PROPERTIES:
:Score: -2
:DateUnix: 1496189321.0
:DateShort: 2017-May-31
:END:

******* u/deleted:
#+begin_quote
  If you want to talk about qualia instead go ahead, but that's not what I mean by conscious.
#+end_quote

That is, however, what /everyone else/ means by it.
:PROPERTIES:
:Score: 4
:DateUnix: 1496244324.0
:DateShort: 2017-May-31
:END:


***** u/deleted:
#+begin_quote
  "this suggests, to be conscious, a physical entity would have to do more than carry out the right sorts of computations. It would have to, as it were, fully participate in the thermodynamic arrow of time: that is, repeatedly take microscopic degrees of freedom that have been unmeasured and unrecorded since the very early universe, and amplify them to macroscopic scale. [...] Such a being also couldn't be instantiated by a lookup table, or by passing slips of paper around, etc."
#+end_quote

Cognition /does/ fully participate in the thermodynamic arrow of time. That's entailed by embodied-cognition and predictive-mind theories. It doesn't merely compute in the symbolic sense, it /infers/, and that inference requires taking in low-entropy sense-data and outputting waste heat in the blood.

It's an engine, not a passive object. Maybe you could temporarily stop the engine and transplant it to some other car, but you couldn't yank it out in the middle of the highway without stuff going severely wrong.
:PROPERTIES:
:Score: 3
:DateUnix: 1496244142.0
:DateShort: 2017-May-31
:END:

****** u/rhaps0dy4:
#+begin_quote
  entailed by embodied-cognition and predictive-mind theories. I'm not familiar with these theories. Are they reductionist to the patterns of matter? Could a mind in embodied-condition and predictive-mind theories be briefly instantiated by random patterns of particles, a Boltzmann brain?
#+end_quote

If so, they don't entail participation in the arrow of time; since random patterns that momentarily /look like they do the work/ are conscious too.

Apologies if I'm arguing against a strawman, if that is the case just dismiss my objection.
:PROPERTIES:
:Author: rhaps0dy4
:Score: 2
:DateUnix: 1496331158.0
:DateShort: 2017-Jun-01
:END:

******* According to the embodied cognition theory, a Boltzmann brain can't really think or be conscious. It doesn't have the living body or the continuous existence to do that. It's a process, not a state.
:PROPERTIES:
:Score: 3
:DateUnix: 1496331335.0
:DateShort: 2017-Jun-01
:END:


*** Given your premise of sleep being a non-interruption, I should like to mention a third scenario that might be acceptable- gradual uploading.

You are put to sleep. In one moment, a suitable neuron, not firing at the moment, is replaced with wires to a computer that emulates it. Next, an adjacent neuron is replaced with wires to the same computer. Of course, one of the second neuron's synapses went to a wire replacing the first neuron. Since this synapse is also replaced with a computer-wire, it can be replaced altogether with the computer internally simulating this connection.

The process is repeated for all neurons, all of which are replaced so that cognition is not interfered with. Would this qualify?
:PROPERTIES:
:Author: LupoCani
:Score: 1
:DateUnix: 1496182353.0
:DateShort: 2017-May-31
:END:

**** I would prefer we rebuild the metaphorical ship of theseus while awake. I can better tell if some part of the process isn't working if I'm there to supervise (so to speak).

But, hypothetically, you are correct, the process should work without a loss of identity processing.
:PROPERTIES:
:Author: Arizth
:Score: 2
:DateUnix: 1496189386.0
:DateShort: 2017-May-31
:END:


**** Depends on the fidelity.

If we're saying that the computer fungus slowly eats my brain and write it to an super-duper-uber next-gen x86 PC, or anything classical in general? Gonna go with /no/. No classical computing substrate will preserve the operation of the mind with total fidelity, so the graph of that person's futures (as a heatmap of probability of branches) wouldn't look the same as mine. Small computational errors effectively mean that if we put the upload and original in identical universes, with equivalent brain-states, those universes won't branch the same way (assuming mwi is ^{t^{r^{u^{u^{u^{e^{e}}}}}}} ).

If we say it's replacing normal neurons with computronium neurons...? Well, is long as they fire appropriately, this method should be far more likely to preserve experiential identity. So it's already better. You can also, by lengthening the upload process indefinitely, make it almost arbitrarily likely that your configuration space locus is a necessary part of the upload's stochastic history. I need to devote a few days of careful thought to what, precisely, that means before I venture anything like an opinion on it, but my intuition a vaguely indicating in favour of your proposition. A notion of identity is after all useless unless it can actually identify things uniquely. We'll call this a "Maybe?".

If we're talking 1:1 substrate, then hell /yeah/ that's me. That was already the case before we made it slow, but the beauty of 1:1 substrate and 1:1 information uploading is that it's still you, no matter how you do it. That's why it's the technology I'm holding out for.
:PROPERTIES:
:Score: 1
:DateUnix: 1496188808.0
:DateShort: 2017-May-31
:END:


** I don't think you can say that the number of "you" goes up to two that would require for me to believe that "I" am a constant and that I don't change subtly from instant to instant, I don't believe I am the same person I was a second ago if that were true then I couldn't have memory or growth or thought at all! All that happens is that there are now two people based on past-me.

If I fork myself (pun not intended but hilarious nonetheless) I make another person and it would be unreasonable of me to expect that new person not to be self-interested and even to choose to die for "original" me.

The text makes a good point about p-zombies though, I have been warned from over-using the word "emergence" but I think that the process of consciousness could just as easily happen in a brain as in a sufficiently detailed emulation of a brain.
:PROPERTIES:
:Score: 2
:DateUnix: 1496139420.0
:DateShort: 2017-May-30
:END:

*** It is unreasonable for you to assume that your forkself would die for you, yes. And it would be unethical for you to decide that you have the right to kill them as you wish. But it is still possible to have such a mindset that the upload would himself decide to die if it would benefit you, as well as other forked-selves.
:PROPERTIES:
:Author: redrach
:Score: 2
:DateUnix: 1496147159.0
:DateShort: 2017-May-30
:END:

**** I guess that's true, there were people out there who have sacrificed their lives for others so I know that the mindset is humanly possible. I think it would require a type of mental discipline I don't have to deliberately set out to create a situation for myself wherein I would kill myself after completion of a task. Even if I pre-committed to kill myself I would almost definitely start rationalizing reasons to stay alive just a bit more and so on.
:PROPERTIES:
:Score: 3
:DateUnix: 1496148581.0
:DateShort: 2017-May-30
:END:

***** That's why I called Patternism an "advanced technique".

In my experience, it took years of playing with the concept to come around to it. And I obviously still haven't tested it in practice.

As far as I can tell, the key is to /identify yourself with the pattern itself/. For a simple System 1 proxy, you can just identify yourself with the surviving copy.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1496180251.0
:DateShort: 2017-May-31
:END:


**** Certainly possible, but without either shenanigans to alter their personality or a consistent pattern of reintegration of forks, such that each instance has a personal experience not just of being one of a set of "real you's" but the personal experience of /being a set/, I don't see it being common or typical.
:PROPERTIES:
:Author: GopherAtl
:Score: 1
:DateUnix: 1496154021.0
:DateShort: 2017-May-30
:END:


** Second criticism:

Why bother imbuing a "fork" that you intend only to complete a specific task with your personality?

To be honest my personality is shit at completing tasks, in an absolute sense. I may be more capable than many humans, but I'm in the bottom percentile of intelligent agents when it comes to completing specific tasks. If I'm going to fork, why not specialize? Why not enhance a fork to be smarter, faster, more focused, single minded even?

And once you are going that far, why give them personality and the simulcrum of consciousness /at all/? This gets at the core contradictions behind "uploading."

If the purpose is to get stuff done, it's wildly inefficient to add baggage like personality and character flaws. The only reason to upload is out of some sense of sentimentalism. You want to make a snapshot of yourself you can send into the future, not because it will be useful to anyone there, but because you want to persist -- while at the same time admitting that this projection isn't you.
:PROPERTIES:
:Author: wren42
:Score: 2
:DateUnix: 1496173241.0
:DateShort: 2017-May-31
:END:

*** You can always do both.
:PROPERTIES:
:Author: FeepingCreature
:Score: 1
:DateUnix: 1496180327.0
:DateShort: 2017-May-31
:END:


*** u/nicholaslaux:
#+begin_quote
  Why not enhance a fork to...
#+end_quote

My understanding around discussions if uploads is that they are posited (right or wrong) to be easier to create than actually understanding how intelligence works.

So the theory would be that you can't improve your fork, or doing so might be much less cost effective than just throwing more people and time at it, as long as it's a problem you expect more effort to ever feasibly be able to solve it with.
:PROPERTIES:
:Author: nicholaslaux
:Score: 1
:DateUnix: 1496199685.0
:DateShort: 2017-May-31
:END:

**** yeah that's ridiculous. if we have technology to completely replicate a human brain we have the technology for AI. the version of uploading you are referring to sounds like Robin Hanson's "Age of Em" nonsense. There's no way we hit full brain simulation before we hit strong AI.
:PROPERTIES:
:Author: wren42
:Score: 2
:DateUnix: 1496203182.0
:DateShort: 2017-May-31
:END:

***** That seems to be the premise of basically anything that talks about uploads like this. Having very little knowledge on the biological side of things, I honestly couldn't comment either way on the realism (or not) of the scenario.
:PROPERTIES:
:Author: nicholaslaux
:Score: 1
:DateUnix: 1496203905.0
:DateShort: 2017-May-31
:END:

****** It was recently found out that gut bacteria can affect our mood. The digestive system has as many neurons as a cat brain. Chemical signals throughout the body has all kinds of downstream effects on our minds.

A fully qualified upload of me would have to be able to accurately simulate all the [[https://www.quora.com/On-average-how-many-chemical-reactions-happen-in-the-body-in-one-second][thousands of billion billion chemical reactions *per second* that goes on in the human body]].
:PROPERTIES:
:Author: KilotonDefenestrator
:Score: 1
:DateUnix: 1496230654.0
:DateShort: 2017-May-31
:END:


** The argument given against P-Zombies also works for animals though. They could just as easily be conscious as we are. Heck, plants too.

Even rocks: How do we know rocks aren't conscious? Maybe gravity isn't a real thing, and rocks are sentient beings whose goals are to clump together.

Even fictional imaginary characters: 2D comic strip characters could be sentient, but forced to do whatever the artist wants. Maybe every time you imagine something bad, that bad thing is happening to the sentient imaginary beings you imagined.
:PROPERTIES:
:Author: ShiranaiWakaranai
:Score: 2
:DateUnix: 1496117923.0
:DateShort: 2017-May-30
:END:

*** The central idea of a p-zombie is that they appear to be sentient yet are not. If something doesn't appear to be sentient, it can't be a p-zombie.

It's possible that rocks are sentient, but we're incapable of discovering this. That's also true of everything though; the Earth, the number 6, a set of all sets that do not contain themselves. All of those /could/ be sentient, but given the complete lack of evidence for it (or, indeed, any coherent definition of what that would even mean), we can't really treat them as if they are.
:PROPERTIES:
:Author: ZeroNihilist
:Score: 11
:DateUnix: 1496142244.0
:DateShort: 2017-May-30
:END:


*** u/deleted:
#+begin_quote
  Even rocks
#+end_quote

I think that's a bit of a leap, we know (to a small extent) the underlying mechanisms for how a human can think and we can infer that something that doesn't have a processing system it can use to think can't think.
:PROPERTIES:
:Score: 6
:DateUnix: 1496139671.0
:DateShort: 2017-May-30
:END:

**** Rocks - well, some rocks, at any rate - are mainly silicon. I have a chunk of (meticulously designed) impure silicon not very far away from me that can either think or do something very closely analogous to thinking when electricity is applied to it.

Sure, the odds of any random chunk of rock containing a CPU are unutterably tiny - and the odds of it getting any power are even smaller - but there is a known and understood mechanism by which a chunk of rock can be used to perform calculations.
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1496218347.0
:DateShort: 2017-May-31
:END:

***** u/deleted:
#+begin_quote
  unutterably tiny
#+end_quote

Isn't that just a synonym for negligible?

The reason a human can think is because we are imperfect self-replicators that gained an advantage by thinking. The process of consciousness was built up over the course of literally unimaginable timescales and was helped along by the fact that there were selection pressures in favour of it. Rocks don't gain an advantage by thinking and afaik don't replicate themselves, to suppose that any given rock has a chance of being a naturally-formed processor powered by unknown mechanisms is technically correct but it's so unlikely that it doesn't even bear thinking about.

But I suspect that you know all that.
:PROPERTIES:
:Score: 2
:DateUnix: 1496218936.0
:DateShort: 2017-May-31
:END:

****** [[/twibeam][]] Yeah, I'm mostly just being argumentative for the fun of the debate.

[[/sp][]]

#+begin_quote
  The reason a human can think is because we are imperfect self-replicators that gained an advantage by thinking.
#+end_quote

[[/twiponder][]] Technically, that's the mechanism by which an unutterably tiny chance of carbon and other atoms coming together in a shape capable of thought became, well, less unutterably tiny. Evolution doesn't make otherwise impossible things happen - it makes otherwise /incredibly unlikely/ things happen.

[[/sp][]]

[[/twipride][]] So, yes, you're right. The odds of any given rock being a naturally-formed processor by unknown mechanisms is so unlikely that one can't expect it to happen. (I wouldn't say it doesn't bear thinking about, but that's only because it's occasionally fun to speculate about extreme low-probability but nonetheless entertaining possibilities). Of course, the odds of a given chunk of silicon being a human-formed processor powered by well-known mechanisms is a lot higher... but it's generally easy to tell those apart from natural rocks, as they tend to be very visibly distinct (to the point where they're often not even considered in the category of 'rocks').
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1496225860.0
:DateShort: 2017-May-31
:END:

******* Wow, it looks like I need to get in the habit of using preciser language. I don't think I can go much further down the ladder of "Well, technically..."
:PROPERTIES:
:Score: 2
:DateUnix: 1496238341.0
:DateShort: 2017-May-31
:END:

******** [[/twisad][]] ...I do tend to go a bit far, don't I?

Well... I hope you've at least found /some/ entertainment in this debate.
:PROPERTIES:
:Author: CCC_037
:Score: 2
:DateUnix: 1496245502.0
:DateShort: 2017-May-31
:END:

********* I have, I'm a "well actually" sort of person myself.
:PROPERTIES:
:Score: 2
:DateUnix: 1496270442.0
:DateShort: 2017-Jun-01
:END:

********** [[/twibeam][]] Most excellent!
:PROPERTIES:
:Author: CCC_037
:Score: 1
:DateUnix: 1496287818.0
:DateShort: 2017-Jun-01
:END:


*** Although I completely disagree with your extrapolation, it's more important that I think your extrapolation is entirely non-applicable.

The point here is that your upload would use the /exact same/ argument to believe itself conscious as you, ass in chair in this literal moment, /currently/ use to come to the same conclusion.

This argument doesn't even have to apply to /other humans/. If you believe yourself conscious, and you believe yourself copied, your copy has a copy of that belief in your consciousness.

Then again, all is theory until and experiment is viable.
:PROPERTIES:
:Author: narfanator
:Score: 2
:DateUnix: 1496164969.0
:DateShort: 2017-May-30
:END:


*** Yes, which is why we consider people conscious /if we have evidence for this/.

The point of the argument is that we have the same [edit kind] of evidence for ourselves being conscious as the upload being conscious. But that is /not nearly/ the same degree as we have for rocks being conscious.
:PROPERTIES:
:Author: FeepingCreature
:Score: 2
:DateUnix: 1496180373.0
:DateShort: 2017-May-31
:END:


*** And this is why epiphenomalism and panpsychism fail to explain anything.
:PROPERTIES:
:Score: 1
:DateUnix: 1496244836.0
:DateShort: 2017-May-31
:END:
