#+TITLE: [BST] What simple/interesting rationalist ideas should I write a short story about?

* [BST] What simple/interesting rationalist ideas should I write a short story about?
:PROPERTIES:
:Author: raymestalez
:Score: 8
:DateUnix: 1424834535.0
:END:
Recent topic about [[http://www.reddit.com/r/rational/comments/2w7ayx/q_rationality_parables/][rational parables]] describes something I wanted to write for a long time - short rationalist fictional stories that would be like metaphors teaching cool and useful thinking patterns(rationality principles, various wise ideas from programming/entrepreneurship, etc).

I thought that some of the stories would be based on Less Wrong or PG essays, although I am having a little trouble expressing them in a form of a story.

I've managed to come up with a few ideas, but I thought it would be awesome if you could help me to brainstorm some more.

So I want to ask you:

- What are some simple principles/ideas that could be well expressed in a story?
- Can you imagine some examples of rationality principles being expressed as metaphors?
- What are some cool concepts that you think would be fun to read/write about?
- Is there anything specific that you would want to read a story about?


** I know nothing about Eliezer's writings on Timeless Decision Theory (haven't gotten to that sequence yet) or very much about Game Theory, but I would LOVE to see Timeless Decision Theory and Game Theory explained via a Choose-Your-Own Adventure Book. I'd make a choice and then read the mathematical/logical implications of that choice afterwards. Maybe throw in some situations involving clones or being part of a swarm intelligence where you are part of a crowd who all make the exact same choices as you do.

I'd also would like to see a story of a person using catchy techniques to deal with akrasia written as a story for children. Teaching kids how to be motivated to do things they don't like through a good story or a song would be huge.
:PROPERTIES:
:Author: xamueljones
:Score: 7
:DateUnix: 1424842204.0
:END:

*** You and an accomplice rob a bank and steal a car to get away in. The police catch you in the stolen car, but you've already hidden the money. They're pretty sure you're also the bank robbers, but they have no proof that will stand up in court. You'll both get a year in prison for stealing the car, and you'd both get ten years for the bank robbery if you were dumb enough to talk.

The police would much rather get a conviction for the bank robbery though, and they're smart. They offer both of you a plea bargain - claim the other person is the bank robbing mastermind that led you astray, and you'll get a light probation while your accomplice gets 20 years in prison. However, if you both do this, they'll have enough evidence to put you both behind bars for 10 years. They're holding the two of you separately, so you can't discuss this.

That's the classic prisoner's dilemma, and you've probably heard it before. To summarize:

#+begin_quote
  Both stay silent: 1 year prison for both of you

  You rat, accomplice stays silent: light probation for you, 20 years for them

  Your accomplice rats: 20 years for you, light probation for them

  You both rat: 10 years in prison for both of you
#+end_quote

You can't control your accomplice's decision here. If they stay silent, you can avoid a year in prison by ratting on them. If they rat on you, you can avoid ten years in prison by ratting on them. So, regardless of what they choose, the action that helps you is to rat. Unfortunately, this is the same logic as your accomplice uses, and you both get 10 years in prison, instead of the both getting 1 you'd have preferred.

That's a classic problem in game theory, and there are various solutions to it, some more satisfying than others.
:PROPERTIES:
:Author: OffColorCommentary
:Score: 2
:DateUnix: 1425364564.0
:END:

**** Now, let's change this scenario up. Your accomplice is your clone. Not a genetic clone like Dolly, but rather a sci-fi clone created shortly before the heist, who thinks just like you do.

Since your clone thinks just like you do, you don't have to guess what their choice will be. Whatever logic you use, you can assume your clone will use the same logic, and come up with the same decision as you. If you choose to rat, so will they, and you'll both get 10 years in prison. If you choose to stay silent, so will they, and you'll both get 1 year in prison. So you choose the good one, obviously. This resolves the entire problem nicely.

However, we just drew a very strange causal arrow - "I choose this so someone else I have no communication with will too" - which means there are people who object to it, of course.

Being limited to using this if you have a clone isn't very good. Eliezer is mostly interested in the case where you have two AI agents that have access to each other's source code. If they both understand TDT, they can just examine the other's code to find the most mutually beneficial trade, trusting that the other will uphold their end of it (and it's not like they can defect, you'd read their "and now I defect code" in the process).

You can also sort of extend it, loosely, to anyone who you know knows TDT and who thinks a lot like you. Humans are all pretty similar for a lot of purposes, so that might come in handy.
:PROPERTIES:
:Author: OffColorCommentary
:Score: 2
:DateUnix: 1425365227.0
:END:


** I can't remember the formal name for the bias, but we have a tendency to think that, if we see a new weird thing, it's a special case of what we know, and usually completely miss the possibility that what we know is a special case of the new weird thing. I'd like to see a story that deals with that directly.

We can find an example of this in HPMOR. Harry finds out about magic, and one of the earlier things he thinks of is that someone built a source-of-magic machine (using standard physics or close to it) and it's responsible for the new weird thing. He doesn't consider the idea that magic is normal, and someone did something special that caused the low-magic environment he's used to. One of many ways this could have happened is if the true physical laws are dangerous, and our precursors added safety mechanisms like conservation of energy with limited specific exceptions. Any specific story like that is unlikely, but despite that, the fact that magical physics is clearly less limited than muggle physics should be a flashing red light saying that muggle physics is the special case.

I wouldn't be that surprised if Eliezer intentionally wrote that blind spot for Harry, and this bias is part of one of the last plot twists of the series. But I wouldn't be THAT surprised if even Eliezer missed the magic=normal case being the most likely one, so I'd still like to see a story explicitly about this bias.
:PROPERTIES:
:Author: OffColorCommentary
:Score: 7
:DateUnix: 1424844096.0
:END:


** The deer is the king of the forest. He marches around to make sure everything is alright.

The bear is angry as the deer's march wakes him up in the morning. "You stupid clumsy oaf! Trample away!" They got into a fight. Eventually, the sour bear left the forest.

The beaver was desolate as the deer trampled down his dam. Again and again the busy beaver built it up, because the deer was too heavy. As the winter came, the beaver had gathered too little food and starved.

The deer trampled down the hedgehogs house of leaves. The hedgehog complained, "Oh king, why do you have to march right here?" "On the left there is a moat and on the right a rock," answered the deer. "You demolish my house here." The deer worried about the hedgehog, but he could not march another path. So, together they built a new house for the hedgehog on the hill. It was bigger and nicer than the old one.

Moral: have a constructive talk instead of escalating or silently suffering.

It sounds so trite, yet I fail so often myself.

Also, quickly written by a non-native. ;)
:PROPERTIES:
:Author: qznc
:Score: 5
:DateUnix: 1424903554.0
:END:

*** Wow, thank you, very cool!! Great example, that will help =)
:PROPERTIES:
:Author: raymestalez
:Score: 1
:DateUnix: 1424908676.0
:END:


** "Admitting your wrongness and uncertainty is not inherently virtuous, at least not by my reckoning, but they're necessary because it's awful hard to stop being wrong if you never admit it and change your mind." ([[http://lesswrong.com/lw/i9/the_importance_of_saying_oops/][The Importance Of Saying Oops]])

Don't forget what's important! If you're going on a long trip, you will probably forget a towel, or extra socks, or some other trivial thing. Don't worry about that. But do double-check that you have your wallet, and working transportation! ([[http://lesswrong.com/lw/dm7/magic_players_how_do_i_lose/][How do I lose?]])

We (fictional characters included) often make the mistake of arguing for really stupid reasons. So I'd love a story to try to get at [[http://lesswrong.com/lw/wj/is_that_your_true_rejection/][true rejections]]. Perhaps someone doesn't want to do a thing because of Reason X, and someone points out that there is in fact a patch to Reason X, but then they still don't want to do the thing, and so what's really stopping you?
:PROPERTIES:
:Author: Charlie___
:Score: 2
:DateUnix: 1424880404.0
:END:

*** Awesome, very cool examples, I will definitely try to write on one of them! Thanks!)
:PROPERTIES:
:Author: raymestalez
:Score: 1
:DateUnix: 1424908797.0
:END:
