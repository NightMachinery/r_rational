#+TITLE: End Goals of Rationality

* End Goals of Rationality
:PROPERTIES:
:Author: gramineous
:Score: 26
:DateUnix: 1599142191.0
:DateShort: 2020-Sep-03
:FlairText: META
:END:
(This whole thingo has been sitting in my end a while now in some form or another, so this is more me getting it down finally rather than some particularly well-constructed argument)

​

So I've been reading quite a few different works that fall under the umbrella of being rational(-ish), and have been meaning to get more into works that are more strictly rationalist (I've been putting off properly reading Sequences for yonks now), and while I can get behind the overarching tone of the majority of work being about things getting better (either through making the world better, improving your thinking, or both), but it's the details of the endings that a lot of rational fiction settles on that seems a bit off to me.

Like, and I am going to post (simplified and watered down) *spoilers* for well-known works of fiction here to illustrate my point so be careful about what you read, a big part of HPMOR's ending is>! "as a result of my/our work/beliefs, I am going to start dolling out immortality to all,"!< The Waves Arisen has "I am going to use my power take over everything to unite everyone and improve all our lives (even if things are going to be a bit shittier in the short term)," Mother of Learning is less grand in "I am a better, kinder, and more powerful person and I am going to do my own things that simultaneously benefit myself, the people I care about, and the world around me." (It's been a while since I've read all those so don't get stuck up on details here).

Like these are all good (and dare I say, happy) endings, and I understand it is much more narratively suitable, generally enjoyable, and arguably relatable (we our live inside our own heads exclusively) to have the protagonist be the focus of the ending, but shouldn't the focus be somewhat broader in the endings? Looking at the world reacting to the events rather than a culmination of the character/s efforts from a first person view?

Like a lot of rational work talks about how /everyone/ can improve their thinking, what pitfalls and fallacies to avoid, what successful strategies to employ, learning from your mistakes and all that. Would it be more suited to the ideology behind rationalism if there was "epilogue: here's how they all lived happily ever after" and followed by "epilogue 2: here's how everything changed in the wider world." One first for the narrative to have a satisfying conclusion, one to reconnect the ideals expressed by the author to the reader themselves in a more explicit manner, talking about benefits beyond the individual. Although this isn't something I'm exactly qualified on and I don't know if there's something about such a bit of writing like this that makes it less enjoyable (it could easily be something done in rational fiction that I just haven't stumbled across because I've read more popular stuff than unpopular stuff).

But anyway, encouraging the spread of rationalism aside, the specific implementation of the ideals expressed by the endings of these works is something that seemed a little off to me. When so much of rationalism is based on dealing with our innate and/or learned flaws as people, the celebration of improving ourselves seems sub-optimal.

That is going to take some explaining (of something I'm not certain how to explain) and go a bit off-topic from this subreddit.

Firstly, being better than you were yesterday is good. Helping others to be better than they were yesterday is good. The world being better than it was yesterday is good. But it could be done better. If we're all starting off from a baseline of our current human limitations, with all its issues, how good can the "end product" be? The immediate next step in that train of thought is transhumanist ideas and all that jazz, but that's not exactly what I'm thinking of.

Why is humanity so front and stage in thoughts of the future? I don't mean in regards to talking about possible alien life, I mean why is humanity being in charge of everything something people see as set in stone? "Friendly AI" is something that is discussed (that I really need to read more discussions of), AI that helps humanity, but with the idea of an AI singularity being a thing that is being actively researched, AI that is (several times) smarter than humans seems a distinct possibility (even if AI growth is restricted to the exponential growth rate of Moore's Law or something similar).

I'm sure everyone reading this can think of several political leaders they think are absolutely horrible, so replacing folks with machines that are better than the brightest people isn't entirely unpalatable. There are obviously massive issues with picking a "fair" AI to stick in charge, how much bias people have in them that would go into making an AI in the first place, and a host of other issues I haven't even began to consider, but is it worth the risk? There's been so many atrocities through (recent) history, and many, many going on today still (treatment of Uyghurs in China, refugees and Australia, wars across the world, the rise of authoritarianism, etc.), as well as the persistent risk we either kill the planet with climate change in the long term or nuke ourselves to death or some other disaster that it at least deserves thought.

And even if we decide it's not worth the risk to create extremely powerful AI, there is the risk of someone else creating extremely powerful AI themselves but doing a worse job of it or being worse to hold it. I'm more inclined to think that somewhere like Finland having access to an incredibly powerful AI, due to the motivation of the potential military benefits that would bring (either directly or through successive AI-created developments) would be better than North Korea. The situation kind of turns into some weird Pascal's Wager type deal - do we take a chance on an incredible benefit of a powerful and benevolent AI making the world a better place moving forward at the risk of something going wrong and wiping us out?

Anyway, to relate back to the actual reason for posting in this subreddit - is the idea of (rather than directly making the world better) making something that itself makes the world better something that meshes with rationalism, either as it is defined here, as you relate to it yourself, or as it is reflected in some works that I am just not aware of?


** You seem to be using some hyper-specific definition of rational fiction. It's just characters acting rationally. That's literally all there is to it. They don't [[https://tvtropes.org/pmwiki/pmwiki.php/Main/IdiotBall][turn into idiots when it's convenient for the plot]], they don't [[https://tvtropes.org/pmwiki/pmwiki.php/Main/BatDeduction][magically deduce]] things that can't be deduced from actual evidence etc.
:PROPERTIES:
:Author: PcCultureIsFascist
:Score: 48
:DateUnix: 1599146137.0
:DateShort: 2020-Sep-03
:END:

*** /Being rational/ isn't exactly the same thing as /Rationality/, though there's significant overlap. I'd say there's a difference between 'rational fiction' and 'rationalist fiction' and it sounds to me like the OP is speaking about the latter while you're thinking about the former.
:PROPERTIES:
:Author: Asviloka
:Score: 16
:DateUnix: 1599177130.0
:DateShort: 2020-Sep-04
:END:


*** Upvoted merely for the idiot ball reference. TIL. But it was definitely something that's always bothered me about tv shows.
:PROPERTIES:
:Author: nosoupforyou
:Score: 6
:DateUnix: 1599162805.0
:DateShort: 2020-Sep-04
:END:


*** You're not wrong at all, it's just...

You know how "literally" has had its definition expanded to include it meaning the exact opposite of its original definition for the purposes of emphasis? It's got significantly less precedent to it's usage in such a manner, but came about through changes in the way it was used. I'm taking a pseudo-definition of rational fiction based on common driving themes in the body of mainstream rational fiction I've read because I'm personally inclined to taking the approach of defining language based on the way it is a part of a broader context. I could easily be wrong about how consistent these themes are in rational fiction as a whole and getting my definition wrong, and I'm probably more inclined to be accepting of such fluent definitions given I'm queer and a lot of language about identities in that whole community is more flexible/fluid/changing (hell, look at the change from "queer is a slur" to "queer is an identity" over the past few decades). Like there is no chance at all your definition is wrong, there is a chance my definition is wrong, but I think there is some justification to looking at definitions from my viewpoint even though I'm bias towards doing so.
:PROPERTIES:
:Author: gramineous
:Score: 10
:DateUnix: 1599151532.0
:DateShort: 2020-Sep-03
:END:

**** u/callmesalticidae:
#+begin_quote
  You know how "literally" has had its definition expanded to include it meaning the exact opposite of its original definition for the purposes of emphasis?
#+end_quote

There are very few places where you are more likely to find extreme disapproval for that trend than on this subreddit.
:PROPERTIES:
:Author: callmesalticidae
:Score: 18
:DateUnix: 1599181760.0
:DateShort: 2020-Sep-04
:END:


**** u/SimoneNonvelodico:
#+begin_quote
  I'm taking a pseudo-definition of rational fiction based on common driving themes in the body of mainstream rational fiction I've read because I'm personally inclined to taking the approach of defining language based on the way it is a part of a broader context.
#+end_quote

Usually here works that also focus a lot on spreading specific ideas about rationality are tagged not just as rational, but as rationalist. However yeah, I can see the point, you should just be a bit more specific I guess. Anime is not just about giant robots and magical girls, but if you're watching a giant robot or magical girl show it's almost certainly anime. Something like that. What you talk about is more like a trope that happens to be common in rational fiction (probably because it's been codified by early works like HPMOR, The Waves Arisen, etc.).
:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 2
:DateUnix: 1599203697.0
:DateShort: 2020-Sep-04
:END:


**** This is a bit of a tangent, but the use of 'literally' for emphasis has been around since at least 1769[[https://www.nationalgeographic.com/news/2013/8/1308016-words-literally-oxford-english-dictionary-linguistics-etymology/][[1]]], so it isn't exactly lacking in precedent.

(To be clear, I agree with the point you're making, I just couldn't resist the urge to nitpick)
:PROPERTIES:
:Author: OuroborosInc
:Score: 2
:DateUnix: 1599343146.0
:DateShort: 2020-Sep-06
:END:


** Well, the idea of rationalism is optimising for any goal of your choice - if something you make will achieve this goal more optimally than you yourself, then of course it's a solution worth going for.

#+begin_quote
  do we take a chance on an incredible benefit of a powerful and benevolent AI making the world a better place moving forward at the risk of something going wrong and wiping us out?
#+end_quote

I don't think this is even a question. Either we prove ourselves incapable of creating a powerful AI, or we wipe ourselves out before creating it, or someone eventually creates it. Taking or not taking a chance is not a possibility.
:PROPERTIES:
:Author: Transcendent_One
:Score: 29
:DateUnix: 1599142869.0
:DateShort: 2020-Sep-03
:END:


** Rationality can be applied to any goal including the genocide of an ethnic group.

Of course if your goal is actually stability of the nation or something rationality might help you recognise the bias of assuming that a certain group of people are more at fault than others but is rationality is not a political stance.
:PROPERTIES:
:Author: RMcD94
:Score: 15
:DateUnix: 1599145079.0
:DateShort: 2020-Sep-03
:END:


** u/ArgentStonecutter:
#+begin_quote
  why is humanity being in charge of everything something people see as set in stone?
#+end_quote

Counterpoints: Friendship is Optimal. The Culture series. In Greg Egan's futures the beings that are in charge of everything are often human-like in many ways but are not actually human any more. In Charlie Stross's /Saturn's Children/ and /Neptune's Brood/ humanity has been replaced by human-derived robot sex slaves. Karl Schroeder's Ventus and Virga series humanity is kind of living on the skirts of far smarter and more powerful beings, many of which are no longer self-aware because self-awareness is inefficient.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 13
:DateUnix: 1599167927.0
:DateShort: 2020-Sep-04
:END:

*** I'll look into those works (probably not the ponies though). Thank you.
:PROPERTIES:
:Author: gramineous
:Score: 1
:DateUnix: 1599190196.0
:DateShort: 2020-Sep-04
:END:

**** The ponies one isn't really about ponies, it's more about artificial superintelligences with weird goal functions. Ponies are a metaphor.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 6
:DateUnix: 1599237271.0
:DateShort: 2020-Sep-04
:END:


** I think the idea of that second epilogue would be somewhat dramatically unnecessary. We already /know/ what kind of future the protagonist envisions, and any deviation from that would be too much detail to contain in a single epilogue. It feels like wish-fulfilment at that point, because it adds nothing to the story.

In most of the stories you described, there are good reasons why Artificial Intelligence never occurred to the protagonist as a viable option. In The Waves Arisen and Mother of Learning, there's no in-world equivalent of AI. In HPMOR, I'm fairly certain Harry plans at some point to build a Friendly AI, but the problem is that even as far as magic's reality-bending powers go, creating new sentient beings that /aren't/ human-derivative would take a long time to work on, and a better idea might simply be to keep track of Muggle AI research, and find ways to improve it with magic, and return the results.

Apart from that, there is also the fact that the existence AI is kind of unsatisfying from a narrative standpoint. As in, instead of having a story about a protagonist who's trying to get smarter and save the world, you'd end up with an idealized autobiography of a real-world AI researcher (or of Eliezer or the like, if it's a world where AI exists in some form, and alignment is the issue).

The benefits to AI are more nuanced than simply sticking one in charge. That's something that would go deeply against public sentiment, and for the sake of democracy, would not be implemented. Powerful AI would exist to advance mathematics, technology, and social /systems/ that benefit the most people while catering to our sensibilities. A weird illustration that just popped into my head is the Yogurt episode from Love Death Robots, where the yogurt didn't ask at first to be in charge of society, but instead gave world leaders a plan that would end the world's problems.

In the real world, AI is inevitable. It's just the natural next stage of a scientific - or indeed, a sentient - civilization. Of course, the downside to creating AI without proper alignment systems is deeply horrifying, which is why given the choice, I'd delay AI research until AI alignment research reaches commensurate levels. But we /don't/ have that choice.
:PROPERTIES:
:Author: Jose1561
:Score: 8
:DateUnix: 1599143200.0
:DateShort: 2020-Sep-03
:END:


** Fully automated luxury gay space communism
:PROPERTIES:
:Author: SeraphimNoted
:Score: 9
:DateUnix: 1599187451.0
:DateShort: 2020-Sep-04
:END:


** u/PastafarianGames:
#+begin_quote
  Why is humanity so front and stage in thoughts of the future?
#+end_quote

Are you asking why SF/F speculative fiction is about humanity and the human experience / condition, or are you asking why rational fiction doesn't throw off the shackles of the SF/F genres and write about aliens and incomprehensibility?

#+begin_quote
  but shouldn't the focus be somewhat broader in the endings? Looking at the world reacting to the events rather than a culmination of the character/s efforts from a first person view?
#+end_quote

Sure, if that's what the story is about. But no, not if that's not what the story is about. Not every story is a polemic about how to build a better future, with a nice neat moral at the end about how the protagonist followed the polemical plan and made that better future (and in fact, almost all of those stories suck as fiction).
:PROPERTIES:
:Author: PastafarianGames
:Score: 5
:DateUnix: 1599175772.0
:DateShort: 2020-Sep-04
:END:

*** I guess I'm trying to get at the broader adherence to common world-building tropes in fiction, especially related to protagonists. How many stories are out there that sit in "simplified 14th century Europe but with magic" or something to that effect? How many Sherlock Holmes-esque "deductive genius with major social issues" protagonists are there in mainstream media over the past decade? Like making fiction from established cultural understanding makes sense, it allows the author to bring their differences into greater focus, it cuts down on how much you need to do to establish the world, and the popular tropes are popular for a reason. But, and I don't know how much of this is wishful thinking, lack of finding such works, or some level of bias I've got going on, shouldn't there be more stuff that violently breaks from such molds?

Jabberwocky is a departure from the narrative norms and world-building taken to a grand level, but is the middle-ish ground between that and what is popular just thoroughly unreadable or unwriteable for some reason I don't know? Is Jabberwocky only functional as a piece of fiction due to its incredibly short length? I guess I don't know how much incomprehensibility or obtuse-ness a work can have before it becomes completely intolerable to either the author or the reader.

​

Polemics about building a better future kinda seemed like the type of thing that'd end up on this subreddit. But given the other commenter in this thread talking about how Eliezer Yudkowsky trying to do that and stopping because the work was incredibly depressing, all those stories sucking as fiction makes sense. I hadn't read any myself, and I didn't know if it was because they did indeed suck or if there was something else at play.
:PROPERTIES:
:Author: gramineous
:Score: 3
:DateUnix: 1599190054.0
:DateShort: 2020-Sep-04
:END:

**** I think the main reason why most fiction doesn't break the mold of its tropes is that most fiction seeks to use tropes. Invoke them for emotional freight, invoke them to save space and time, lean on them because it's easier than generating everything whole cloth, they're useful!

There's a /lot/ of utopian science fiction out there. The vast majority of what I'd consider utopian is queer, and basically none of it is even remotely commercially successful, even stuff that's been published through traditional means. Basically the only really successful example here is the Culture books, and a lot of people consider them horribly dystopian.

There's also a lot of "blueprint for a better future" type science fiction out there. The vast, vast majority of it is implicit; it's written set in a society that has done the work, and explores the themes of "okay, what now". (I count in this category Lois McMaster Bujold's SF, Becky Chambers's books and in particular Record of a Spaceborn Few, again the entire Culture series, and basically anything that came out of the "fully automated luxury gay space communism" meme/notion, a lot of which you'd have to delve into Tumblr for or find the various Patreon pages of people who write stories in that oeuvre.)

I would recommend ignoring anything E.Y. writes on the matter of writing, and would recommend looking for stuff by more widely-successful authors. The aforementioned Banks, Bujold, and Chambers have all written works that serve as a sufficient refutation, though what prevents him from being able to do what they did is something I feel no need to speculate about.

But here's the key: good fiction about the utopian future doesn't tend to be about the path there, and it doesn't tend to be about technology or structures or strictures. Good fiction about the utopian future, just like all the rest of good sf/f, tends to be about people. Specific people, dealing with other specific people, as a reflection of what it means to be human. That's sort of what SF/F as a genre is about.

("What is the path to the utopian society" is more of a thing in non-fiction. I find reading it depressing because I don't believe that good will prevail, but it's certainly out there.)
:PROPERTIES:
:Author: PastafarianGames
:Score: 4
:DateUnix: 1599194069.0
:DateShort: 2020-Sep-04
:END:


** Most stories focus on particular characters because they carry narrative power much more easily than broader containers like "society" or "the world" do. You could certainly have more epilogues that end with zoomed out, top down views on how everything ends not just for the protagonists but for the world, but that sort of hampers the potential of more stories between the end of the one you're reading and the utopian singularity (or dystopian one) the same way the epilogue in the canon of Harry Potter sort of restricted the possibility of any post-Hogwarts story between the 7th year and then.
:PROPERTIES:
:Author: DaystarEld
:Score: 4
:DateUnix: 1599188252.0
:DateShort: 2020-Sep-04
:END:


** u/SimoneNonvelodico:
#+begin_quote
  I mean why is humanity being in charge of everything something people see as set in stone?
#+end_quote

Eh, no matter how you turned it around, "we are now in charge of some non-human entity that does better than we would" would sound a lot like dystopia to many people. And I'm not necessarily one of them, in the hypothetical scenario in which I could have a genuinely benevolent, wise ruling AI, I'd probably be better off, and with no less control than with some President or Parliament that is always overwhelmingly dominated by the party I did /not/ vote for anyway. But a lot of people highly value self-determination for its own sake, and you can't ignore that.

By the way, did you read Friendship is Optimal? That one's basically about a caretaker AI.
:PROPERTIES:
:Author: SimoneNonvelodico
:Score: 2
:DateUnix: 1599203560.0
:DateShort: 2020-Sep-04
:END:


** The reason why we do not have the Second Epilogue, is that it would either have to be blatant Mary Sue Land, or be realistically flawed, which would be a downer ending.

It is better to leave the final epilogue to reader's imagination.

Have you read Transhumanist Wager? Or Micro Gates? Or Atlas Shrugged? All these stories deal with the long term outcomes of transformative rationalism, and in both cases billions die in the inevitable 'birthing" process of the new rationalist era. This makes these books pretty off-putting in the end, because while the rationalist protagonists manage to build the foundations of utopias, they are built on bones of billions of irrationalists who killed themselves in a desperate frenzy to stop the optimization.
:PROPERTIES:
:Author: Freevoulous
:Score: 2
:DateUnix: 1599207951.0
:DateShort: 2020-Sep-04
:END:


** u/Bowbreaker:
#+begin_quote
  but shouldn't the focus be somewhat broader in the endings? Looking at the world reacting to the events rather than a culmination of the character/s efforts from a first person view?
#+end_quote

HPMoR had epilogues regarding the futures of at least two side characters not directly bound to Harry as well. Draco and Snape

#+begin_quote
  "epilogue 2: here's how everything changed in the wider world." One first for the narrative to have a satisfying conclusion, one to reconnect the ideals expressed by the author to the reader themselves in a more explicit manner, talking about benefits beyond the individual.
#+end_quote

Luminosity and The World As It Appears To Be both have this. And they are both worth reading in my opinion.

#+begin_quote
  I mean why is humanity being in charge of everything something people see as set in stone? "Friendly AI" is something that is discussed (that I really need to read more discussions of), AI that helps humanity, but with the idea of an AI singularity being a thing that is being actively researched, AI that is (several times) smarter than humans seems a distinct possibility
#+end_quote

There definitely is rational(ist/ish) fiction that ends up with a good AI. But you can't end all stories the same way, else it becomes unoriginal.
:PROPERTIES:
:Author: Bowbreaker
:Score: 2
:DateUnix: 1599295621.0
:DateShort: 2020-Sep-05
:END:


** I could also wax poetic about some implications of AI leadership, but felt it'd detract from the (already scatterbrained) thread too much.

Like how if AI wipes out humans, does it matter that much? If our descendants aren't (our) flesh and blood, does that really matter? How fleshy and bloody do our descendants have to be anyway? Like if you don't have kids of your own directly, do you write off everyone else's kids? Does every species that predated our current species in the long history of evolution not count as our ancestors? What distinctions are you making between whether or not something is classed as a descendant?

Also, kinda what I was getting at with shoving "Pascal's Wager" in there, but how does this relate to the definition of "God"? If we make some ultimate arbiter by our own hand rather than by whatever scriptures a bunch of folks subscribe to, does that make the endeavor greater or lesser by association? Religion has been a popular sentiment across the world for millennia, even if I doubt that people reading this are too enamored with the idea themselves (and myself for that matter).

I don't know the answers to these questions. I'm posting this whole thread here because I don't know a lot of things and my head runs in circles thinking about these topics. Comments and thoughts and any resources on the topic very much appreciated.
:PROPERTIES:
:Author: gramineous
:Score: 3
:DateUnix: 1599142756.0
:DateShort: 2020-Sep-03
:END:

*** When people talk about AI wiping out humans, we're talking of wiping out all sentient beings in this tiny section of the universe. Something like "wiping out" humanity by evolving us into a new race isn't the kind of philosophical issue I've seen come up in serious discussion (Then again, I could just not have had enough experience there), so I think most people agree with you on it being a non-issue (Of course, this also implies that /we'd/ also be non-humans, not just our descendants, because biological enhancements are a thing.

I don't understand your second point about God. Could you explain that a bit further?
:PROPERTIES:
:Author: Jose1561
:Score: 11
:DateUnix: 1599143499.0
:DateShort: 2020-Sep-03
:END:

**** There's some discussion of that in Toby Ord's /Thr Precipice/ (non fiction intro to existential risks), but in most conversations it just doesn't matter much - we already have essentially the same value drift problem just from having children, so...
:PROPERTIES:
:Author: PeridexisErrant
:Score: 2
:DateUnix: 1599175552.0
:DateShort: 2020-Sep-04
:END:


**** Eh, evolving into a new race could easily be an extension of Ship of Theseus arguments. It's not something I've looked into too hard, I'm sure there's arguments from people opposed to organ donation taking that tack, I don't remember when I looked into that stuff a while back on a whim, but it's not something that I am majorly interested in. It could be because I am far enough removed from the conversations that I don't know interesting debates are happening though.

Uh, it's more that God, in broad strokes from my only vaguely-sorta-not-really Christian-adjacent upbringing can be construed as the whole "Big guy in the sky looking down and moving things around imperceptibly to keep us chugging along and avoiding disaster." You could, once you got the whole AI singularity thing online, potentially have it decide that is the approach it would take to helping humanity out. It probably wouldn't, but the potential of an AI overseer/arbiter/whatever having the effective powers one would attribute to a modern day interaction with a God is the parallel I was drawing. Not the unknowable/unprovable afterlife stuff, or the timescale of supposed influenced from the dawn of creation, or the creating of prophets or holy scriptures. Just the stuff that a modern day believer would attribute to the designs of deity and not be able to be conclusively disproved over (unless they're overeager to ascribe things to His power, and likely won't consider any arguments about).
:PROPERTIES:
:Author: gramineous
:Score: -1
:DateUnix: 1599149954.0
:DateShort: 2020-Sep-03
:END:

***** The Ship of Theseus argument would be valid if we were talking about slowly evolving the cognitive nature of our /minds/. We are not our body, and changing the vessel our body inhabits is no different than changing the clothes that vessel wears.

#+begin_quote
  people opposed to organ donation
#+end_quote

I don't mean to be snide, but I did say serious discussion about transhumanism. As far as I know, that's a pretty regressive opinion (If I'm wrong, feel free to correct me, obviously).

Well yes, a super-intelligence is as close to God as we'll ever likely come in real life. It /really/ depends on your definition of the term, however, and I suspect most religious people (at least, every religious person I know) would not consider an AI God. I don't think the name has much use otherwise, because we use these terms to better label things, and if we're capable of building a super-intelligence, somehow I doubt categorizing would be one of our issues.
:PROPERTIES:
:Author: Jose1561
:Score: 9
:DateUnix: 1599152303.0
:DateShort: 2020-Sep-03
:END:

****** Bit of a disconnect here on "serious discussion." You're taking it as "discussion worth taking seriously" (the categorisation of the current topics here I agree with though), I'm taking it as "discussion the people involved with are having seriously." Like if a genuinely misguided racist sits down and tries to argue their specific brand of bullshit (not just spewing hate one-sidedly), it could be done with the intent of being a serious discussion, even if everyone who'd end up in this thread/subreddit wouldn't take any of their points seriously.

This kinda leads into allegories to God, the more bible-thumping-y folks call things like abortion and organ donation "playing God," people which I think the both of us have actively avoided throughout our personal acquaintances, but their aim is looking at the things within the realm of possibility, putting it on a spectrum, then drawing a line between what they think we should and should not do. I was doing a poor job of conveying it, but I was trying to get at the idea that a large chunk of people (not just the hyper bible-thumpers again, the more progressive/middle-ground-ish folks who only loosely stick a religion too) would be vehemently opposed to the idea of elevating something to the levels a super-intelligent AI could reach. If there is something that, considering the spread of religion, the majority of the population would have somewhere from misgivings about to definite strong opposition towards, should we consider this opinion?

That question itself can be broadly made into "Do we allow people to make decisions that make their own lives worse?" and is easily applied when talking about the laws and broader legality of things like alcohol and gambling, and the general balancing act between freedom and well-being. Also the whole "we are an interconnected society and how much responsibility to we bear for living a good life?" as the next argument from that. These questions are a hell of a lot broader in scope than what this thread started from though.
:PROPERTIES:
:Author: gramineous
:Score: 3
:DateUnix: 1599186783.0
:DateShort: 2020-Sep-04
:END:

******* A religious person would not consider a super-intelligent AI as God, I think. Considering that the more public definitions of 'intelligence' keep changing depending on whatever modern AI /can't/ do, I doubt that's likely to change. Already we have AI sufficiently capable of writing stories, and people aren't crying for its dismantling. Even when we reach the stage where AI can create new mathematical theorems on its own (A level where it can affect more change than nearly anything else), those people would likely not care about it in that way.

I do think people should be allowed to take decisions that make their own lives worse, but with a pretty big caveat to factor in the fact that many times, people simply aren't capable of making those shots. I'm not talking about intellectual capacity here, I'm talking about addicts who don't really have a choice about their own lives. There's a reason sex when one party is inebriated is counted as rape, and I agree with the logic.

Apart from that, I think a far more effective route in preventing the kind of problems that arise from letting people make their own decisions otherwise, is simply to make them smarter. The people on this subreddit likely weren't /born/ smart, and even if some were, that doesn't change the fact that this sort of intelligence is about Bayesian thinking and overcoming bias, which is mostly a matter of pedagogy.
:PROPERTIES:
:Author: Jose1561
:Score: 1
:DateUnix: 1599202836.0
:DateShort: 2020-Sep-04
:END:


*** If some descendants of ours survive who aren't exactly human but share some fundamental values with us (e.g., they know what fun is and what boredom is, and they are conscious), then I think we could be very happy. However, that's certainly not the default outcome. The default outcome is that we all die and are replaced by something that, to us, looks /mindless/ (it might still be intelligent in the sense that it can outsmart any human, but it lacks any concept of fun or love or friendship or honor or anything like that). For example, a [[https://wiki.lesswrong.com/wiki/Paperclip_maximizer][paperclip maximizer]].

All these futures that you are considering are in this tiny tiny part of possibility space. The vast majority of possibility space is a universe that is "dead" forever. That's the thing we should prevent.

#+begin_quote
  Would it be more suited to the ideology behind rationalism if there was "epilogue: here's how they all lived happily ever after" and followed by "epilogue 2: here's how everything changed in the wider world."
#+end_quote

Eliezer Yudkowsky (of HPMOR fame) once tried to write a story set in a post-singularity world where our world had been /fixed/, [[https://www.lesswrong.com/posts/88BpRQah9c2GWY3En/seduced-by-imagination][but he couldn't finish it]]:

#+begin_quote
  [...] It's not a good idea to dwell much /on/ imagined pleasant futures, since you can't actually dwell /in/ them. It can suck the emotional energy out of your actual, current, ongoing life.

  [...]

  I am now explaining why you shouldn't apply this knowledge to invent an extremely seductive Utopia and write stories set there. That may suck out your soul like an emotional vacuum cleaner.
#+end_quote
:PROPERTIES:
:Author: thomas_m_k
:Score: 4
:DateUnix: 1599162410.0
:DateShort: 2020-Sep-04
:END:

**** Ok, yep, makes sense. I guess there's a big difference in sitting down and loosely plotting out a better world than the one we're in, and taking care and effort to construct and convey a thoroughly planned idea to others. I often get stuck in a mental rut trying myself thinking of that broad utopia topic, but chronic depression kinda normalizes thinking things that make you feel varying degrees of shitty and I've been like this for literally the majority of my life so I don't know how it is on the other side of the fence so to speak. Like when so much of your thoughts lead you down that path anyway, whether or not to avoid a single specific instance of that pattern isn't typically worth considering.
:PROPERTIES:
:Author: gramineous
:Score: 2
:DateUnix: 1599187260.0
:DateShort: 2020-Sep-04
:END:


** u/ConscientiousPath:
#+begin_quote
  shouldn't the focus be somewhat broader in the endings? Looking at the world reacting to the events rather than a culmination of the character/s efforts from a first person view?
#+end_quote

I don't think so, no. The ideal of rationality is to achieve your goals through perfection of awareness, improved speed and accuracy in predictions, improved quality of the choices you think to make, and elite control of self to consistently take advantage of all those things. It doesn't dictate what your goals /are/, and doesn't promise more change to the world than is possible for you to effect. Therefore it doesn't promise that the broader world will change or react significantly.

The ideal of rationality is something that's very hard to make progress towards in real life because there's a large gap between knowing scientific findings and statistics, and using them to /correctly/ implement significant behavioral changes that don't backfire, burn you out, over-correct, make you miserable, just straight up prove impossible to do. If it weren't extremely hard to do, especially on your own, CFAR wouldn't have a business model.

Rationalist non-fiction often tries to extrapolate what a world where rational people lived might look like because people often see politics as a place that especially needs more rationality, and imagining futures is part of coming up with policy. I think that for most people this is jumping the gun because you're unlikely to have a positive effect when attempting enormous tasks like improving an institution if you can't manage simpler things in your personal life. People with messy rooms shouldn't be trying to clean up government. But I'm getting off topic...

Rationalist fiction, as opposed to rationalist non-fiction like the Sequences, is in many ways the art of writing enjoyable Mary Sue characters. The stories are often about characters who are far more industrious than anyone willing to stop working long enough to find and read these books. MC self control is often far above average, their attempts at deduction turn out to be correct, and when they're unable to control their emotions the problem is limited in duration and/or scale enough that they aren't prevented from reaching their goals. The whole genre is basically "stories by people who are smart enough to write characters with a high wisdom stat, for people who are smart enough to care about plot holes."

#+begin_quote
  Why is humanity so front and stage in thoughts of the future? I don't mean in regards to talking about possible alien life, I mean why is humanity being in charge of everything something people see as set in stone? "Friendly AI" is something that is discussed (that I really need to read more discussions of), AI that helps humanity, but with the idea of an AI singularity being a thing that is being actively researched, AI that is (several times) smarter than humans seems a distinct possibility (even if AI growth is restricted to the exponential growth rate of Moore's Law or something similar).
#+end_quote

The whole concept of the Singularity is partly that we can't predict what would happen afterwards. Many authors aren't smart enough to write smart characters well because brains aren't capable of simulating anyone smarter than themselves. Similarly if/when GAI exists and has greater than human level intelligence, no current human can effectively write about what that will be like because we aren't smart enough to know how a thing greatly smarter than us will act outside of generalities like "it will be very good at learning to achieve its goals".

Part of the reason too is that not all rationalist stories are told within a world that matches our current state of technology. Given the difficulty of writing AI in well, and the singularity's tendency to take over an entire setting, it's easier to just leave it out.

#+begin_quote
  Anyway, to relate back to the actual reason for posting in this subreddit - is the idea of (rather than directly making the world better) making something that itself makes the world better something that meshes with rationalism, either as it is defined here, as you relate to it yourself, or as it is reflected in some works that I am just not aware of?
#+end_quote

Making the world better on a large scale (directly, indirectly, or with positive feedback loops) isn't precluded in the genres in this sub, and some authors want to include that plot point in their narrative, but IMO it isn't a required component of any of them. "Rational" is mostly just "the plot doesn't have huge obvious holes or characters making decisions that they wouldn't if they were actual humans." "Rationalist" is mostly just "explicitly uses some cogsci, in a story about how a smart person with good self control wins by being smart".
:PROPERTIES:
:Author: ConscientiousPath
:Score: 1
:DateUnix: 1599186838.0
:DateShort: 2020-Sep-04
:END:
