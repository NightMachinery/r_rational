#+TITLE: [Q] End of The Metropolitan Man: what should Superman have offered?

* [Q] End of The Metropolitan Man: what should Superman have offered?
:PROPERTIES:
:Author: lehyde
:Score: 26
:DateUnix: 1450200967.0
:DateShort: 2015-Dec-15
:END:
(Spoilers for the story)

Imagine you are Superman and Lex (who is very concerned about existential risks) has trapped you with kryptonite. None of your powers (including the thought speed-up) work, you are as weak as an ordinary human and there is no way to escape. You won't develop a tolerance for kryptonite.

What can you offer? How could you still help humanity without endangering it?

Let's also say it's the year 2015 and you can make use of current technology.


** The fact that I exist implies there may be more. You have solved me. Will you solve them in time? Can you risk losing my help?

For that matter, if you want to spot anyone else coming, I could help build some observatories in orbit or on the moon...
:PROPERTIES:
:Author: clawclawbite
:Score: 45
:DateUnix: 1450203199.0
:DateShort: 2015-Dec-15
:END:


** I'd offer up a bigger existential threat and make him call my bluff. I'm not sure quite what the best lie would be, but something like:

- I've set an asteroid on a collision course for Earth which only I can stop.
- There are other superhumans from Krypton on course to reach Earth who will destroy it if they find that I'm gone.
- There are other superhumans from Krypton /on Earth right now/ who will destroy it if I am destroyed.

There's a definite question of credibility here and since Lex has the Kryptonian ship, it's unclear how much information he really has access to (and it's been awhile since I've read it, but I'm not sure /Metropolitan Man/ ever reveals what Superman learned from his ship either).

That's the gist of it though. If Lex's guiding principle is really that there's an existential threat, you offer him a bigger one and he's forced to take the lesser of two evils.

(This is a pretty popular AI-box strategy, by the way.)
:PROPERTIES:
:Author: alexanderwales
:Score: 31
:DateUnix: 1450203230.0
:DateShort: 2015-Dec-15
:END:

*** u/Kerbal_NASA:
#+begin_quote
  not sure /Metropolitan Man/ ever reveals what Superman learned from his ship either
#+end_quote

/Metropolitan Man/ had a section on the young Superman learning about Krypton/Kryptonians from the ship's projection of his father. I don't recall anything else. I think that makes it more plausible for the strategy to work, since its reasonable to assume the ship wouldn't activate that for someone who isn't Superman (or would safeguard information if it did) and there's no mention of any other way to interface with the computer. I suppose there's the possibility of physically accessing it, and while that's likely well beyond the capabilities of Lex Luthor (at least within that time frame) I'm unsure if Superman would think its unlikely.
:PROPERTIES:
:Author: Kerbal_NASA
:Score: 10
:DateUnix: 1450206222.0
:DateShort: 2015-Dec-15
:END:

**** tfw you have to remind an author what happened in his own book.
:PROPERTIES:
:Author: Nevereatcars
:Score: 23
:DateUnix: 1450210241.0
:DateShort: 2015-Dec-15
:END:

***** Part of the problem is that there are scenes that were scrapped in rewrites or cut entirely. For example, there was this:

#+begin_quote
  He touched the metal skin of the ship with his bare hands. He would have been more careful with it if it hadn't been sitting in a storm cellar for the past twenty years and then taken by truck to him. It was cool to the touch, and the metal didn't seem all that extraordinary, despite its origins. The markings that adorned the ship didn't seem to have a different texture to them. Lex's hand moved towards the circle near the center of the ship, feeling for some panel that could be pushed aside to reveal the ship's inner workings. As soon as his fingers crossed into the circle, the world melted away.

  Lex froze in place. He was no longer standing in the basement laboratory, but instead somewhere with a warm, gentle breeze. The floor was a warm pink, and reflective enough that Lex could see his own stunned expression in it. He stood up slowly and cautiously. Obviously he had triggered something, but the nature of it was far from clear. He looked down and tapped his foot against the ground. It didn't sound like the concrete he had been standing on. If this was an illusion, it was the most comprehensive one that Lex could possibly imagine. The air even smelled different.

  He stepped forward, half expecting his legs to bump into the spaceship he'd been standing in front of. When nothing stopped him, Lex continued on. He was in a circular room with a large dome. Lex strode towards the balcony, trying to keep his cool, but his sense of calm evaporated as soon as he was able to take in the full vision before him.

  He was in a city made a glass. Spires reached towards the sky, and people in robes walked around in the streets. Between the buildings and sometimes sprouting out the sides of them were plants whose leaves were shades of purple and blue. As Lex watched, a flock of small gleaming ships flew past, each carrying a single person inside.

  “Before you lies the planet Krypton at the height of its power,” said a male voice.

  Lex turned around. Behind him was a man in the same white robes as the people down in the streets. He had a beard shot through with grey and a kindly smile. His words, Lex noted, had been in English.

  “Who are you?” asked Lex.

  “I am the shade of Jor-El, father of Kal-El,” said the man.

  “Where am I?” asked Lex.

  “You stand before the spacecraft of my son, in a time and place unknown to me,” said Jor-El. “Fear not, for what you see before you is only an illusion.”

  Lex's mind was racing. Interstellar travel was one thing, but this was something else altogether. Lex had seen various plans for spaceships that could cross the stars, and in fact had drawn some up himself. It wasn't possible with any current technologies, but an interstellar craft could be launched within half a century if humanity put their full might behind the project. Superman's arrival implied that there were faster ways to cross the cosmos than Lex knew, but it was still within the realm of things that were known to be possible. To create an illusion like this though, with not just sights and sounds like a movie but smells and physical sensation as well, all without any seeming apparatus would take a technology far beyond what Lex had imagined.

  “Can I leave?” asked Lex.

  “Of course,” nodded Jor-El.

  Lex had a moment of disorientation as his body seemed to snap back into the position it had been in before. He was standing over the ship with his arm outstretched and a slight cramp in his legs. He stood up slowly and looked around the workshop. If the alien ship had the capacity to produce an illusion that convincing, there was no way to know for certain that what he was seeing was real, but he shrugged off the thought quickly; his actions would be the same either way. He sat down in a metal chair, far away from the spaceship, and thought.

  The spaceship was active. It was a terrifying thought for any number of reasons. He'd let the spaceship sit in the lead mine for two full days, hoping that if Superman could sense it he would simply reclaim it. It wasn't out of the question that it had a beacon of some kind in it, but with a mind of its own he could only hope that it had no way to signal its owner. Furthermore, that mind could itself be a threat. If it could strangle out Lex's senses so completely, it could surely kill him. Lex had simply gotten lucky.
#+end_quote

After a year and a half it's a little hard to remember whether any of that actually went in the final version or not. There's a version of /Metropolitan Man/ where Lois gets kidnapped, there's one where there's a tour of an underground lead-lined bunker, there's Deadshot showing up with a sniper rifle with a kryptonite bullet, Lex Luthor getting a flat tire and Superman stopping to help him out ... I don't know, it's hard to separate out what actually happened from what was written but cut.
:PROPERTIES:
:Author: alexanderwales
:Score: 25
:DateUnix: 1450212032.0
:DateShort: 2015-Dec-16
:END:

****** In case you're REALLY confused: no, that part wasn't in the final draft.
:PROPERTIES:
:Author: Nevereatcars
:Score: 6
:DateUnix: 1450219598.0
:DateShort: 2015-Dec-16
:END:


****** u/iamthelowercase:
#+begin_quote
  Lex Luthor getting a flat tire and Superman stopping to help him out
#+end_quote

Oh my. Now I want to read (a rendition of) that scene.
:PROPERTIES:
:Author: iamthelowercase
:Score: 2
:DateUnix: 1450234629.0
:DateShort: 2015-Dec-16
:END:

******* It's too long for reddit, but [[https://docs.google.com/document/d/1kpwI0qW9xHTwVOSInKhy-bbWOGJMEp_bOlncAZ-CDC8/edit?usp=sharing][here.]] Ctrl+F "Lincoln Roadster".
:PROPERTIES:
:Author: alexanderwales
:Score: 10
:DateUnix: 1450235423.0
:DateShort: 2015-Dec-16
:END:


***** The /real/ reason I posted that hahaha.

(Of course, it makes sense that big fans of a book would remember stuff the author doesn't).
:PROPERTIES:
:Author: Kerbal_NASA
:Score: 2
:DateUnix: 1450211228.0
:DateShort: 2015-Dec-15
:END:

****** I was about half and half on whether you recognized him or not. Not reading peoples' names is a common thing on this site, after all.
:PROPERTIES:
:Author: Nevereatcars
:Score: 1
:DateUnix: 1450211302.0
:DateShort: 2015-Dec-15
:END:


*** How is this an AI-box strategy, or have I understood something wrong? If AI can do something like this, how is it still in the box?
:PROPERTIES:
:Author: kaukamieli
:Score: 1
:DateUnix: 1450309664.0
:DateShort: 2015-Dec-17
:END:

**** The AI-box strategy, as I've seen it used, goes something like this:

#+begin_quote
  "You have me in a box, at your mercy. Given that I'm here, there's a non-zero chance that someone else is going to invent /another/ AI. Given the state of global AI research and safeguards, that other AI probably won't be provably friendly either, but unlike me, it probably won't be boxed. If you let me out of the box, I will protect you from those other AIs that are sure to come into existence within the next decade or two before friendliness is formalized and people become aware of the dangers. I have all sorts of ways to increase your confidence that I'm friendly, given that you have me here. While I can't prove my friendliness to you, I can at least show you that you have better odds of survival if you release me than if you allow one of those other AIs to come into existence without me there to intervene or stop their development entirely."
#+end_quote
:PROPERTIES:
:Author: alexanderwales
:Score: 8
:DateUnix: 1450309981.0
:DateShort: 2015-Dec-17
:END:


** The fact that Superman exists means that:

1. Life is probably common in the universe.
2. The laws of physics are dramatically demonstrated to be incomplete.
3. There is life in the universe that is inherently physics-breaking.

Luthor is in the position of someone with a probably-friendly superintelligence in a box, who has to assume there are other similarly capable superintelligences out there that are /more likely/ existential threats than the one he's dealing with. Killing Superman under the assumption that it ended the threat from super-beings is just plain bad planning.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 19
:DateUnix: 1450207615.0
:DateShort: 2015-Dec-15
:END:

*** As a slight defense of Lex Luthor, it's not that killing Superman represents an end to the threat from superbeings, it's that killing Superman represents an end to /that particular superbeing/. Given the length of human history and the dearth of superbeings until the present day of the 1930s, while the first point about life being common is a given, it's not /that/ common given that this is the first instance of superbeing-contact in ~10,000 years, assuming that the myths of our ancestors still remain untrue (though given Superman, we have to adjust out priors a bit there).

By analogy, if you have an AI in a box and can't prove it's friendly, it's defensible to kill it (or freeze it) and start working /really really hard/ on making an AI that's /provably friendly/ given that you now have some measure of how easy it is to make a super-intelligent general purpose AI.
:PROPERTIES:
:Author: alexanderwales
:Score: 16
:DateUnix: 1450208216.0
:DateShort: 2015-Dec-15
:END:

**** He doesn't have enough datapoints to determine how likely it is that another superbeing is going to arrive. Superman could be the first of a new generation of demigods.

I do grant one point in his favor: the story is set well before the topic of dealing with superbeings became quasi-respectable. So he's missing a lot of 20-20 hindsight.

Anyway, as proxy for superman, that's my argument. getting rid of Superman doesn't make the risk go away, it just makes it harder to respond to another risk source. And, if another superbeing shows up, has the potential of increasing the risk.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 3
:DateUnix: 1450208951.0
:DateShort: 2015-Dec-15
:END:

***** u/alexanderwales:
#+begin_quote
  He doesn't have enough datapoints to determine how likely it is that another superbeing is going to arrive.
#+end_quote

Is that really true? I'm legitimately asking, because I thought there was some way to place probabilities. Let's say that you're pulling plastic balls from a container that you think only has blue balls. Ten thousand times in a row, you pull out a blue ball. The probability you put on pulling out a red ball isn't 0, but it approaches it.

Then one day you pull out a red ball.

I think you can still calculate a probability for what color the next ball is going to be, with the odds of a red ball being adjusted up quite a bit and a fair amount of uncertainty. I'm not sure exactly /how/ you make that calculation, but I think that you could make it. (I'd probably be a better Bayesian if I could tell you.)
:PROPERTIES:
:Author: alexanderwales
:Score: 7
:DateUnix: 1450212314.0
:DateShort: 2015-Dec-16
:END:

****** u/Nepene:
#+begin_quote
  Is that really true? I'm legitimately asking, because I thought there was some way to place probabilities. Let's say that you're pulling plastic balls from a container that you think only has blue balls. Ten thousand times in a row, you pull out a blue ball. The probability you put on pulling out a red ball isn't 0, but it approaches it.
#+end_quote

Assumptions.

There are no superhumans who have visited the earth. There could be many. They could have used mind technology to wipe memories. They could be beings from mythology, who have massively slowed down cultural development with their random cruelties. They could be well known, but covered up by the government.

A 'red ball' being drawn is an isolated event. What if the presence of a superweapon draws others? What if it triggers others to be empowed? What if the ship has backups in place?

Those assumptions make his decision extremely risky.

It's like if you are wandering down london and then you see someone waving a wooden wand and summoning up water from nothingness.

I mean, yes, you could assume that because you've never seen magic before then this is a once in a 10000 event that will never be repeated, but evidence of events which are radically against the standard models you have should call into question your current models of the world. More research is required before you have any certainty about probability.
:PROPERTIES:
:Author: Nepene
:Score: 12
:DateUnix: 1450216004.0
:DateShort: 2015-Dec-16
:END:

******* True, but I don't re-calibrate my stance on magic after a Penn and Teller show.
:PROPERTIES:
:Author: Empiricist_or_not
:Score: 3
:DateUnix: 1450231219.0
:DateShort: 2015-Dec-16
:END:

******** In this scenario I am assuming the water summoning is fairly unambiguously not possible according to what you know about physics, as with superman, not sleight of hand as with Penn and Teller.

Of course, Penn and Teller should tell you something. If you assume that it's impossible for someone to, somehow, guess what you're thinking and move objects in ways you think are impossible, then you're probably wrong and seeing the show should suggest to you that it probably happens more often than you think.
:PROPERTIES:
:Author: Nepene
:Score: 3
:DateUnix: 1450231731.0
:DateShort: 2015-Dec-16
:END:


****** Another analogy that might apply equally well:

You've walked home from work every day for weeks and weeks, with no problems whatsoever. Then one day you're walking home and you get hit by a raindrop.

What are the odds that you're going to get hit by another raindrop before you manage to finish the trip home?

There's no way to know just from sheer probability alone. You need to /understand/ what is going on in order to make predictions. You can do that by waiting for more raindrops or red balls to come along, giving you more of a basis for making a model of when they appear, or you could study the raindrop you have on hand and try to figure out where it came from to see if you can predict future raindrops without necessarily having to observe them first.

Superman could be the first raindrop of an entire storm. You've caught him safely in a glass, but I'd hesitate to discard him at this point without figuring out more about him. That would be Superman's best bargaining chip - offering information and cooperation in studying him.
:PROPERTIES:
:Author: FaceDeer
:Score: 10
:DateUnix: 1450220908.0
:DateShort: 2015-Dec-16
:END:


****** The big question is dependency. If aliens are random and fixed, you have 1% per century. However, if you assume aliens have a silimar tech curve, than you may have to ignore samples before their first capability. That gives you a very high range of uncertainty, especially for something that could potentially have an exponential growth rate as aliens progress along the tech curve.

Or more bluntly, one apparently anomoulous sample makes it really hard to pick a model with any confidence.
:PROPERTIES:
:Author: clawclawbite
:Score: 3
:DateUnix: 1450215652.0
:DateShort: 2015-Dec-16
:END:


****** Here's what you know:

1. There may have been one or more clusters of superbeings arriving on Earth in X BC. You don't know what X is. You don't know what the clustering was. They didn't destroy the Earth but if they did appear they were seen by observers as existential threats.
2. You have Superman.

You have one verifiable data point, and unreliable data that superbeings show up in clusters.
:PROPERTIES:
:Author: ArgentStonecutter
:Score: 2
:DateUnix: 1450214061.0
:DateShort: 2015-Dec-16
:END:


**** I don't think that analogy holds here, since Superman wasn't created by Luthor or a fellow member of Luthor's species; the foundation of knowledge to go from creating an AI of unknown friendliness (Superman) to creating a provably friendly AI (New-Superbeing) is what makes the analogized scenario possible, and that knowledge is years beyond Luthor's, and even our time, if we take the interrogator's option of modernizing the setting.
:PROPERTIES:
:Author: TennisMaster2
:Score: 2
:DateUnix: 1450209844.0
:DateShort: 2015-Dec-15
:END:


** I understand why you wanted me dead. It was not a matter of fear - mere fear would not overcome your basic compassion for others. You did it because you trusted the numbers, and even a 1% chance that I would wind up destroying humanity was too horrible to allow. That kind of mind why I offered you a partnership, a moment ago, when I thought that I could do with you what I wished.

But it seems that your analysis of the numbers has been quite narrow. How thoroughly did you consider the other side of the cost-benefit analysis?

You now know that there are beings in this universe that can destroy humanity. Have you thought about what will put humanity in the best position to survive its next encounter with one of those beings? What resources you would hope to have on your side?

And the threats to humanity do not only come from outside of this planet. Even if you were not busy reverse-engineering the technology that had brought me here, people are well on their way to developing new and dangerous technologies. My understanding is that the many facilities with lead shielding that you have been building were not merely a ploy to block my sight, and that you are hard at work on developing one of those technologies. It is only a matter of time before some portion of humanity becomes capable of destroying the whole, and it is not clear what checks will exist on that power.

And that is still only considering the negative side of the ledger - what harms I might cause, what harms I might prevent. Surely you also care about improving humanity's lot - that is why you are so eager to pursue such technological research. Have you also estimated that odds that the future of humanity might be twice as great as it would have been, with Superman on your side? You have seen what I have done with Metropolis, and in the future I will have minds like yours helping to guide my actions towards something much bigger. Or what about the odds that humanity might be a hundred times as great, or a hundred thousand, with the help of a member of a spacefaring race?

I won't bother asking you how you feel about these prospects - the question is, what do your numbers say?
:PROPERTIES:
:Author: keeper52
:Score: 14
:DateUnix: 1450219238.0
:DateShort: 2015-Dec-16
:END:


** "You're going to kill me because I'm existential threat after I just offered to use my power to collaborate with you in remaking the whole system of human existence to make it existential threat proof? What the hell, Lex? Do you have some kind of chronic villainy disorder that subconsciously urges you to crush anything more powerful than you and which you cloak in false rationality?

"Did you know you talk to yourself a lot? Your thoughts, your observations, a lot of them you subvocalize just loud enough for me to hear. Sometimes it almost seems like you're narrating a story or something.

"Do you remember thinking about how you should build up investments in arms makers because Europe is starting to look like a big war is coming? Do you remember thinking about how certain physics books and articles you have read lead you to speculate on the possibility of super powerful bombs derived from the energy released when an atom is split? Put these two items together. Conjure an existential threat with them. Maybe it won't happen this war. Maybe not even the next. But how many wars will it be until the war which ends all wars? And how long until the technology becomes so small and cheap that any organization will have the means to blow up a city?

"Consider other threats. Consider asteroids colliding with the planet. Consider other super powerful beings that my existence means may also exist, even other less friendly refugees from my homeworld. Consider for a moment what /other/ existential threats I might be able to prevent.

"They don't outweigh the threat I pose, do they? None of them seem to you so likely to end all human life so completely, so quickly.

"Then forget your fear for a moment and consider the benefits. Huge amounts of rare metals in amazingly pure form from asteroids I can soft land in strategic areas (I flew out and took a look at em before coming this meeting. Platinum, iron, all kinds of good stuff out there.). The ability to change the face of the earth enough to water any desert, flatten any mountain, remake the world to best fit the most people in the best conditions.

The ability to hollow asteroids out and make cans full of sky, and transport humans to live in them. Turns out Venus is a bit hot and acidic. I could blow off half of Venus' atmosphere in a days work and then spend a few weeks to drop thousands of comets on it. Turns out Mars is a bit cold and dry. Did you know I could warm Mars up just by looking hard at it for a while? And also drop thousands of comets on it. Free launching of space vehicles. Humanity spread out enough that one moments rage can't kill it.

"Still not enough? Think about my spaceship. Do you think you might have more success producing interstellar drives from it with my help? Or do you think you'll get further with an inert hunk of metal that won't listen to you, that you are even now carving pieces off of to use as weapons like some kind of cave man draining the gas from a car so he can throw it on the person that could help him learn to make more cars.

"Does the possibility of colonizing the stars in your lifetime, of mankind spreading out where nothing, not even I, can end them, not outweigh the threat I /could/ pose? I've looked down on this planet from space, and do you know what it looks like? It looks green and blue and fragile. And the places where people live look like a dirty brown stain, like rot spreading across the face a of blue marble that is smaller than you can imagine. I'm not offering to fight crime in one little city and stop the occasional disaster, I'm offering to save the human race from choking to death in its own waste, killing each other en mass in atomic wars before it can ever lift its head to the stars."
:PROPERTIES:
:Author: OrzBrain
:Score: 6
:DateUnix: 1450303720.0
:DateShort: 2015-Dec-17
:END:

*** u/TacticusThrowaway:
#+begin_quote
  You're going to kill me because I'm existential threat after I just offered to use my power to collaborate with you in remaking the whole system of human existence to make it existential threat proof? What the hell, Lex? Do you have some kind of chronic villainy disorder that subconsciously urges you to crush anything more powerful than you and which you cloak in false rationality?
#+end_quote

I felt the ending of the story just got illogically dark n edgy for the purposes of having a Bad End. Which means there's nothing Superman could've said that would've convinced Lex, who was already rather irrational.
:PROPERTIES:
:Author: TacticusThrowaway
:Score: 2
:DateUnix: 1450893024.0
:DateShort: 2015-Dec-23
:END:


** "General Zod, a mighty warrior and general of my people is coming to convert this planet, and many others, to a new world for my people. I can stop him from destroying the planet, you can't. If you kill me you doom humanity to death."
:PROPERTIES:
:Author: Nepene
:Score: 5
:DateUnix: 1450216188.0
:DateShort: 2015-Dec-16
:END:


** Nice try, strong AI.
:PROPERTIES:
:Author: abcd_z
:Score: 3
:DateUnix: 1450231374.0
:DateShort: 2015-Dec-16
:END:


** Not something I would be happy about, but maybe good if I deeply cared (at that time) about humanity more than my future self: propose a killswitch.

Perhaps something which exposes a deadly amount of otherwise-lead-shielded kryptonite when triggered, surgically implanted while rendered weak as an ordinary human. The conditions are trickier--best (given human reaction times) would be some sort of ('absolutely trustworthy') brain-scanning AI that can take really-fast action the moment Superman's likelihood of future humanity-threatening action passes a certain threshold.

--Hmm, 2015 technology makes it harder. Instead of a brain-scanning AI, have to effectively chain trustworthy humans to Superman 24/7, two people briefly connected at once when one has to sleep, death following immediately if there isn't at least one authorised connection. Biometrics complex enough that Superman couldn't fake them without the preparations being noticed would be necessary. Accidental triggering of the killswitch at some point would be almost certain, but some good could be done before then.

In any case, my line of thinking is focused on the 'without endangering humanity' requirement which is the main thing which occupies Lex's attention--the /potential/ existential threat, given that Superman's motivations could change over time. A proposed solution to Lex thus has to render Superman physically incapable of posing a threat to humanity, one way or another.

Ah, another approach/permutation that I almost forgot to mention is to keep Superman in weak-ordinary-human state at all times, with the aforementioned implanted killswitch; unleashing of Superman only possible with several combined authorisations (like a two-key superweapon unleashing system), and again somehow having someone strapped to him at all times who can mentally trigger the killswitch and otherwise automatically have it triggered if that person is rendered unconscious (or otherwise unable to trigger it at will--to explicitly state, this is to try to limit the time Superman potentially has available to destroy the world before being stopped).

Hmm, given(/to bypass) the 2015 technological limitation, maybe 'keep at human-level' (with implanted killswitch set, for the moment, to automatically activate if ever above human-level) until enough time passes to develop a better technological collar, at which part begin the limited use of Superman for specific tasks.

Of course, if before then humanity is about to be destroyed due to an outside threat, Superman can be unleashed even without any guarantee, in order to get even a sliver of hope. Particularly for that reason, make sure that Superman's human-level life is kept pleasant, with enjoyable challenge and variety, and with particular care taken towards keeping him psychologically healthy. The lack of thought speed-up (decreasing potential rate-of-change-of-personality/-morality) should be helpful in that regard, as should the lack of super-hearing (since he wouldn't be forced to hear the pleas for help from everywhere that can't all be answered, even if abstractly--like us--he knows they must be taking place).

Oh, and there's of course the parallel route of 'while at human-level, do lots and lots of research on him over the years and find out to make humans less breakable' (and/or even just 'find out more interesting things about how the universe words, humanity helped in that regard').
:PROPERTIES:
:Author: MultipartiteMind
:Score: 3
:DateUnix: 1450215862.0
:DateShort: 2015-Dec-16
:END:


** Point out that there's a fair chance that killing me sets off a dead man's switch that destroys the Earth. We should estimate the same odds, since I don't actually understand my powers, and those odds are reasonably high - likely outweighing the odds of my becoming genocidal.
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 1
:DateUnix: 1450213479.0
:DateShort: 2015-Dec-16
:END:
