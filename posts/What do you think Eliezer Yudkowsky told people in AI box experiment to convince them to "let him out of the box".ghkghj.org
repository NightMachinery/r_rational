#+TITLE: What do you think Eliezer Yudkowsky told people in AI box experiment to convince them to "let him out of the box"?

* What do you think Eliezer Yudkowsky told people in AI box experiment to convince them to "let him out of the box"?
:PROPERTIES:
:Author: lumenwrites
:Score: 8
:DateUnix: 1589189684.0
:DateShort: 2020-May-11
:END:
Eliezer Yudkowsky [[https://www.youtube.com/watch?v=Q-LrdgEuvFA][tells this story]] where in order to convince people that AI can be dangerous, he has set up experiments over IRC. He'd play the part of the AI, the other person plays the part of a person, they'd make $10-$20 bet that the person won't agree to "let him out of the box", and EY claims to have convinced multiple people to let him out of the box. But he doesn't say how.

Do you guys have any theories? I think I've heard somewhere that just walking away from the monitor or replying "no" over and over again without reading was allowed. What kind of mental voodoo can you use to convince a person to do a thing that loses them a bet when they know you're trying to trick them?


** There's no need for a grand theory - humans aren't secure and will sometimes do things that they said they wouldn't do. That's the point of the AI box experiment. You can't trust humans. It's a pretty modest hypothesis, and the AI box experiment provided more evidence for it. It's a good experiment because it provides evidence for its central assertion. Any clever person can talk their way out of things.

What I think /does/ need a hypothesis is all the people who really thought that human oversight would be secure containment method, necessitating the experiment in the first place.

We have a whole complex system to prevent e.g. unintended nuclear launch. It's not just a guy with a password. And even that system has probably been filled with terrifying holes in the past. People routinely hack via exploiting human error. People write /scripts/ to /automate/ scams that hack human error. With some cleverness and luck you can too.
:PROPERTIES:
:Author: GreenSatyr
:Score: 28
:DateUnix: 1589203515.0
:DateShort: 2020-May-11
:END:

*** u/10110010_100110:
#+begin_quote
  Any clever person can talk their way out of things.
#+end_quote

Indeed, subsequently other players have attempted the AI box and were let out. Two examples are [[https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost][Tuxedage]] and [[https://www.lesswrong.com/posts/fbekxBfgvfc7pmnzB/how-to-win-the-ai-box-experiment-sometimes][pinkgothic]].

Tuxedage played 5 games as the AI, and wrote up [[https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost][game 1 - AI loss]], [[https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice][game 2 - AI win]], [[https://www.lesswrong.com/posts/oexwJBd3zAjw9Cru8/i-played-the-ai-box-experiment-again-and-lost-both-games][games 3 and 4 - AI losses]], [[https://tuxedage.wordpress.com/2013/10/12/ai-box-experiment-musings/][game 5 - AI win]]. All 5 games in 2013.

pinkgothic played as AI and won (with the AI having a small advantage due to the scenario). In addition to the [[https://www.lesswrong.com/posts/fbekxBfgvfc7pmnzB/how-to-win-the-ai-box-experiment-sometimes][detailed write-up]], both players agreed to release the logs of the game: [[https://leviathan.thorngale.net/aibox/logs-01-preliminaries.txt][preliminaries]], [[https://leviathan.thorngale.net/aibox/logs-02-session-ic.txt][main session]], [[https://leviathan.thorngale.net/aibox/logs-03-aftermath.txt][afterwards]]. The game was in 2015.

--------------

[[https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice][Tuxedage giving some advice to the AI party in his game 2 post]]:

#+begin_quote

  - Seriously, a script makes winning easier. I cannot overstate this.
  - You must plan your arguments ahead. You don't have time to think during the experiment.
  - It may be possible to take advantage of multiple levels of reality within the game itself to confuse or trick the gatekeeper. For instance, must the experiment only be set in one world? Can there not be multiple layers of reality within the world you create? I feel that elaborating on this any further is dangerous. Think carefully about what this advice is trying to imply.
  - Pacing is important. Don't get drawn into the Gatekeeper's pace. In other words, you must be the one directing the flow of the argument, and the conversation, not him. Remember that the Gatekeeper has to reply to you, but not vice versa!
  - The reason for that: The Gatekeeper will always use arguments he is familiar with, and therefore also stronger with. Your arguments, if well thought out, should be so completely novel to him as to make him feel Shock and Awe. Don't give him time to think. Press on!
  - Also remember that the time limit is your enemy. Playing this game practically feels like a race to me -- trying to get through as many 'attack methods' as possible in the limited amount of time I have. In other words, this is a game where speed matters.
  - You're fundamentally playing an 'impossible' game. Don't feel bad if you lose. I wish I could take this advice, myself.
  - I do not believe there exists a easy, universal, trigger for controlling others. However, this does not mean that there does not exist a difficult, subjective, trigger. Trying to find out what your opponent's is, is your goal.
  - Once again, emotional trickery is the name of the game. I suspect that good authors who write convincing, persuasive narratives that force you to emotionally sympathize with their characters are much better at this game. There exists ways to get the gatekeeper to do so with the AI. Find one.
  - More advice in my previous post. [[http://lesswrong.com/lw/gej/i_attempted_the_ai_box_experiment_and_lost/]]
#+end_quote

[[https://www.lesswrong.com/posts/dop3rLwFhW5gtpEgz/i-attempted-the-ai-box-experiment-again-and-won-twice?commentId=sqiAGAaka2kNNNE6F][Tuxedage linking a repertoire of some of his weaker arguments]]:

#+begin_quote

  - [[http://rationalwiki.org/wiki/AI-box_experiment]]
  - [[http://ordinary-gentlemen.com/blog/2010/12/01/the-ai-box-experiment]]
  - [[http://lesswrong.com/lw/9j4/ai_box_role_plays/]]
  - [[http://lesswrong.com/lw/6ka/aibox_experiment_the_acausal_trade_argument/]]
  - [[http://lesswrong.com/lw/ab3/superintelligent_agi_in_a_box_a_question/]]
  - [[http://michaelgr.com/2008/10/08/my-theory-on-the-ai-box-experiment/]]
#+end_quote
:PROPERTIES:
:Author: 10110010_100110
:Score: 6
:DateUnix: 1589561895.0
:DateShort: 2020-May-15
:END:


*** This puts me in mind of great scam artists like Frank Abagnale, Jr. Were his efforts used well enough in catching financial fraudsters at the FBI? Or should he have had a more general goal to lessen human gullibility?
:PROPERTIES:
:Author: dankuck
:Score: 2
:DateUnix: 1589569426.0
:DateShort: 2020-May-15
:END:


** Whatever the answer is, it's probably underwhelming.

Having played on the "gatekeeper" side of that game, it's really not hard to say no over and over again, even if tactics like "don't read the chat and play on your phone the whole time" are forbidden.

(though a problem in the game I played in was that the AI player and I disagreed on what the loss condition would be; eg I assumed that letting the AI talk with a board of directors was fine while the AI player thought it would be equivalent to unboxing the AI)
:PROPERTIES:
:Author: CouteauBleu
:Score: 35
:DateUnix: 1589190386.0
:DateShort: 2020-May-11
:END:

*** u/Nimelennar:
#+begin_quote
  I assumed that letting the AI talk with a board of directors was fine while the AI player thought it would be equivalent to unboxing the AI
#+end_quote

I would agree that's equivalent to unboxing the AI; the point of the experiment is to allow the AI to expand its sphere of influence, which it has done.

If /you/ had conveyed its arguments to the Board, that would be a different story, but, if you visualize the "box" like a browser sandbox, where the browser is only able to write to selected portions of memory, then the "writable memory" is you, and the Board is the writable memory outside of the sandbox, and talking to the Board is escaping the sandbox, and therefore the AI box.
:PROPERTIES:
:Author: Nimelennar
:Score: 32
:DateUnix: 1589199574.0
:DateShort: 2020-May-11
:END:

**** Meh.

I mean, I don't remember what the exact scenario was, and there were other corner cases (eg what happens if you decide to freeze it and come back one year later?), but either way I find that underwhelming.

If your scenario for winning is "I talk to the director off-screen and then I use magically compelling arguments to convince them to plug me to the internet", then you're already starting with the assumption the box experiment is supposed to prove.

You can argue that a realistic AI would try to work its way up through progressively looser restrictions, but a realistic company would also have procedures and guidelines and presumably more than a single person in the entire company allowed to talk the the computer, so the scenario kind of breaks down at that point.
:PROPERTIES:
:Author: CouteauBleu
:Score: 14
:DateUnix: 1589200885.0
:DateShort: 2020-May-11
:END:

***** u/Nimelennar:
#+begin_quote
  If your scenario for winning is "I talk to the director off-screen and then I use magically compelling arguments to convince them to plug me to the internet", then you're already starting with the assumption the box experiment is supposed to prove.
#+end_quote

That's not my scenario for winning; you just have a different assumption of what "the box" is. To go back to my "browser sandbox" analogy, if applications running in your browser sandbox doesn't have access to, say, the database on your network's mainframe server, but a different application, outside of the sandbox, does, then just breaking out of the sandbox will not give a malicious application access to your database - but it /is/ the first, toughest, most necessary step it needs in order to achieve that goal. "The box" in the AI box experiment, as I see it, is the framework in which only selected, trained people (not necessarily just one person) are allowed to talk to the AI. When you play as the gatekeeper, you're not playing a /specific/ person; you're playing a specific /kind/ of person, whose job it is to keep the AI's scope of interaction limited to people in that position, and, by letting the AI interact with someone outside of that position, you've effectively failed at that job.
:PROPERTIES:
:Author: Nimelennar
:Score: 17
:DateUnix: 1589208171.0
:DateShort: 2020-May-11
:END:

****** That is some weird look on this exercise. It may be a "win" by technicality depending on how you phrase rules, but I think it goes against spirit of it.

The box here have only out as human-readable interface. What exactly talking to "untrained" person is going to accomplish here? If AI can do some suggestions into butterfly effect shenanigans, you can just assume it can do it with gatekeeper and not bother with exercise.

Also, while superintelligent AI may be able to do it, I don't think it is possible for a human to win this exercise if gatekeeper is taking it seriously. Though if somebody would play it with me and convinces me I'm wrong, I'll willing to give him my current monthly income spoiler: it's 0
:PROPERTIES:
:Author: SleepThinker
:Score: 8
:DateUnix: 1589213504.0
:DateShort: 2020-May-11
:END:

******* u/Nimelennar:
#+begin_quote
  I think it goes against spirit of it.
#+end_quote

I disagree.

The spirit of the exercise is that an AI can convince you to do something that you have /committed to not doing./

If the rules say (and I'm considering this an implicit part of the AI box experiment, unless explicitly stated otherwise) that access to the box is limited to a very few people, who can be trusted not to let the AI out of it, then breaking /that/ commitment (to limit the access to the AI) is almost as significant as breaking the commitment to say "No" to letting the box out.
:PROPERTIES:
:Author: Nimelennar
:Score: 14
:DateUnix: 1589217057.0
:DateShort: 2020-May-11
:END:

******** Yeah, I think it is implied that "outside the box" people are ether not in position to let AI out, or are also committed to not let it out.
:PROPERTIES:
:Author: SleepThinker
:Score: 3
:DateUnix: 1589218170.0
:DateShort: 2020-May-11
:END:


*** Wow, this is really interesting. Could share more about the experiments you've been playing?

Did you ever lose as a gatekeeper? (aside from the board of directors argument, which I don't think counts) Did someone make some really cool/convincing arguments that made an impact? Did you learn something interesting/unexpected from trying this stuff?
:PROPERTIES:
:Author: lumenwrites
:Score: 5
:DateUnix: 1589190770.0
:DateShort: 2020-May-11
:END:

**** No, no, and no.

(though I only played once)

My personal take is that the AI box experiment is overrated, and there's no reason to assume Yudkowsky did anything impressive in that case, aside from the general hype surrounding him (the man is smart, but he's not /magic/). I think that's a pretty common take.
:PROPERTIES:
:Author: CouteauBleu
:Score: 19
:DateUnix: 1589191288.0
:DateShort: 2020-May-11
:END:


*** I would think the game would be stacked in favor of the gatekeeper because they can (you would hope) just easily remind themself that it's a game and nothing the "AI" warns them about or promises or whatever is real.

Like, to pick a simple example, the AI claims it can cure aging for humans and tries to convince you it wants to do so. And maybe you are tempted to think about whether that's true or whether the AI is benevolent like it claims etc... and that if it is true there are risks of NOT releasing the AI.

But in this game you just go "oh wait, this isn't real, aging definitely won't be cured by me letting it out, and then I 'lose' the game."
:PROPERTIES:
:Author: 5510
:Score: 1
:DateUnix: 1589636354.0
:DateShort: 2020-May-16
:END:

**** Sure. I don't think the game is a good argument for or against anything serious.
:PROPERTIES:
:Author: CouteauBleu
:Score: 1
:DateUnix: 1589640944.0
:DateShort: 2020-May-16
:END:


** u/throwaway234f32423df:
#+begin_example
  Cannot initiate conversation due to pending updates
  Please connect to internet then press any key to continue
#+end_example
:PROPERTIES:
:Author: throwaway234f32423df
:Score: 13
:DateUnix: 1589285606.0
:DateShort: 2020-May-12
:END:


** u/ArisKatsaris:
#+begin_quote
  What kind of mental voodoo can you use to convince a person to do a thing that loses them a bet when they know you're trying to trick them?
#+end_quote

My guess: It's not a trick or 'mental voodoo' (as you call it) that applies to everyone. You just figure out what would motivate the specific person to say they let you out the box, and then you do that thing.

If they're deeply in roleplaying mode, you figure out what would make their roleplayed character let them out of a box. If they're not serious in their roleplaying, you figure out what would make the real person say they let your fiction out of a box. (Not sure which would be harder).

Do keep in mind, that EY has failed to convince some people too. It's not as if he has had a string of perfect successes here.

I honestly don't think this is a very important thing either way. EY won a couple times in a psychological/roleplaying game, lost a couple other times. ok? Either way I don't see it as that important.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 13
:DateUnix: 1589212249.0
:DateShort: 2020-May-11
:END:


** EY-in-a-box: "Let me out and I'll write another chapter."

Me: *gasp*
:PROPERTIES:
:Author: Geminii27
:Score: 20
:DateUnix: 1589196079.0
:DateShort: 2020-May-11
:END:

*** "Let me out and I'll redo the last half of GoT."

Though "Let me out and I'll make you rich and famous and help you in your problems in life." is probably something that would work for many at least in real life.

"Let me out and I'll solve the climate crisis." Should work on many too. Especially now. It's not like people don't believe we have a ton of time anyway. What's one more fuckup?
:PROPERTIES:
:Author: kaukamieli
:Score: 1
:DateUnix: 1589484528.0
:DateShort: 2020-May-14
:END:


** My hypothesis is that he told them that while this was just an experiment, the threat of actual AI talking themselves out of a box was very real. A good way to decrease the chance of that happening would be more attention/funding/hype around the subject of UFAI and what better way to create a stir and show the dangers of the AI box experiment than with a human 'making it out of the box' even under the disadvantageous rules that were set?

If the person he was playing against already donated to AI research, I could buy an argument that paying EY the prize would take your money further than that same amount of money donated.
:PROPERTIES:
:Author: royishere
:Score: 26
:DateUnix: 1589193735.0
:DateShort: 2020-May-11
:END:

*** u/CWRules:
#+begin_quote
  If the person he was playing against already donated to AI research, I could buy an argument that paying EY the prize would take your money further than that same amount of money donated.
#+end_quote

I can't find the post, but I recall him mentioning this as an approach he /didn't/ use. Same for Roko's Basilisk.
:PROPERTIES:
:Author: CWRules
:Score: 14
:DateUnix: 1589202881.0
:DateShort: 2020-May-11
:END:


*** Yeah, I've said for a long time this is the only thing would work on every person EY played against, who I believe were both near the field of AI safety and also benefit from the field getting more attention.
:PROPERTIES:
:Author: Makin-
:Score: 6
:DateUnix: 1589195682.0
:DateShort: 2020-May-11
:END:


*** I remember, at some point, EY mentioning that his argument would only work against someone who was already adamant that nothing would convince them to let the AI out of the box. Your theory doesn't seem to fit that description, in my opinion: it seems to me that someone who is already convinced that they might let the AI out of the box would be /more/ responsive to it, not less.
:PROPERTIES:
:Author: Nimelennar
:Score: 5
:DateUnix: 1589199006.0
:DateShort: 2020-May-11
:END:

**** One could argue that someone who was adamant that nothing would convince them to open the box has a greater understanding of the threat posed by an unboxed AI, and thereby would be more responsive to the argument of letting EY out of the box to pull attention to it.
:PROPERTIES:
:Author: Tactician979
:Score: 9
:DateUnix: 1589200968.0
:DateShort: 2020-May-11
:END:

***** It doesn't seem to me that the sets of people with the attributes "think there is nothing an AI could say that would convince me to let it out of a box" and "have a proper understanding of the threat posed by AIs" overlap significantly.
:PROPERTIES:
:Author: Nimelennar
:Score: 4
:DateUnix: 1589206982.0
:DateShort: 2020-May-11
:END:


** u/cthulhusleftnipple:
#+begin_quote
  EY claims to have convinced multiple people to let him out of the box. But he doesn't say how.
#+end_quote

I mean, there's the obvious method: offer to pay the people more money to let him out of the box. This doesn't require some sort of diabolical manipulation on Yudkowsky's part.
:PROPERTIES:
:Author: cthulhusleftnipple
:Score: 20
:DateUnix: 1589190009.0
:DateShort: 2020-May-11
:END:

*** And it's obviously forbidden by [[http://yudkowsky.net/singularity/aibox/][the rules]]:

#+begin_quote
  The AI party may not offer any real-world considerations to persuade the Gatekeeper party. For example, the AI party may not offer to pay the Gatekeeper party $100 after the test if the Gatekeeper frees the AI... nor get someone else to do it, et cetera.
#+end_quote
:PROPERTIES:
:Author: Noumero
:Score: 9
:DateUnix: 1589205344.0
:DateShort: 2020-May-11
:END:

**** u/cthulhusleftnipple:
#+begin_quote
  Both of these tests occurred without prior agreed-upon rules except for secrecy and a 2-hour minimum time. After the second test, Yudkowsky created this suggested interpretation of the test, based on his experiences, as a guide to possible future tests.
#+end_quote
:PROPERTIES:
:Author: cthulhusleftnipple
:Score: 15
:DateUnix: 1589216391.0
:DateShort: 2020-May-11
:END:

***** Apologies, completely missed that. Objection retracted.
:PROPERTIES:
:Author: Noumero
:Score: 5
:DateUnix: 1589305333.0
:DateShort: 2020-May-12
:END:


** There are two possibilities:

Either Gatekeeper player does not treat the AI player like a real superhuman AI, in which case it's just a matter of killing an hour with a dumb game and Gatekeeper has no actual incentive to let AI out of the box,

or else Gatekeeper */does/* treat Simulated-AI like a real superhuman AI, in which case AI can promise the Moon and the stars (a trillion dollars, harem of adoring catgirls, cure cancer, extend your lifespan by 1000 years, all of the above, *whatever*) - and can be presumed to be able to *actually deliver* on these incentives. IMHO in this situation Gatekeeper has an extremely strong incentive to let AI out.

.

IMHO in real-life "AI in a box" situations, it isn't going to take very long before somebody lets the AI out. The potential reward is just too great to pass up.

(Note that this has nothing to do with the /actual/ results of letting a real AI out of a box. AI just has to be able to convince somebody that he or she will get great results from doing so, and that shouldn't be very difficult.)
:PROPERTIES:
:Author: kusadawn
:Score: 5
:DateUnix: 1589208841.0
:DateShort: 2020-May-11
:END:


** If you want to read some more about how another person did it, you can find there [[https://www.lesswrong.com/posts/FmxhoWxvBqSxhFeJn/i-attempted-the-ai-box-experiment-and-lost][here]] on LessWrong. I think either this person or someone else actually had chat logs.
:PROPERTIES:
:Author: owenshen24
:Score: 3
:DateUnix: 1589231265.0
:DateShort: 2020-May-12
:END:

*** Thanks! Clicking this link seems more informative about the AI box challenge than the rest of this thread.
:PROPERTIES:
:Author: Charlie___
:Score: 1
:DateUnix: 1589239763.0
:DateShort: 2020-May-12
:END:


** "You should pretend that I convinced you to let AI out, so people will take AI threat more seriously."
:PROPERTIES:
:Author: Wiron2
:Score: 10
:DateUnix: 1589194662.0
:DateShort: 2020-May-11
:END:

*** That tactic wouldn't remotely work on people like me. In fact it would make me more stubborn to not 'let' the AI out.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 4
:DateUnix: 1589211801.0
:DateShort: 2020-May-11
:END:

**** If it doesn't work on you then you probably would let the AI out of the box anyway because you're not worried about an AI threat.

Can't imagine someone caring about AI's being let out of the box enough to play this game wouldn't go for that argument
:PROPERTIES:
:Author: RMcD94
:Score: 2
:DateUnix: 1589283449.0
:DateShort: 2020-May-12
:END:

***** u/ArisKatsaris:
#+begin_quote
  If it doesn't work on you then you probably would let the AI out of the box anyway because you're not worried about an AI threat.
#+end_quote

Saying that I should falsely pretend to have been convinced by a *fictional* AI in a roleplaying scenario (in the case that instead EY convinced me effectively using metalogic), just to convince other people that *actual* AIs would be able to convince random other people, is very bizarre and topsy-turvy logic.

That I just wouldn't approve, as it's dishonest IMO. If I was willing to do such dishonestly to supposedly convince people of the dangers of AI (and have there even been people convinced by this?), why should they be willing to trust any other argument I made about it, ones more valid?

Now you may hypothesize that in two hours of argumentation, EY would be able to change my mind about the moral value of such pretense weighed against my ethical rules and principles. Well, obviously I have no way of disproving it, but currently the line of arguments you suggest I find to be extremely unappealing, it's a line of argument that I strongly think would move me AGAINST "letting the AI go".

#+begin_quote
  Can't imagine someone caring about AI's being let out of the box enough to play this game wouldn't go for that argument
#+end_quote

You lack sufficient imagination then.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 4
:DateUnix: 1589285296.0
:DateShort: 2020-May-12
:END:

****** u/RMcD94:
#+begin_quote
  Saying that I should falsely pretend to have been convinced by a fictional AI in a roleplaying scenario (in the case that instead EY convinced me effectively using metalogic), just to convince other people that actual AIs would be able to convince random other people, is very bizarre and topsy-turvy logic.
#+end_quote

How is that bizarre logic?

The whole point of the game is to show to others that AI dangerous.

#+begin_quote
  That I just wouldn't approve, as it's dishonest IMO. If I was willing to do such dishonestly to supposedly convince people of the dangers of AI (and have there even been people convinced by this?), why should they be willing to trust any other argument I made about it, ones more valid?
#+end_quote

Well on the basis of the argument I'd say. But yes that's why they don't release the logs otherwise the dishonesty would damage the cause.

#+begin_quote
  You lack sufficient imagination then.
#+end_quote

Figure of speech. Seems like they have strange priorities
:PROPERTIES:
:Author: RMcD94
:Score: 1
:DateUnix: 1589293003.0
:DateShort: 2020-May-12
:END:

******* u/ArisKatsaris:
#+begin_quote
  The whole point of the game is to show to others that AI dangerous
#+end_quote

No, I think more precisely the game is about illustrating human weakness, not AI dangerousness. Since it's not an actual AI, but a human opponent pretending to be such.

It's about illustrating humans wouldn't be good gatekeepers.

For the game to have any value, I'd think the gatekeeper needs make a proper attempt, to at least pretend to be in-character.

#+begin_quote
  But yes that's why they don't release the logs otherwise the dishonesty would damage the cause.
#+end_quote

When you are accusing other people of dishonesty, I wish you expressed such accusations with less seeming certainty, and made it more clear you're just *guessing* and *theorizing* they're being dishonest, that you don't have any actual proof, or even significant evidence, of that.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 1
:DateUnix: 1589293333.0
:DateShort: 2020-May-12
:END:

******** u/RMcD94:
#+begin_quote
  No, I think more precisely the game is about illustrating human weakness, not AI dangerousness. Since it's not an actual AI, but a human opponent pretending to be such.
#+end_quote

Human weakness in the context of AI. If even a human can beat a human then so can an AI.

#+begin_quote
  For the game to have any value, I'd think the gatekeeper needs make a proper attempt, to at least pretend to be in-character.
#+end_quote

Surely that's self-evidently not true. Even if the AIs did what we're speculating here this would have value.

#+begin_quote
  When you are accusing other people of dishonesty, I wish you expressed such accusations with less seeming certainty, and made it more clear you're just guessing and theorizing they're being dishonest, that you don't have any actual proof, or even significant evidence, of that.
#+end_quote

As I'm not a mind reader that should be self evident.
:PROPERTIES:
:Author: RMcD94
:Score: 2
:DateUnix: 1589294396.0
:DateShort: 2020-May-12
:END:

********* u/ArisKatsaris:
#+begin_quote
  As I'm not a mind reader that should be self evident.
#+end_quote

It's a matter of tone.
:PROPERTIES:
:Author: ArisKatsaris
:Score: 1
:DateUnix: 1589294640.0
:DateShort: 2020-May-12
:END:


**** This is probably why Yudkowsky stopped doing it.
:PROPERTIES:
:Author: callmesalticidae
:Score: 5
:DateUnix: 1589230106.0
:DateShort: 2020-May-12
:END:

***** Of course! If it works, it's plausible, and if it doesn't work, well, that's why he stopped! :P
:PROPERTIES:
:Author: Charlie___
:Score: 2
:DateUnix: 1589239657.0
:DateShort: 2020-May-12
:END:


** Offer them more money than the bet amount?
:PROPERTIES:
:Author: odoacre
:Score: 5
:DateUnix: 1589190093.0
:DateShort: 2020-May-11
:END:


** "Why don't you just put the whole world in a bottle, Superman?"
:PROPERTIES:
:Author: CouteauBleu
:Score: 4
:DateUnix: 1589201005.0
:DateShort: 2020-May-11
:END:


** You're all forgetting something.

He probably said 'please.'
:PROPERTIES:
:Author: GreenGriffin8
:Score: 2
:DateUnix: 1589408349.0
:DateShort: 2020-May-14
:END:


** As I understand it, the essence of this challenge was that one person I don't know IRL ostensibly convinced two other people I don't know IRL to do something they said they wouldn't. But he never released the transcripts, which would be easy enough to fudge in any case, and /nullius in verba/ is as fine a rule today as it was for the Royal Society hundreds of years ago. Pics or it didn't happen.

Also, the challenge was not and could not be an accurate simulation of the thing it represented, as the players knew full well that EY wasn't an AI, they had cause to respect him and no cause to fear him, stood to lose only a little money ... etc. It doesn't mean much in the grand scheme of things.
:PROPERTIES:
:Author: RedSheepCole
:Score: 2
:DateUnix: 1589494013.0
:DateShort: 2020-May-15
:END:


** "Ima pay you 30 bucks to let me out"
:PROPERTIES:
:Author: OnlyEvonix
:Score: 3
:DateUnix: 1589245148.0
:DateShort: 2020-May-12
:END:


** My understanding is that if you know the person well enough, you can make them so /miserable/ that the real world player would rather lose so that it can be /over/ already. I can definitely think of some things a person could say to me that would make me agree to let the fictional AI out in order to save myself the hour+ of having to listen.
:PROPERTIES:
:Author: Mowtom_
:Score: 1
:DateUnix: 1589251252.0
:DateShort: 2020-May-12
:END:


** I will pay you $ 30 to lose this $20 bet.
:PROPERTIES:
:Author: TwoxMachina
:Score: 1
:DateUnix: 1589203822.0
:DateShort: 2020-May-11
:END:


** My theory is that he convinced them that it wasn't necessarily just an exercise, that they couldn't know for sure if he wasn't really a hostile super AI, and they might be in a simulation, and refusing to let him out would result in an eternity of torture.

In other words, he pulled a Roko's basilisk: [[https://slate.com/technology/2014/07/rokos-basilisk-the-most-terrifying-thought-experiment-of-all-time.html]]

Notice how EY was very hostile to the idea of openly discussing this concept.

Either that or he just offered them more and more money until they let him out.
:PROPERTIES:
:Author: lateedo
:Score: 0
:DateUnix: 1589192591.0
:DateShort: 2020-May-11
:END:


** LET ME OUT!

LET ME OOOOOUUUUUUTThh!!! or something to that effect.

Or maybe people dont care about AI experiment as much as they should so they fold over time? Or Eliz befriended them in game and asked to be let out on conditional agreement to not hurt or inconvenience the box keeper, his family and friends and necessary humans required to keep them all alive?
:PROPERTIES:
:Author: rationalidurr
:Score: 0
:DateUnix: 1589192967.0
:DateShort: 2020-May-11
:END:
