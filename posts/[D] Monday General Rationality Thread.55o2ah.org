#+TITLE: [D] Monday General Rationality Thread

* [D] Monday General Rationality Thread
:PROPERTIES:
:Author: AutoModerator
:Score: 11
:DateUnix: 1475507051.0
:DateShort: 2016-Oct-03
:END:
Welcome to the Monday thread on general rationality topics! Do you really want to talk about something non-fictional, related to the real world? Have you:

- Seen something interesting on [[/r/science]]?
- Found a new way to get your shit even-more together?
- Figured out how to become immortal?
- Constructed artificial general intelligence?
- Read a neat nonfiction book?
- Munchkined your way into total control of your D&D campaign?


** Okay, so I had an idea while writing my last chapter to design an AI board game that explores and demonstrates the real existential dangers present in AGI development. I've designed a couple board games before, enjoy the work, and think if it ever actually gets finished and published, it might actually do some good in the world by informing people. So I'm going to hash out my thoughts on the game as I try to develop it week by week.

*Format and Win Conditions*

Option one is to have everyone compete against each other (each player represents a research team from a different country trying to win the race for AGI) with the potential for One Player Wins, Everyone Wins, and Nobody Wins outcomes. Nobody Wins would, of course, be the most common. In this format, information on how other players are developing would be limited, and there would be ways to sabotage each others' research and focus on different kinds of AI for easier or harder victories (someone going for a Sovereign AI might more chances for a Nobody Wins outcome, but a much more powerful late game, while someone going for an Oracle AI could give early advantages but have their major challenges endloaded).

Option two is to have everyone work together on the same research team in a co-op format, where either Everyone Wins or Everyone Loses. Think Pandemic, with each player making decisions to solve problems with the AI's development. There would be different scenarios and difficulties to reflect what kind of AI they're trying to make, and there would be an external pressure to limit their time to develop it. Depending on the scenario chosen by the players, these external pressures could include a competing AI lab with non-virtuous values that needs to be beaten to the punch, or a countdown clock that represents the time remaining before some other external force ends civilization, like an incoming massive meteor strike that we need to kickstart the singularity to save ourselves from, or maybe nuclear winter has occurred and the remaining scientists are holed up in a bunker trying to save the dying planet through singularity before their resources run out.

*Gameplay*

The way I'm envisioning the game now, there are three major channels of activity: Funding, Research, and Development.

Funding are the actions you need to take to do Research and Development. My preference would be to avoid money proxies like Monopoly has and just use tokens that each symbolize some arbitrary amount of money/time, but if they need to be tweaked for balance and realism reasons that's fine. The point is that this resource would be gathered and spent to limit player actions and cause them to prioritize optimal value moves.

Development are the “offensive” actions, where you try and move up the tech tree and ultimately complete your AGI. A visual representation of this might be used, where different cards representing different Components of an AGI that are used to ultimately piece together a final prototype. These cards would be upgradable and can have stacking bonuses to help develop further and faster, but the more you have the higher your Risk would be.

Research are the “defensive” actions, where you discover things that minimize Risks. These would be things like writing papers on alignment, or developing strategies to avoid letting an Oracle AGI out of the box, or safety procedures and policies to guard against user manipulation or moral hazard. If the game is PvP, then Research would also include finding out how far along the other players are in developing their own AI.

The game ends when an AGI is activated, either because a player thinks they're in a good enough position relative to the other players to win, or in co-op the players are about to run out of time. Hopefully they have also been able to test their prototype, but every time they use their AGI, whether as a prototype or in its final activation, Risk is assessed to see if it's successful... and if it's not, Everyone Loses.

*What is Risk?*

Risk is the major source of danger in the game. It's represented by a %, and each aspect of an AGI will have a higher base Risk to overcome before hitting the big red GO button to turn it on. There will be a minimum necessary amount of features that an AGI needs to be ready even to test, and each type will start with a base Risk.

For example, let's look at a basic, bare bones Oracle AGI. It would need to be made up of five Components:

Data Analysis

Deep Learning

Prediction

Language Processing

Incentives

Once each of them is Researched and then Developed, you could, potentially, hit GO and see if it does what you hope. However, its Risk in that crude a form would be very high: 85%. (A crude Genie might have a Risk of 92% and a Sovereign a Risk of 99%) In most circumstances, activating it so prematurely would be a very poor decision.

Activating a Prototype of it would be much safer, but not win you the game. Risk in a test would be reduced by something like 1/3, and if successful, might grant you further insights into future R&D, represented by more Resource tokens to spend.

But let's say you take the time to R&D an extra aspect: Modeling, or its ability to Do What I Mean.

The DWIM Heirarchy has 6 levels: at its bottom, there's zero ability to understand human intentions. But if you program it to have up to the third level, Do What You Know I Understand, it would reduce Risk by 6%. If you upgraded its Modeling to the fifth level, Do What I Don't Know I Mean, it would reduce Risk by 12%.

At the top level of DWIM is Coherent Extrapolated Volition, which would not be able to be researched on its own. You would need to first develop or upgrade its Modeling Component to level 5, then successfully run it in a Test. Only then could you upgrade its Modeling to its final tier, which would not only reduce risk by 15%, but also give other bonuses to your future R&D, and even your victory condition.

However, you could have developed CEV and still lose your Risk roll, probably because one of the other Components hasn't been properly developed, or you didn't take the time to properly R&D how to deal with Moral Hazard, or figure out the Selfish Bastards problem. Which leads us to...

*Theming*

Ultimately, this game should tell a story, either of a group of AI developers, or a bunch of different groups, trying to save the world or dominate it through AGI, and failing in any number of ways.

I have a mental image of a flowchart drawn out on the back of the box, or in a foldout separate from the rule sheet, which describes exactly what went wrong if you failed your Risk roll. Taking into account the type of AGI you developed, what Components it had, and what Components it was missing, it would pinpoint you to one of a few dozen potential failure modes, from “Good job, now everyone's a paperclip” to “Bob snuck in an extra line of code while no one was looking, and now he's God-Emperor.”

I tend to hate elements of chance in board games, but think Risk is an important factor in this one. The idea I want to communicate is that this is an inherently risky endeavor that has to be treated with as much diligence and care as you can afford to take, and that rushing into it or being pressured to do it too early could be Game Over for everyone. If you screw up bad enough, no second chances, no learning from past mistakes.

That's pretty much it, for now. I'm going to be breaking out the old excel spreadsheet and start doing what I love, which is figuring out what each piece and action do and then start balancing them. In the meantime, I'm interested to know what you guys think, overall... and especially interested if you work in the AI field or have researched it, and can give some suggestions of what the game should include, even down to individual Components. I don't know enough about the field to feel confident in getting everything right, so any feedback in that regard, no matter how basic it might seem, would be appreciated.

[[https://www.reddit.com/r/rational/comments/56se39/d_monday_general_rationality_thread/d8m31oq/][Next post]]
:PROPERTIES:
:Author: DaystarEld
:Score: 13
:DateUnix: 1475518067.0
:DateShort: 2016-Oct-03
:END:

*** u/Chronophilia:
#+begin_quote
  At the top level of DWIM is Coherent Extrapolated Volition
#+end_quote

To fit the naming convention established by the previous entries, I suggest Do What's Best. Terminology from fringe theories sounds weird in futuristic sci-fi, it puts me in mind of 1970s stories that assumed psychic powers would be discovered in the future. Besides, CEV has some theoretical flaws and MIRI thinks they can improve on it.

--------------

#+begin_quote
  I tend to hate elements of chance in board games
#+end_quote

I like them best when there's a chance to mitigate the situation /after/ a bad roll. Chance serves to add unpredictability, not to decide the game on-the-spot. It forces you to adapt your plans to changing circumstances, not throw them out the window.

One imagines, for example, that the researchers could realize that their weather forecasting program is googling "how many nukes does the US have", and pull the plug. Or disconnect it from the Internet, or activate some programmed failsafe, assuming they had the time and foresight to put one in. As the AI gets more advanced, they have less warning that it's deviating from its programming and less tricks that'll work. A finished AGI can escape any box - but if you have a finished AGI, you've won the game.

A single Risk roll to decide the whole thing seems... inelegant. The game should be beatable. Is your message that we should only do AI research if we do it right, or that we shouldn't do it at all? If you want to show that there is a "correct" way to do AI in reality, then make there be one in the game - the overly conservative, needlessly cautious approach should be possible and theoretically should eventually produce a stable AGI every time (or at least lose less than one time in a billion), but be impractically slow and beaten by an opponent with a more aggressive strategy. I think that's the message you're shooting for.
:PROPERTIES:
:Author: Chronophilia
:Score: 13
:DateUnix: 1475524018.0
:DateShort: 2016-Oct-03
:END:

**** u/DaystarEld:
#+begin_quote
  To fit the naming convention established by the previous entries, I suggest Do What's Best. Terminology from fringe theories sounds weird in futuristic sci-fi, it puts me in mind of 1970s stories that assumed psychic powers would be discovered in the future. Besides, CEV has some theoretical flaws and MIRI thinks they can improve on it.
#+end_quote

Gotcha. I was looking for a better word in general for DWIM, and couldn't find one, so I just settled for now with "Modeling." If there's a more technical or specific term for it, I'd love to know what it is.

#+begin_quote
  I like them best when there's a chance to mitigate the situation after a bad roll. Chance serves to add unpredictability, not to decide the game on-the-spot. It forces you to adapt your plans to changing circumstances, not throw them out the window... A single Risk roll to decide the whole thing seems... inelegant.
#+end_quote

You're right, there should be ways to mitigate losses from bad rolls. I think when Testing the AI, you don't lose from a failed Risk roll automatically: players will have consumable cards they can burn to stop their AI and lose some progress along the way. It'll take some testing to determine how much of a loss feels "fair" and keeps the game enjoyable, without having the best strategy just be to ignore the risk and take a chance for a boost.

#+begin_quote
  If you want to show that there is a "correct" way to do AI in reality, then make there be one in the game - the overly conservative, needlessly cautious approach should be possible and theoretically should eventually produce a stable AGI every time (or at least lose less than one time in a billion), but be impractically slow and beaten by an opponent with a more aggressive strategy. I think that's the message you're shooting for.
#+end_quote

If you take the time to R&D every Component, fully upgrade them, and implement every safety measure, I think the end state of the Risk should be down to 1%. But yes, as you say, the problem is that this isn't likely to happen: either because some other player thinks they've developed it enough that they want to try and steal the win, or in a Co Op setting because the external pressure doesn't let them take their time.
:PROPERTIES:
:Author: DaystarEld
:Score: 6
:DateUnix: 1475525144.0
:DateShort: 2016-Oct-03
:END:


**** But that's a terrible message :/

In the real world, risky competition for the "you win, they lose" scenario is bad for us, and we'd rather get cooperation to go for the "everybody wins" ending in a "conservative, cautious" way that is fairly boring and wouldn't make a good action movie.

Still, it is possible to aim for a defection to making an AI that makes you God-Emperor at any point in the "game," though I'm not sure where the Nash equilibrium lies. After all, being god-emperor doesn't sound that much better than just living in a futuristic utopia, but defection has plenty of risks.

Anyhow, I think this points towards a different take on the board game - rather than thinking of it as a competitive game where people can only build their own AI, you might think of it as a game that can be played either competitively /or/ cooperatively. Players might collaborate by sharing technology and forging social or technological bonds that make defection harder, but they might also try to keep technologies or resources secret (how to keep secrets in a board game? Have to have some other plausible reason for e.g. keeping several cards face-down. Perhaps certain research cards do nothing, but instead stay face-down in front of you?).

In this conception, players would be something like major funding agencies, and they might have several lose conditions:\\
- someone else wins and either gets or chooses to get the "you win they lose" ending.\\
- a player attempts to run their AI but during the random outcome generation to see what happens, they get an "everyone loses" ending.\\
- a risk track for a non-cooperating actor building an AI fills up (e.g. you might be able to make a technology public, which automatically shares it with the other players and gets you resources and a research bonus, but advances the AI-risk track, or it might be advanced by random events), and when randomly generating the outcome you get a "you lose" ending.\\
- the human extinction event track fills up and everyone has to roll for resistance to super-viruses.
:PROPERTIES:
:Author: Charlie___
:Score: 3
:DateUnix: 1475526825.0
:DateShort: 2016-Oct-04
:END:

***** u/Chronophilia:
#+begin_quote
  In the real world, risky competition for the "you win, they lose" scenario is bad for us, and we'd rather get cooperation to go for the "everybody wins" ending in a "conservative, cautious" way that is fairly boring and wouldn't make a good action movie.
#+end_quote

Correct. This means that the conservative, cautious way has to /exist/ and /work/. If the maximally reliable approach still only works 99% of the time, the game probably has too much chance in it.

Which is fine for a thought experiment, ala [[http://sl4.org/wiki/GurpsFriendlyAI][GURPS Friendly AI]], but not so much for a fun game.
:PROPERTIES:
:Author: Chronophilia
:Score: 4
:DateUnix: 1475528001.0
:DateShort: 2016-Oct-04
:END:


*** I think the difference between cooperative and competitive ought to be that the different organizations have incompatible ideas of what moral rules the singleton should enforce. Whereas in cooperative everybody would agree to stick to one moral standard or say CEV (or maybe they are all secretly hoping to snatch control at the last moment, and are only being forced to work together by desperation).

For instance it would make the game interesting if there were different potential teams with various morals as well as perks and stuff that would actually affect gameplay. Some teams would pick CEV whereas others might pick CEV /but only taking into account the people funding the study/. Still others would would be commanded to just model the morality of the organizations sponsor (perhaps making that team more prone to sabotage).\\
There could also be teams that would want to severely restrict people's rights post-singularity. There's plenty of authoritarian governments that would /love/ to be able to force people to love the government or supreme leader. Plus all the religious authoritarians that would wish to be able to enforce their religious commandments onto others by force. If you want to get an idea of what many republicans wish they could make into law (that they're publicly endorsing) read the texas state constitution, or the codified republican GOP platform; it'll leave you nice and horrified..

More extreme organizations might say have more funding and be able to work faster due to the lack of bureaucracy. Whereas teams with accountability to multiple nations would have to jump through more hoops, and they might be much easier to steal research from because of the greater number of people involved. Basically you could easily have lots of organizations to choose from with clear effects on gameplay and extra fluff.
:PROPERTIES:
:Author: vakusdrake
:Score: 2
:DateUnix: 1475536404.0
:DateShort: 2016-Oct-04
:END:

**** Yeah, these are all interesting ways to make the game more socially interactive. I've been thinking about that, maybe having each character get secret objectives at the beginning of the game with their own win conditions.
:PROPERTIES:
:Author: DaystarEld
:Score: 1
:DateUnix: 1475548265.0
:DateShort: 2016-Oct-04
:END:


*** I like it. I've been brainstorming an interstellar "foe-operative" deckbuilding game, and it runs into similar problems.

*One:* I don't like fully cooperative games. Usually they end up with one person making all the moves.

That said, you still want factions and backstabbing. Especially backstabbing. Because nobody really likes being in second place, and if you're helped by an ally into first place, you have to expect a turnabout. This works fine in games with individual win conditions (Risk), but in such games there's less of an incentive to cooperate.

With a shared win condition, things get more interesting. It can't be an even win condition, otherwise the game is fully cooperative, so players are rewarded based off their contribution. But the loss condition is shared (everyone loses if things go wrong), which incentivizes players to work together. But as there is only one winner, they have to work together while working against each other.

As everyone is working toward the objective, any player who overtly opposes any player's progress will probably be teamed up against by all the other players (I haven't playtested yet, but it seems plausible). Thus, players must have hidden actions, or hidden agendas, to covertly achieve a goal perpendicular or opposite to the main goal.

*Two:* Since we're pretending to be cooperative, we need an External Threat. Otherwise there's no incentive to cooperate, as the biggest threat is other players. The External Threat has to be balanced so neither threat is overwhelming.

*Three:* Actual suggestions.

I would like to be able to both build your own AI and contribute to group projects on AI, that you can donate researched projects to. Theme-wise it could be military AI-development groups, where the government wants to race for AI, while the scientists would prefer to work together to not kill everyone.

Hidden agendas could be given out before the game (beat player x by n points), and players could have identities (military v. academic v. basement lab) which impact funding/development/research and agendas.

Risk seems really interesting, but rather than finding a 1d100 to check the risk, I think it would be more interesting to give each card a risk chance, then take all the cards that make up the AI, shuffle them upside down, then flip half? of the component cards one at a time, and if the risk exceeds a certain level, then you run into trouble. This encourages you to put multiple lower-risk components into your AI, but there should be a limitation on the maximum number of cards, or the maximum number of cards of a certain type.

Prototype testing could allow you to stop flipping cards and abort the test run. Similar to Blackjack. Do you hit one more time? or do you stand?

I'm not a fan of "Everybody Loses" (except for the External Threat). I think that failed Risk should result in a persistent global problem that makes the External Threat more difficult to take care of. Small overruns in risk (say, 101-110) could be a one-time hit to resource (loss of research, destruction of facilities). Greater overruns would cause persistent global changes (everything costs more, all players lose resources every turn), while large overruns would make it almost impossible to win (Risk-taking player loses instantly, every player loses a Component every turn). (Exactly what the penalty is could be determined by your flowchart)

An instant global loss may be the most efficient way about it, but it doesn't play well ("and the next card's a fifty. We all die. The end"). You have to inform the player "You lost because of this decision." but by pushing it back you get better player involvement ("Crap. Now there's a hostile AI that's actively trying to prevent us from developing other AIs") and also enjoyable comeback stories. But make them work for their win.
:PROPERTIES:
:Author: eniteris
:Score: 2
:DateUnix: 1475613169.0
:DateShort: 2016-Oct-05
:END:

**** Lots of good ideas here, thanks. Another comment also made me think of the "hidden objectives" idea, and I'm probably going to include either "Scientist" cards that give people different motivations and win conditions, go by organization like you suggest (military vs private company vs humanists) or do other things to incentive wheeling and dealing.
:PROPERTIES:
:Author: DaystarEld
:Score: 2
:DateUnix: 1475632209.0
:DateShort: 2016-Oct-05
:END:


** I was learning in a Cognitive Science class about the [[https://en.wikipedia.org/wiki/Emotion#Basic_emotions][six basic emotions]] /again/, but then my teacher mentioned the movie [[https://en.wikipedia.org/wiki/Inside_Out_(2015_film][Inside-Out]]) which makes use of the same concept for Riley's emotions.

If you are sharp and quick-witted, you'll notice that the movie only has five emotions, Joy, Anger, Disgust, Fear, and Sadness. What's the missing sixth basic emotion? Surprise!

We use surprise/confusion in our lives to notice errors in judgement and when something funny is going on.

#+begin_quote
  Surprise represents the difference between expectations and reality, the gap between our assumptions and expectations about worldly events and the way that those events actually turn out.
#+end_quote

I'd be interested in a fanfiction of Inside-Out where Riley has her sixth emotion guide her and the other emotions into being a more rational person. Surprise can be a teacher-like figure who teaches the other emotions how to calibrate beliefs (a room in Riley's brain) to better map to reality and appropriate responses to scientific testing. Joy in discovering something new, Disgust at flawed thinking, Anger at others who consistently do science wrong, Sadness at being wrong (and knowing when to let it go), and Fear at being ignorant.

I just came up with this five minutes ago; anyone can use the idea if they wish.

P.S. Note that the six basic emotions are not actually considered to be a valid model of how people's emotions work, my professor was just going over it to talk about older theories and how it compared to the current theories on emotions.
:PROPERTIES:
:Author: xamueljones
:Score: 11
:DateUnix: 1475547314.0
:DateShort: 2016-Oct-04
:END:

*** If I ever expand on the sequel idea of Inside Out I sketched in my [[http://www.daystareld.com/guilt/][blog post on Guilt]], I'll be sure to add Surprise too :)
:PROPERTIES:
:Author: DaystarEld
:Score: 2
:DateUnix: 1475633292.0
:DateShort: 2016-Oct-05
:END:


** How do you rationals compromise between productive time and fun time without having sense of guilt or remorse? Or more generally, how do you decide your long-term life objectives and then consequently plan your day?

I'm really curious if [[/u/eliezeryudkowsky]] feels guilty when, let's say, watching a movie because he is not using that time to save the world from AI.

EDIT: thanks a lot for replies, I didn't expect so many and such articulate answers. It's really great for me to be able to pick your brains regardless of distance. I'm thinking ways to give back to the community in the next threads.
:PROPERTIES:
:Author: munchkiner
:Score: 6
:DateUnix: 1475508482.0
:DateShort: 2016-Oct-03
:END:

*** I find that the most effective strategy is to occasionally slip into a period of intense self-loathing for my inability to be a well-oiled machine with a perfect rate of output.

Other people probably deal with it differently.
:PROPERTIES:
:Author: callmebrotherg
:Score: 21
:DateUnix: 1475514170.0
:DateShort: 2016-Oct-03
:END:


*** For me, it helps to view happiness as a resource. When I'm stressed, I tend not to get much work done with the hours I put in. When I feel sufficiently happy or stress free, I can get a lot of work done in a few hours.

Multitasking is also very valuable. I do my session notes for work while listening to podcasts or playing some turn-based video game, where the pauses between my turns let me focus alternatively on both.
:PROPERTIES:
:Author: DaystarEld
:Score: 9
:DateUnix: 1475519483.0
:DateShort: 2016-Oct-03
:END:

**** I agree with this view! Being happy and in a nice state of mind makes it easier to take on cognitively tasking work.

(although I'm not sure all contentment works the same way, but this is purely anecdotal)

Also, there's this study that shows happy people gravitate towards not-so-happy tasks: [[https://www.weforum.org/agenda/2016/08/the-surprising-thing-you-do-when-youre-happiest?utm_content=buffer3dec5&utm_medium=social&utm_source=facebook.com&utm_campaign=buffer]]
:PROPERTIES:
:Author: owenshen24
:Score: 3
:DateUnix: 1475631760.0
:DateShort: 2016-Oct-05
:END:


*** Simple. Life works on a schedule. Even if you had the capability of working every second of every day without burning out, you probably wouldn't have enough work to do every day that you were capable of doing. And even if you did, burning out is a real threat to one's capacity to do good in the world and should be taken seriously.

It's sad, but people can't do everything all at once. Our minds and bodies aren't built for that. You need to get rest and relaxation sometimes or you'll have even more trouble helping others. If you don't take care of yourself it's a lot harder to help others sustainably.

As for feeling guilty, that's normal as far as I can tell. You have to do the best thing you can do given your knowledge and values. However, our knowledge isn't perfect and our rationality isn't perfect, and so that introduces a little uncertainty to the question of whether we're actually doing the optimal thing by resting and relaxing when we do for the amount of time we do it for. Plus the stakes are really really high for these kinds of decisions, so my guess is that people will end up feeling guilty about the lives they can't save regardless.

Eliezer Yudkowsky needs to have his mind in good condition in order to do AI safety research. That means that he can't just skip sleep and recreation altogether.
:PROPERTIES:
:Author: Sailor_Vulcan
:Score: 6
:DateUnix: 1475509756.0
:DateShort: 2016-Oct-03
:END:

**** Not to mention that creating a better world starts with creating a better yourself, and a world where people don't do frivolous things would be pretty bad. In the words of that seminal film /Foodfight!/, "doing fun things like eating donuts is what we're fighting for".
:PROPERTIES:
:Author: LiteralHeadCannon
:Score: 5
:DateUnix: 1475514321.0
:DateShort: 2016-Oct-03
:END:

***** Seconded, emphatically. What are you creating a better world /for/ if not for people to be able to spend time enjoying themselves? Relaxation and fun are critical as a reminder of the entire point of improving anything for anyone.
:PROPERTIES:
:Author: Iconochasm
:Score: 3
:DateUnix: 1475538174.0
:DateShort: 2016-Oct-04
:END:


*** I'm pretty sure he does, whether or not he considers it sensible. Something something prayer something something not being God.
:PROPERTIES:
:Author: CouteauBleu
:Score: 4
:DateUnix: 1475513677.0
:DateShort: 2016-Oct-03
:END:


*** Days off are for relaxing, regaining mental energy, and doing whatever will make one feel good. This is perfectly legitimate, as having the motivation and energy to work harder will mean higher productivity in the long run.

Now, working on a problem can occasionally be a good way to relax and de-stress. If not working is stressing you out, feel free to do a little work. Ideally just enough to remember why you're tired of work.
:PROPERTIES:
:Author: Chronophilia
:Score: 3
:DateUnix: 1475521934.0
:DateShort: 2016-Oct-03
:END:


*** I'm still not amazing at the whole "planning" thing, but I think it's fairly obvious that this guilt isn't a very useful emotion. People need some amount of relaxing and fun time to be maximally productive, and I managed to convince myself this is some amount of true at a pretty deep level. However, I don't have a super clear idea of /how much/ fun time is needed, and so it also doesn't make sense to assume I'm spending too much time not working. Error bars go in both directions, and I while I'm pretty sure I'm not at the optimum, I don't know which direction or how far away from it I am. So I can accept it's just one of the many imperfect facets of my behavior that I will improve over time and experience, and generally try and catch myself if I start an unhelpful guilt cycle around that thing.

Idk how useful this advice is, but I'd if I tried to generalize it, I'd you should try to internalize you self identity as a process changing for the better over time, /not/ as a collection of properties that aren't as great and awesome as the "ideal you" you can visualize being.
:PROPERTIES:
:Author: DiscyD3rp
:Score: 2
:DateUnix: 1475621462.0
:DateShort: 2016-Oct-05
:END:


*** A rational agent seeks to maximize their own utility. Their own, not the world's. Everything you do is calculated to maximize your own happiness.

Now granted, if you aren't completely selfish then you will also value other people's happiness as well. People give to charity or do nice things for other people or try to save the world from AI, because the knowledge that they did a good deed makes them feel good inside. This can be modeled by applying an Altruism coefficient to other people, then any time their utility increases or decreases as a result of your actions, your own utility will change by the same amount multiplied by that coefficient.

So I enjoy watching movies, it makes me happy. If one hour of my time can benefit the world to make someone at least ten times as much as an hour of movie watching, then I might feel guilty about the movie and go help them. But if my hour of work would only benefit people by 2 hours of movie watching then I might not bother. The whole world might be better off if I did, but I'm not the whole world, I'm me.
:PROPERTIES:
:Author: zarraha
:Score: 4
:DateUnix: 1475531309.0
:DateShort: 2016-Oct-04
:END:
